,paper_1,paper_2,paper_1_id,paper_2_id,datasets_diff,approach_diff
0,wk_paper_1,wk_paper_2,11,12,"sent: Article 1 uses different datasets from the medical domain sourced mainly from PubMed, while Article 2 uses medical domain datasets sourced from different sources. sent: Article 1 uses 20 systematic review datasets, while Article 2 uses 23 systematic review datasets. ","sent: Article 1 and Article 2 use supervised learning approach. sent: Article 1 uses TF-IDF, log-linear model and LDA topic modelling for feature extraction, while Article 2 uses deep denoinsing encoder and feed forward neural networks for feature extraction. Article 1 and Article 2 uses SVM classification for systematic reviews for binary relevance."
1,wk_paper_1,wk_paper_3,11,13,sent: Article 1 and Article 2 uses different datasets from the medical domain sourced mainly from PubMed. sent: Article 1 and Article 2 uses 20 systematic review datasets.,"sent: Article 1 and Article 2 use supervised learning approach. sent: Article 1 uses TF-IDF, log-linear model and LDA topic modelling for feature extraction, while Article 2 GLOVE word embeddings for feature extraction. Article 1 uses SVM classification for systematic reviews for binary relevance, while Articla 2 uses a Multi-Channel Convolutional Neural Network (CNN) clasification approach. "
2,wk_paper_2,wk_paper_3,12,13,"sent: Article 1 uses medical domain datasets sourced from different sources,  Article 2 uses different datasets from the medical domain sourced mainly from PubMed. sent: Article 1 uses 23 systematic review datasets, while Article 2 uses 20 systematic review datasets. ","sent: Article 1 and Article 2 use supervised learning approach. sent: Article 1 uses deep denoinsing encoder and feed forward neural networks for feature extraction, while Article 2 GLOVE word embeddings for feature extraction. Article 1 uses SVM classification for systematic reviews for binary relevance, while Articla 2 uses a Multi-Channel Convolutional Neural Network (CNN) clasification approach. "
3,an_paper_1,an_paper_2,31,32,sent: Article 1 and Article 2 use datasets sourced from AskApatient reviews and CADEC datasets sourced from SNOMED-CT. sent: Article 1 and Article 2 use medical domain datasets for infomal and formal text. sent: Article 1 and Article 2 use 5 folds of the datasets.,"sent: Article 1 uses semantic similarity approach for mapping informal medical terms to formal terms, Article 2 uses deep learning classification approach for mapping informal medical terms to formal terms. sent: Article 1 uses a bidirectional RNN with attention on top of the embedding layer (HealthVec and PubMedVec) for feature representation, while Article 2 uses HealthVec and Google News embeddings in combination  with ELMo ( HV, GN, GELMo, BELMo and CELMo) embeddings for feature representation. sent: Article 1 uses cosine similarity, while Article 2 uses a BiLSTM model for classification."
4,yg_paper_1,yg_paper_2,41,42,"sent: Article 1 uses datasets sourced from TREC Genomics collection, while Article 2 uses datasets sourced from TREC 2014 Web Track dataset. sent: Article 1 uses datasets from the gemomics domain, while Article 2 uses dataset from various domains. sent: Article 1 uses 4 topics for the datasets, while Article 2 uses 11 topics for the datasets. sent:  Article 1 48 participants were recruited, while Article 2 468 participants were recruited.",sent: Article 1 and Article 2 use supervised learning approach. sent: Article 1 and Article 2 use diffferent behaviour features. sent: Article 1 and Article 2 use regression models to predict user knowledge.
5,yg_paper_1,yg_paper_4,41,44,"sent: Article 1 uses datasets sourced from TREC Genomics collection, while Article 2 uses datasets sourced from TREC 2014 Web Track dataset. sent: Article 1 uses datasets from the gemomics domain, while Article 2 uses dataset from various domains. sent: Article 1 uses 4 topics for the datasets, while Article 2 uses 11 topics for the datasets. sent:  Article 1 48 participants were recruited, while Article 2 1100 search sessions were included.","sent: Article 1 and Article 2 use supervised learning approach. sent: Article 1 uses diffferent behaviour features, while Article 2 uses resource-centric features and behaviour featuress . sent: Article 1 uses regression models to predict user knowledge, while Article 2 uses Naive Bayes, Logistic Regression, SVM and Random Forest modesl to predict user knowledge."
6,yg_paper_2,yg_paper_4,42,44,"sent: Article 1 and  Article 2 use datasets sourced from TREC 2014 Web Track dataset. sent: Article 1 and Article use datasets from various domains. sent: Article 1 and Article 2 use 11 topics for the datasets. sent:  Article 1 468 participants were recruited, while Article 2 1100 search sessions were included.","sent: Article 1 and Article 2 use supervised learning approach. sent: Article 1 uses diffferent behaviour features, while Article 2 uses resource-centric features and behaviour featuress . sent: Article 1 uses regression models to predict user knowledge, while Article 2 uses Naive Bayes, Logistic Regression, SVM and Random Forest modesl to predict user knowledge."
7,ae_paper_1,ae_paper_2,51,52,sent: Article 1 and Article 2 use datasets sourced from SciDTB corpus from the ACL Anthology. sent: Article 1 and Article 2 use scientific datasets from Computational Linguistics domain. Article 1 and Article 2 use a subset of the SciDTB corpus of 60 abstracts.,"sent: Article 1 and Article 2 propose a fine-grained annotation schema for argument mining. sent: Article 1 and Article 2 use supervised learining approaches for identifying argumentative units, types, functions and attachments. sent: Article 1 uses non-neural (CRF) and neural (BiLSTM-ST for single task and BiLSTM-MT for multi task), while Article 2 uses transfere learning approach to improve the perfomance of argument mining model trained on small corpus. sent: Article 1: uses positional, syntactic and discourse features, while Article 2 uses DEmb, ELMo, Glove and RSTEnc embeddings."
8,ae_paper_1,ae_paper_3,51,53,sent: Article 1 and Article 2 use datasets sourced from SciDTB corpus from the ACL Anthology. sent: Article 1 and Article 2 use scientific datasets from Computational Linguistics domain. Article 1 and Article 2 use a subset of the SciDTB corpus of 60 abstracts.,"sent: Article 1 and Article 2 propose a fine-grained annotation schema for argument mining. sent: Article 1 and Article 2 use supervised learining approaches for identifying argumentative units, types, functions and attachments. sent: Article 1 uses non-neural (CRF) and neural (BiLSTM-ST for single task and BiLSTM-MT for multi task), while Article 2 uses two transfere learning approaches (multi-task learning and sequential learning) to improve the perfomance of argument mining model trained on small corpus. sent: Article 1: uses positional, syntactic and discourse features, while Article 2 uses DEmb, ELMo, and RSTEnc embeddings."
9,ae_paper_1,ae_paper_4,51,54,"sent: Article 1 uses datasets sourced from SciDTB corpus from the ACL Anthology, while Article 2 uses SciArg corpus sourced from ACL Anthology and MEDLINE/PubMed. sent: Article 1 uses scientific datasets from Computational Linguistics domain, while Article 2 uses scientific datasets from Computational Linguistics and Biomedical domain. Article 1 uses a subset of the SciDTB corpus of 60 abstracts, while Article 2 uses 510 scientific abstracts (225 from Computation Linguistics and 285 from Biomedicine).","sent: Article 1 proposes a fine-grained annotation schema for argument mining, while Article 2 uses a refined annotation schema on sentence level for argument mining. sent: Article 1 and Article 2 use supervised learining approaches for identifying argumentative units, types, functions and attachments. sent: Article 1 uses non-neural (CRF) and neural (BiLSTM-ST for single task and BiLSTM-MT for multi task), while Article 2 uses fine-tune and evaluate BERT-based argument mining models (in single and muti-task settings). sent: Article 1: uses positional, syntactic and discourse features, while Article 2 uses BERT representations."
10,ae_paper_2,ae_paper_3,52,53,sent: Article 1 and Article 2 use datasets sourced from SciDTB corpus from the ACL Anthology. sent: Article 1 and Article 2 use scientific datasets from Computational Linguistics domain. Article 1 and Article 2 use a subset of the SciDTB corpus of 60 abstracts.,"sent: Article 1 and Article 2 propose a fine-grained annotation schema for argument mining. sent: Article 1 and Article 2 use supervised learining approaches for identifying argumentative units, types, functions and attachments. sent: Article 1 uses transfere learning approach to improve the perfomance of argument mining model trained on small corpus, while Article 2 uses two transfere learning approaches (multi-task learning and sequential learning) to improve the perfomance of argument mining model trained on small corpus. sent: Article 1: DEmb, ELMo, Glove and RSTEnc embeddings, while Article 2 uses DEmb, ELMo, and RSTEnc embeddings."
11,ae_paper_2,ae_paper_4,52,54,"sent: Article 1 uses datasets sourced from SciDTB corpus from the ACL Anthology, while Article 2 uses SciArg corpus sourced from ACL Anthology and MEDLINE/PubMed. sent: Article 1 uses scientific datasets from Computational Linguistics domain, while Article 2 uses scientific datasets from Computational Linguistics and Biomedical domain. Article 1 uses a subset of the SciDTB corpus of 60 abstracts, while Article 2 uses 510 scientific abstracts (225 from Computation Linguistics and 285 from Biomedicine).","sent: Article 1 proposes a fine-grained annotation schema for argument mining, while Article 2 uses a refined annotation schema on sentence level for argument mining. sent: Article 1 and Article 2 use supervised learining approaches for identifying argumentative units, types, functions and attachments. sent: Article 1 uses transfere learning approach to improve the perfomance of argument mining model trained on small corpus, while Article 2 uses fine-tune and evaluate BERT-based argument mining models (in single and muti-task settings). sent: Article 1: DEmb, ELMo, Glove and RSTEnc embeddings, while Article 2 uses BERT representations."
12,ae_paper_3,ae_paper_4,53,54,"sent: Article 1 uses datasets sourced from SciDTB corpus from the ACL Anthology, while Article 2 uses SciArg corpus sourced from ACL Anthology and MEDLINE/PubMed. sent: Article 1 uses scientific datasets from Computational Linguistics domain, while Article 2 uses scientific datasets from Computational Linguistics and Biomedical domain. Article 1 uses a subset of the SciDTB corpus of 60 abstracts, while Article 2 uses 510 scientific abstracts (225 from Computation Linguistics and 285 from Biomedicine).","sent: Article 1 proposes a fine-grained annotation schema for argument mining, while Article 2 uses a refined annotation schema on sentence level for argument mining. sent: Article 1 and Article 2 use supervised learining approaches for identifying argumentative units, types, functions and attachments. sent: Article 1 uses two transfere learning approaches (multi-task learning and sequential learning) to improve the perfomance of argument mining model trained on small corpus, while Article 2 uses fine-tune and evaluate BERT-based argument mining models (in single and muti-task settings). sent: Article 1:  uses DEmb, ELMo, and RSTEnc embeddings, while Article 2 uses BERT representations."