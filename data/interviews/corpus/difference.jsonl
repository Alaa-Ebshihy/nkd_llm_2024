{"doc_id": 1112, "title": "", "abstract": ["Article 1 uses different datasets from the medical domain sourced mainly from PubMed, while Article 2 uses medical domain datasets sourced from different sources.", "Article 1 uses 20 systematic review datasets, while Article 2 uses 23 systematic review datasets.", "Article 1 and Article 2 use supervised learning approach.", "Article 1 uses TF-IDF, log-linear model and LDA topic modelling for feature extraction, while Article 2 uses deep denoinsing encoder and feed forward neural networks for feature extraction. Article 1 and Article 2 uses SVM classification for systematic reviews for binary relevance."], "structured": false}
{"doc_id": 1113, "title": "", "abstract": ["Article 1 and Article 2 uses different datasets from the medical domain sourced mainly from PubMed.", "Article 1 and Article 2 uses 20 systematic review datasets.", "Article 1 and Article 2 use supervised learning approach.", "Article 1 uses TF-IDF, log-linear model and LDA topic modelling for feature extraction, while Article 2 GLOVE word embeddings for feature extraction. Article 1 uses SVM classification for systematic reviews for binary relevance, while Articla 2 uses a Multi-Channel Convolutional Neural Network (CNN) clasification approach."], "structured": false}
{"doc_id": 1213, "title": "", "abstract": ["Article 1 uses medical domain datasets sourced from different sources,  Article 2 uses different datasets from the medical domain sourced mainly from PubMed.", "Article 1 uses 23 systematic review datasets, while Article 2 uses 20 systematic review datasets.", "Article 1 and Article 2 use supervised learning approach.", "Article 1 uses deep denoinsing encoder and feed forward neural networks for feature extraction, while Article 2 GLOVE word embeddings for feature extraction. Article 1 uses SVM classification for systematic reviews for binary relevance, while Articla 2 uses a Multi-Channel Convolutional Neural Network (CNN) clasification approach."], "structured": false}
{"doc_id": 3132, "title": "", "abstract": ["Article 1 and Article 2 use datasets sourced from AskApatient reviews and CADEC datasets sourced from SNOMED-CT.", "Article 1 and Article 2 use medical domain datasets for infomal and formal text.", "Article 1 and Article 2 use 5 folds of the datasets.", "Article 1 uses semantic similarity approach for mapping informal medical terms to formal terms, Article 2 uses deep learning classification approach for mapping informal medical terms to formal terms.", "Article 1 uses a bidirectional RNN with attention on top of the embedding layer (HealthVec and PubMedVec) for feature representation, while Article 2 uses HealthVec and Google News embeddings in combination  with ELMo ( HV, GN, GELMo, BELMo and CELMo) embeddings for feature representation.", "Article 1 uses cosine similarity, while Article 2 uses a BiLSTM model for classification."], "structured": false}
{"doc_id": 4142, "title": "", "abstract": ["Article 1 uses datasets sourced from TREC Genomics collection, while Article 2 uses datasets sourced from TREC 2014 Web Track dataset.", "Article 1 uses datasets from the gemomics domain, while Article 2 uses dataset from various domains.", "Article 1 uses 4 topics for the datasets, while Article 2 uses 11 topics for the datasets.", "Article 1 48 participants were recruited, while Article 2 468 participants were recruited.", "Article 1 and Article 2 use supervised learning approach.", "Article 1 and Article 2 use diffferent behaviour features.", "Article 1 and Article 2 use regression models to predict user knowledge."], "structured": false}
{"doc_id": 4144, "title": "", "abstract": ["Article 1 uses datasets sourced from TREC Genomics collection, while Article 2 uses datasets sourced from TREC 2014 Web Track dataset.", "Article 1 uses datasets from the gemomics domain, while Article 2 uses dataset from various domains.", "Article 1 uses 4 topics for the datasets, while Article 2 uses 11 topics for the datasets.", "Article 1 48 participants were recruited, while Article 2 1100 search sessions were included.", "Article 1 and Article 2 use supervised learning approach.", "Article 1 uses diffferent behaviour features, while Article 2 uses resource-centric features and behaviour featuress .", "Article 1 uses regression models to predict user knowledge, while Article 2 uses Naive Bayes, Logistic Regression, SVM and Random Forest modesl to predict user knowledge."], "structured": false}
{"doc_id": 4244, "title": "", "abstract": ["Article 1 and  Article 2 use datasets sourced from TREC 2014 Web Track dataset.", "Article 1 and Article use datasets from various domains.", "Article 1 and Article 2 use 11 topics for the datasets.", "Article 1 468 participants were recruited, while Article 2 1100 search sessions were included.", "Article 1 and Article 2 use supervised learning approach.", "Article 1 uses diffferent behaviour features, while Article 2 uses resource-centric features and behaviour featuress .", "Article 1 uses regression models to predict user knowledge, while Article 2 uses Naive Bayes, Logistic Regression, SVM and Random Forest modesl to predict user knowledge."], "structured": false}
{"doc_id": 5152, "title": "", "abstract": ["Article 1 and Article 2 use datasets sourced from SciDTB corpus from the ACL Anthology.", "Article 1 and Article 2 use scientific datasets from Computational Linguistics domain. Article 1 and Article 2 use a subset of the SciDTB corpus of 60 abstracts.", "Article 1 and Article 2 propose a fine-grained annotation schema for argument mining.", "Article 1 and Article 2 use supervised learining approaches for identifying argumentative units, types, functions and attachments.", "Article 1 uses non-neural (CRF) and neural (BiLSTM-ST for single task and BiLSTM-MT for multi task), while Article 2 uses transfere learning approach to improve the perfomance of argument mining model trained on small corpus.", "Article 1: uses positional, syntactic and discourse features, while Article 2 uses DEmb, ELMo, Glove and RSTEnc embeddings."], "structured": false}
{"doc_id": 5153, "title": "", "abstract": ["Article 1 and Article 2 use datasets sourced from SciDTB corpus from the ACL Anthology.", "Article 1 and Article 2 use scientific datasets from Computational Linguistics domain. Article 1 and Article 2 use a subset of the SciDTB corpus of 60 abstracts.", "Article 1 and Article 2 propose a fine-grained annotation schema for argument mining.", "Article 1 and Article 2 use supervised learining approaches for identifying argumentative units, types, functions and attachments.", "Article 1 uses non-neural (CRF) and neural (BiLSTM-ST for single task and BiLSTM-MT for multi task), while Article 2 uses two transfere learning approaches (multi-task learning and sequential learning) to improve the perfomance of argument mining model trained on small corpus.", "Article 1: uses positional, syntactic and discourse features, while Article 2 uses DEmb, ELMo, and RSTEnc embeddings."], "structured": false}
{"doc_id": 5154, "title": "", "abstract": ["Article 1 uses datasets sourced from SciDTB corpus from the ACL Anthology, while Article 2 uses SciArg corpus sourced from ACL Anthology and MEDLINE/PubMed.", "Article 1 uses scientific datasets from Computational Linguistics domain, while Article 2 uses scientific datasets from Computational Linguistics and Biomedical domain. Article 1 uses a subset of the SciDTB corpus of 60 abstracts, while Article 2 uses 510 scientific abstracts (225 from Computation Linguistics and 285 from Biomedicine).", "Article 1 proposes a fine-grained annotation schema for argument mining, while Article 2 uses a refined annotation schema on sentence level for argument mining.", "Article 1 and Article 2 use supervised learining approaches for identifying argumentative units, types, functions and attachments.", "Article 1 uses non-neural (CRF) and neural (BiLSTM-ST for single task and BiLSTM-MT for multi task), while Article 2 uses fine-tune and evaluate BERT-based argument mining models (in single and muti-task settings).", "Article 1: uses positional, syntactic and discourse features, while Article 2 uses BERT representations."], "structured": false}
{"doc_id": 5253, "title": "", "abstract": ["Article 1 and Article 2 use datasets sourced from SciDTB corpus from the ACL Anthology.", "Article 1 and Article 2 use scientific datasets from Computational Linguistics domain. Article 1 and Article 2 use a subset of the SciDTB corpus of 60 abstracts.", "Article 1 and Article 2 propose a fine-grained annotation schema for argument mining.", "Article 1 and Article 2 use supervised learining approaches for identifying argumentative units, types, functions and attachments.", "Article 1 uses transfere learning approach to improve the perfomance of argument mining model trained on small corpus, while Article 2 uses two transfere learning approaches (multi-task learning and sequential learning) to improve the perfomance of argument mining model trained on small corpus.", "Article 1: DEmb, ELMo, Glove and RSTEnc embeddings, while Article 2 uses DEmb, ELMo, and RSTEnc embeddings."], "structured": false}
{"doc_id": 5254, "title": "", "abstract": ["Article 1 uses datasets sourced from SciDTB corpus from the ACL Anthology, while Article 2 uses SciArg corpus sourced from ACL Anthology and MEDLINE/PubMed.", "Article 1 uses scientific datasets from Computational Linguistics domain, while Article 2 uses scientific datasets from Computational Linguistics and Biomedical domain. Article 1 uses a subset of the SciDTB corpus of 60 abstracts, while Article 2 uses 510 scientific abstracts (225 from Computation Linguistics and 285 from Biomedicine).", "Article 1 proposes a fine-grained annotation schema for argument mining, while Article 2 uses a refined annotation schema on sentence level for argument mining.", "Article 1 and Article 2 use supervised learining approaches for identifying argumentative units, types, functions and attachments.", "Article 1 uses transfere learning approach to improve the perfomance of argument mining model trained on small corpus, while Article 2 uses fine-tune and evaluate BERT-based argument mining models (in single and muti-task settings).", "Article 1: DEmb, ELMo, Glove and RSTEnc embeddings, while Article 2 uses BERT representations."], "structured": false}
{"doc_id": 5354, "title": "", "abstract": ["Article 1 uses datasets sourced from SciDTB corpus from the ACL Anthology, while Article 2 uses SciArg corpus sourced from ACL Anthology and MEDLINE/PubMed.", "Article 1 uses scientific datasets from Computational Linguistics domain, while Article 2 uses scientific datasets from Computational Linguistics and Biomedical domain. Article 1 uses a subset of the SciDTB corpus of 60 abstracts, while Article 2 uses 510 scientific abstracts (225 from Computation Linguistics and 285 from Biomedicine).", "Article 1 proposes a fine-grained annotation schema for argument mining, while Article 2 uses a refined annotation schema on sentence level for argument mining.", "Article 1 and Article 2 use supervised learining approaches for identifying argumentative units, types, functions and attachments.", "Article 1 uses two transfere learning approaches (multi-task learning and sequential learning) to improve the perfomance of argument mining model trained on small corpus, while Article 2 uses fine-tune and evaluate BERT-based argument mining models (in single and muti-task settings).", "Article 1:  uses DEmb, ELMo, and RSTEnc embeddings, while Article 2 uses BERT representations."], "structured": false}
