{
  "paper_id": "1701.06264",
  "sections": [
    {
      "section_name": "Abstract",
      "selected_sentences": [
        {
          "par_id": 0,
          "sentences": [
            {
              "sent": "In this paper, we present the Lipschitz regularization theory and algorithms for a novel LossSensitive Generative Adversarial Network (LSGAN).",
              "tag": "Method"
            },
            {
              "sent": "Specifically, it trains a loss function to distinguish between real and fake samples by designated margins, while learning a generator alternately to produce realistic samples by minimizing their losses.",
              "tag": "Method"
            },
            {
              "sent": "The LSGAN further regularizes its loss function with a Lipschitz regularity condition on the density of real data, yielding a regularized model that can better generalize to produce new data from a reasonable number of training examples than the classic GAN.",
              "tag": "Claim"
            },
            {
              "sent": "We will further present a Generalized LSGAN (GLSGAN) and show it contains a large family of regularized GAN models, including both LSGAN and Wasserstein GAN, as its special cases.",
              "tag": "Claim"
            },
            {
              "sent": "Compared with the other GAN models, we will conduct experiments to show both LSGAN and GLSGAN exhibit competitive ability in generating new images in terms of the Minimum Reconstruction Error (MRE) assessed on a separate test set.",
              "tag": "Other"
            },
            {
              "sent": "We further extend the LSGAN to a conditional form for supervised and semi-supervised learning problems, and demonstrate its outstanding performance on image classification tasks.",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Introduction",
      "selected_sentences": [
        {
          "par_id": 2,
          "sentences": [
            {
              "sent": "A classic Generative Adversarial Net (GAN) [13] learns a discriminator and a generator by playing a two-player minimax game to generate samples from a data distribution.",
              "tag": "Method"
            },
            {
              "sent": "The discriminator is trained to distinguish real samples from those generated by the generator, and it in turn guides the generator to produce realistic samples that can fool the discriminator.",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Paper Structure",
      "selected_sentences": [
        {
          "par_id": 4,
          "sentences": [
            {
              "sent": "The remainder of this paper is organized as follows.",
              "tag": "Claim"
            },
            {
              "sent": "Section 2 reviews the related work, and the proposed LSGAN is presented in Section 3. In Section 4, we will analyze the LSGAN by proving the distributional consistency between generated and real data with the Lipschitz regularity condition on the data distribution.",
              "tag": "Claim"
            },
            {
              "sent": "In Section 5, we will discuss the generalizability problem arising from using sample means to approximate the expectations in the training objectives.",
              "tag": "Claim"
            },
            {
              "sent": "We will make a comparison with Wasserstein GAN (WGAN) in Section 6.1, and present a generalized LSGAN with both WGAN and LSGAN as its special cases in Section 6.2.",
              "tag": "Method"
            },
            {
              "sent": "A non-parametric analysis of the algorithm is followed in Section 7. Then we will show how the model can be extended to a conditional model for both supervised and semi-supervised learning in Section 8. Experiment results are presented in Section 9, and we conclude in Section 10.",
              "tag": "Method"
            },
            {
              "sent": "The source codes for both LSGAN and GLSGAN are available at https:// github.com/maple-research-lab, in the frameworks of torch, pytorch and tensorflow.",
              "tag": "Claim"
            },
            {
              "sent": "LSGAN is also supported by Microsoft CNTK at https://www.cntk.ai/pythondocs/CNTK_206C_WGAN_",
              "tag": "Claim"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Related Work",
      "selected_sentences": []
    },
    {
      "section_name": "Loss-Sensitive GAN",
      "selected_sentences": [
        {
          "par_id": 11,
          "sentences": [
            {
              "sent": "On the contrary, in the LSGAN we seek to learn a loss function L \u03b8 (x) parameterized with \u03b8 by assuming that a real example ought to have a smaller loss than a generated sample by a desired margin.",
              "tag": "Method"
            },
            {
              "sent": "Then the generator can be trained to generate realistic samples by minimizing their losses.",
              "tag": "Method"
            }
          ]
        },
        {
          "par_id": 12,
          "sentences": [
            {
              "sent": "Formally, consider a generator function G \u03c6 that produces a sample G \u03c6 (z) by transforming a noise input z \u223c P z (z) drawn from a simple distribution P z such as uniform and Gaussian distributions.",
              "tag": "Method"
            },
            {
              "sent": "Then for a real example x and a generated sample G \u03c6 (z), the loss function can be trained to distinguish them with the following constraint:",
              "tag": "Result"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Theoretical Analysis: Distributional Consistency",
      "selected_sentences": [
        {
          "par_id": 27,
          "sentences": [
            {
              "sent": "Under Assumption 1, there exists a Nash equilibrium (\u03b8 * , \u03c6 * ) such that both L \u03b8 * and P G * are Lipschitz.",
              "tag": "Claim"
            },
            {
              "sent": "Now we can prove the main lemma of this paper.",
              "tag": "Conclusion"
            },
            {
              "sent": "The Lipschitz regularity relaxes the strong non-parametric assumption on the GAN's discriminator with infinite capacity to the above weaker Lipschitz assumption for the LSGAN.",
              "tag": "Claim"
            },
            {
              "sent": "This allows us to show the following lemma that establishes the distributional consistency between the optimal P G * by Problem (4)- (5) and the data density P data .",
              "tag": "Claim"
            },
            {
              "sent": "Lemma 2. Under Assumption 1, for a Nash equilibrium (\u03b8 * , \u03c6 * ) in Lemma 1, we have",
              "tag": "Claim"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Learning and Generalizability",
      "selected_sentences": []
    },
    {
      "section_name": "Generalizability",
      "selected_sentences": []
    },
    {
      "section_name": "Bounded Lipschitz Constants for Regularization",
      "selected_sentences": [
        {
          "par_id": 56,
          "sentences": [
            {
              "sent": "Our generalization theory in Theorem 2 conjectures that the required number of training examples is lower bounded by a polynomial of Lipschitz constants \u03ba L and \u03ba of the loss function wrt \u03b8 and x.",
              "tag": "Claim"
            },
            {
              "sent": "This suggests us to bound both constants to reduce the sample complexity of the LSGAN to improve its generalization performance.",
              "tag": "Claim"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Wasserstein GAN and Generalized LS-GAN",
      "selected_sentences": [
        {
          "par_id": 60,
          "sentences": [
            {
              "sent": "In this section, we discuss two issues about LSGAN.",
              "tag": "Claim"
            },
            {
              "sent": "First, we discuss its connection with the Wasserstein GAN (WGAN), and then show that the WGAN is a special case of a generalized form of LSGAN.",
              "tag": "Claim"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Comparison with Wasserstein GAN",
      "selected_sentences": [
        {
          "par_id": 61,
          "sentences": [
            {
              "sent": "We notice that the recently proposed Wasserstein GAN (WGAN) [2] uses the EarthMover (EM) distance to address the vanishing gradient and saturated JS distance problems in the classic GAN by showing the EM distance is continuous and differentiable almost everywhere.",
              "tag": "Claim"
            },
            {
              "sent": "While both the LSGAN and the WGAN address these problems from different perspectives that are independently developed almost simultaneously, both turn out to use the Lipschitz regularity in training their GAN models.",
              "tag": "Conclusion"
            },
            {
              "sent": "This constraint plays vital but different roles in the two models.",
              "tag": "Claim"
            },
            {
              "sent": "In the LSGAN, the Lipschitz regularity naturally arises from the Lipschitz assumption on the data density and the generalization bound.",
              "tag": "Claim"
            },
            {
              "sent": "Under this regularity condition, we have proved in Theorem 1 that the density of generated samples matches the underlying data density.",
              "tag": "Result"
            },
            {
              "sent": "On the contrary, the WGAN introduces the Lipschitz constraint from the KantorovichRubinstein duality of the EM distance but it is not proved in [2] if the density of samples generated by WGAN is consistent with that of real data.",
              "tag": "Conclusion"
            }
          ]
        },
        {
          "par_id": 65,
          "sentences": [
            {
              "sent": "This lemma shows both the LSGAN and the WGAN are based on the same Lipschitz regularity condition.",
              "tag": "Conclusion"
            }
          ]
        },
        {
          "par_id": 67,
          "sentences": [
            {
              "sent": "On the contrary, the LSGAN treats real and generated examples in pairs, and maximizes the difference of their losses up to a data-dependant margin.",
              "tag": "Claim"
            },
            {
              "sent": "Specifically, as shown in the second term of Eq. ( 4), when the loss of a generated sample z G becomes too large wrt that of a paired real example x, the maximization of",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 70,
          "sentences": [
            {
              "sent": "Below we discuss a Generalized LSGAN (GLSGAN) model in Section 6.2, and show that both WGAN and LSGAN are simply two special cases of this GLSGAN.",
              "tag": "Claim"
            }
          ]
        }
      ]
    },
    {
      "section_name": "GLS-GAN: Generalized LS-GAN",
      "selected_sentences": [
        {
          "par_id": 81,
          "sentences": [
            {
              "sent": "In experiments, we will demonstrate the GLSGAN has competitive generalization performance on generating new images (c.f.",
              "tag": "Claim"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Non-Parametric Analysis",
      "selected_sentences": []
    },
    {
      "section_name": "Corollary 1. All the functions in",
      "selected_sentences": []
    },
    {
      "section_name": "The parameters \u03b8",
      "selected_sentences": []
    },
    {
      "section_name": "Conditional LS-GAN",
      "selected_sentences": [
        {
          "par_id": 107,
          "sentences": [
            {
              "sent": "To prove this, we say a loss function L \u03b8 (x, y) is Lipschitz if it is Lipschitz continuous in its first argument x.",
              "tag": "Method"
            },
            {
              "sent": "We also impose the following regularity condition on the conditional density P data (x|y).",
              "tag": "Method"
            },
            {
              "sent": "Assumption 4. For each y, the conditional density P data (x|y) is Lipschitz, and is supported in a convex compact set of x.",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Semi-Supervised LS-GAN",
      "selected_sentences": []
    },
    {
      "section_name": "Experiments",
      "selected_sentences": [
        {
          "par_id": 119,
          "sentences": [
            {
              "sent": "First, we will assess the quality of generated images by the LSGAN in comparison with the classic GAN model.",
              "tag": "Method"
            },
            {
              "sent": "Then, we will make an objective evaluation on the CLSGAN to classify images.",
              "tag": "Method"
            },
            {
              "sent": "This task evaluates the quality of feature representations learned by the CLSGAN in terms of its classification accuracy directly.",
              "tag": "Method"
            }
          ]
        },
        {
          "par_id": 120,
          "sentences": [
            {
              "sent": "Finally, we will assess the generalizability of various GAN models in generating new images out of training examples by proposing the Minimum Reconstruction Error (MRE) on a separate test set.",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Architectures",
      "selected_sentences": []
    },
    {
      "section_name": "Training Details",
      "selected_sentences": [
        {
          "par_id": 127,
          "sentences": [
            {
              "sent": "For the generator network of LSGAN, it took a 100-dimensional random vector drawn from Unif[\u22121, 1] as input.",
              "tag": "Method"
            },
            {
              "sent": "For the CLSGAN generator, an one-hot vector encoding the image class condition was concatenated with the sampled random vector.",
              "tag": "Method"
            },
            {
              "sent": "The CLSGAN was trained by involving both unlabeled and labeled examples as in Section 8.",
              "tag": "Method"
            },
            {
              "sent": "This was compared against the other state-of-the-art supervised and semi-supervised models.",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Generated Images by LS-GAN",
      "selected_sentences": []
    },
    {
      "section_name": "Image Classification",
      "selected_sentences": []
    },
    {
      "section_name": "CIFAR-10",
      "selected_sentences": []
    },
    {
      "section_name": "Methods",
      "selected_sentences": [
        {
          "par_id": 142,
          "sentences": [
            {
              "sent": "We also compared with the other recently developed supervised and semi-supervised models in literature, including the baseline 1 Layer K-means feature extraction pipeline, a multi-layer extension of the baseline model (3 Layer K-means Learned RF [7]), View Invariant K-means [16], Examplar CNN [9], Ladder Network [30], as well as CatGAN [33].",
              "tag": "Result"
            },
            {
              "sent": "In particular, among the compared semi-supervised algorithms, the improved GAN [31] had recorded the best performance in literature.",
              "tag": "Result"
            },
            {
              "sent": "Furthermore, we also compared with the ALI [11] that extended the classic GAN by jointly generating data and inferring their representations, which achieved comparable performance to the Improved GAN.",
              "tag": "Other"
            },
            {
              "sent": "This pointed out an interesting direction to extend the CLSGAN by directly inferring the data representation, and we will leave it in the future work.",
              "tag": "Claim"
            }
          ]
        }
      ]
    },
    {
      "section_name": "SVHN",
      "selected_sentences": []
    },
    {
      "section_name": "Analysis of Generated Images by CLS-GAN",
      "selected_sentences": []
    },
    {
      "section_name": "Evaluation of Generalization Performances",
      "selected_sentences": [
        {
          "par_id": 149,
          "sentences": [
            {
              "sent": "Most of existing metrics like Inception Score [31] for evaluating GAN models focus on comparing the qualities and diversities of their generated images.",
              "tag": "Claim"
            },
            {
              "sent": "However, even though a GAN model can produce diverse and high quality images with no collapsed generators, it is still unknown if the model can generate unseen images out of given examples, or simply memorizing existing ones.",
              "tag": "Claim"
            },
            {
              "sent": "While one of our main pursuits in this paper is a generalizable LSGAN, we were motivated to propose the following Minimum Reconstruction Error (MRE) to compare its generalizability with various GANs.",
              "tag": "Method"
            }
          ]
        },
        {
          "par_id": 152,
          "sentences": [
            {
              "sent": "In Figure 9, we compare the test MREs over 100 epochs by LSGAN, GLSGAN, WGAN [2],  WGANGP [15] and DCGAN [29] on CIFAR-10 respectively.",
              "tag": "Method"
            },
            {
              "sent": "For the sake of a fair comparison, all models were trained with the network architecture used in [29].",
              "tag": "Result"
            },
            {
              "sent": "The result clearly shows the regularized models, including GLSGAN, LSGAN, WGANGP and WGAN, have apparently better generalization performances than the unregularized DCGAN based on the classic GAN model.",
              "tag": "Result"
            },
            {
              "sent": "On CIFAR-10, the test MRE was reduced from 0.1506 by DCGAN to as small as 0.1109 and 0.1089 by WGAN and GLSGAN respectively; on tiny ImageNet, the GLSGAN reaches the smallest test MRE of 0.2085 among all compared regularized and unregularized GANs.",
              "tag": "Result"
            }
          ]
        },
        {
          "par_id": 154,
          "sentences": [
            {
              "sent": "We illustrate some examples of reconstructed images by different GANs on the test set along with their test MREs in Figure 11.",
              "tag": "Result"
            },
            {
              "sent": "The results show the GLSGAN achieved the smallest test MRE of 0.1089 and 0.2085 with a LeakyReLU cost function of slope 0.01 and 0.5 on CIFAR-10 and tiny ImageNet, followed by the other regularized GAN models.",
              "tag": "Result"
            },
            {
              "sent": "This is not a surprising result since it has been shown in Section 6.2 that the other regularized GANs such as LSGAN and WGAN are only special cases of the GLSGAN model that covers larger family of models.",
              "tag": "Method"
            },
            {
              "sent": "Here we only considered LeakyReLU as the cost function for GLSGAN.",
              "tag": "Method"
            },
            {
              "sent": "Of course, there exist many more cost functions satisfying the two conditions in Section 6.2 to expand the family of regularized GANs, which should have potentials of yielding even better generalization performances.",
              "tag": "Claim"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Conclusions",
      "selected_sentences": [
        {
          "par_id": 155,
          "sentences": [
            {
              "sent": "In this paper, we present a novel LossSensitive GAN (LSGAN) approach to generate samples from a data distribution.",
              "tag": "Claim"
            },
            {
              "sent": "The LSGAN learns a loss function to distinguish between generated and real samples, where the loss of a real sample should be smaller by a margin than that of a generated sample.",
              "tag": "Method"
            },
            {
              "sent": "Our theoretical analysis shows the distributional consistency between the real and generated samples based on the Lipschitz regularity.",
              "tag": "Conclusion"
            },
            {
              "sent": "This no longer needs a non-parametric discriminator with infinite modeling ability in the classic GAN, allowing us to search for the optimal loss function in a smaller functional space with a bounded Lipschitz constant.",
              "tag": "Claim"
            },
            {
              "sent": "Moreover, we prove the generalizability of LSGAN by showing its required number of training examples is polynomial in its complexity.",
              "tag": "Result"
            },
            {
              "sent": "This suggests the generalization performance can be improved by penalizing the Lipschitz constants (via their gradient surrogates) of the loss function to reduce the sample complexity.",
              "tag": "Result"
            },
            {
              "sent": "Furthermore, our non-parametric analysis of the optimal loss function shows its lower and upper bounds are cone-shaped with non-vanishing gradient almost everywhere, implying the generator can be continuously updated even if the loss function is over-trained.",
              "tag": "Result"
            },
            {
              "sent": "Finally, we extend the LSGAN to a Conditional LSGAN (CLSGAN) for semi-supervised tasks, and demonstrate it reaches competitive performances on both image generation and classification tasks.",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "A Proof of Lemma 2",
      "selected_sentences": []
    },
    {
      "section_name": "C Proof of Theorem 2",
      "selected_sentences": []
    }
  ],
  "title": "Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities"
}