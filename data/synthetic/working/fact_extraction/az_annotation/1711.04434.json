{
  "paper_id": "1711.04434",
  "sections": [
    {
      "section_name": "Abstract",
      "selected_sentences": [
        {
          "par_id": 0,
          "sentences": [
            {
              "sent": "Unlike extractive summarization, abstractive summarization has to fuse different parts of the source text, which inclines to create fake facts.",
              "tag": "Result"
            },
            {
              "sent": "Our preliminary study reveals nearly 30% of the outputs from a state-of-the-art neural summarization system suffer from this problem.",
              "tag": "Result"
            },
            {
              "sent": "While previous abstractive summarization approaches usually focus on the improvement of informativeness, we argue that faithfulness is also a vital prerequisite for a practical abstractive summarization system.",
              "tag": "Method"
            },
            {
              "sent": "To avoid generating fake facts in a summary, we leverage open information extraction and dependency parse technologies to extract actual fact descriptions from the source text.",
              "tag": "Method"
            },
            {
              "sent": "The dual-attention sequence-to-sequence framework is then proposed to force the generation conditioned on both the source text and the extracted fact descriptions.",
              "tag": "Method"
            },
            {
              "sent": "Experiments on the Gigaword benchmark dataset demonstrate that our model can greatly reduce fake summaries by 80%.",
              "tag": "Method"
            },
            {
              "sent": "Notably, the fact descriptions also bring significant improvement on informativeness since they often condense the meaning of the source text.",
              "tag": "Result"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Introduction",
      "selected_sentences": [
        {
          "par_id": 2,
          "sentences": [
            {
              "sent": "The exponentially growing online information has necessitated the development of effective automatic summarization systems.",
              "tag": "Claim"
            },
            {
              "sent": "In this paper, we focus on an increasingly intriguing task, ie, abstractive sentence summarization (Rush, Chopra, and Weston 2015a) which generates a shorter version of a given sentence while attempting to preserve its original meaning.",
              "tag": "Claim"
            },
            {
              "sent": "This task is different from documentlevel summarization since it is hard to apply the common extractive techniques (Over and Yen 2004).",
              "tag": "Claim"
            },
            {
              "sent": "Selecting existing sentences to form the sentence summary is impossible.",
              "tag": "Claim"
            },
            {
              "sent": "Early studies on sentence summarization involve handcrafted rules (Zajic et al 2007), syntactic tree pruning (Knight and Marcu 2002) and statistical machine translation techniques (Banko, Mittal, and Witbrock 2000).",
              "tag": "Claim"
            },
            {
              "sent": "Recently, the application of the attentional sequence-tosequence (s2s) framework has attracted growing attention in this area (Rush, Chopra, and Weston 2015a;Chopra et al 2016;Nallapati et al 2016).",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 4,
          "sentences": [
            {
              "sent": "As we know, sentence summarization inevitably needs to fuse different parts in the source sentence and is abstractive.",
              "tag": "Claim"
            },
            {
              "sent": "Consequently, the generated summaries often mismatch with the original relations and yield fake facts.",
              "tag": "Result"
            },
            {
              "sent": "Our preliminary study reveals that nearly 30% of the outputs from a state-of-the-art s2s system suffer from this problem.",
              "tag": "Claim"
            },
            {
              "sent": "Previous researches are usually devoted to increasing summary informativeness.",
              "tag": "Claim"
            },
            {
              "sent": "However, one of the most essential prerequisites for a practical abstractive summarization system is that the generated summaries must accord with the facts expressed in the source.",
              "tag": "Claim"
            },
            {
              "sent": "We refer to this aspect as summary faithfulness in this paper.",
              "tag": "Claim"
            },
            {
              "sent": "A fake summary may greatly misguide the comprehension of the original text.",
              "tag": "Claim"
            },
            {
              "sent": "Look at an illustrative example of the generation result using the state-of-the-art s2s model (Nallapati et al 2016) in Table 1.",
              "tag": "Claim"
            },
            {
              "sent": "The actual subject of the verb \"postponed\" is \"repatriation\".",
              "tag": "Claim"
            },
            {
              "sent": "Nevertheless, probably because the entity \"bosnian moslems\" is closer to \"postponed\" in the source sentence, the summarization system wrongly regards \"bosnian moslems\" as the subject and counterfeits a fact \"bosnian moslems postponed\".",
              "tag": "Claim"
            },
            {
              "sent": "Meanwhile, the s2s system generates another fake fact: \"unhcr pulled out of bosnia\" and puts it into the summary.",
              "tag": "Result"
            },
            {
              "sent": "Consequently, although the informativeness (ROUGE-1 F1=0.57) and readability of this summary are high, its meaning departs far from the original.",
              "tag": "Result"
            },
            {
              "sent": "This sort of summaries is nearly useless in practice.",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 5,
          "sentences": [
            {
              "sent": "Since the fact fabrication is a serious problem, intuitively, encoding existing facts into the summarization system should be an ideal solution to avoid fake generation.",
              "tag": "Method"
            },
            {
              "sent": "To achieve this goal, the first step is to extract the facts from the source sentence.",
              "tag": "Claim"
            },
            {
              "sent": "In the relatively mature task of Open Information Extraction (OpenIE) (Banko et al 2007), a fact is usually represented by a relation triple consisting of (subject; predicate; object).",
              "tag": "Claim"
            },
            {
              "sent": "For example, given the source sentence in Table 1, the popular OpenIE tool (Angeli, Premkumar, and Manning 2015) generates two relation triples including (repatriation; was postponed; friday) and (unhcr; pulled out of; first joint scheme).",
              "tag": "Claim"
            },
            {
              "sent": "Obviously, these triples can help rectify the mistakes made by the s2s model.",
              "tag": "Claim"
            },
            {
              "sent": "However, the relation triples are not always extractable, eg, from the imperative sentences.",
              "tag": "Method"
            },
            {
              "sent": "Hence, we further adopt a dependency parser and supplement with the (subject; predicate) and (predicate; object) tuples identified from the parse tree of the sentence.",
              "tag": "Method"
            },
            {
              "sent": "This is also inspired by the work of parse tree based sentence compression (eg, (Knight and Marcu 2002)).",
              "tag": "Method"
            },
            {
              "sent": "We represent a fact through merging words in a triple or tuples to form a short sentence, defined as a fact description.",
              "tag": "Method"
            },
            {
              "sent": "Fact descriptions actually form the skeletons of sentences.",
              "tag": "Method"
            },
            {
              "sent": "Thus we incorporate them as an additional input source text in our model.",
              "tag": "Result"
            },
            {
              "sent": "Our experiments reveal that the words in the extracted fact descriptions are 40% more likely to be included in the actual summaries than the entire words in the source sentences.",
              "tag": "Result"
            },
            {
              "sent": "That is, fact descriptions clearly provide the right guidance for summarization.",
              "tag": "Method"
            },
            {
              "sent": "Next, using both source sentence and fact descriptions as input, we extend the state-of-the-art attentional s2s model (Nallapati et al 2016) to fully leverage their information.",
              "tag": "Method"
            },
            {
              "sent": "Specially, we use two Recurrent Neural Network (RNN) encoders to read the sentence and fact descriptions in parallel.",
              "tag": "Method"
            },
            {
              "sent": "With respective attention mechanisms, our model computes the sentence and fact context vectors.",
              "tag": "Method"
            },
            {
              "sent": "It then merges the two vectors according to their relative reliabilities.",
              "tag": "Method"
            },
            {
              "sent": "Finally, a RNN decoder makes use of the integrated context to generate the summary wordby-word.",
              "tag": "Method"
            },
            {
              "sent": "Since our summarization system encodes facts to enhance faithfulness, we call it FTSum.",
              "tag": "Method"
            }
          ]
        },
        {
          "par_id": 6,
          "sentences": [
            {
              "sent": "To verify the effectiveness of FTSum, we conduct extensive experiments on the Gigaword sentence summarization benchmark dataset (Rush, Chopra, and Weston 2015b).",
              "tag": "Method"
            },
            {
              "sent": "The results show that our model greatly reduces the fake summaries by 80% compared to the state-of-the-art s2s framework.",
              "tag": "Result"
            },
            {
              "sent": "Due to the compression nature of fact descriptions, the use of them also brings the significant improvement in terms of automatic informativeness evaluation.",
              "tag": "Conclusion"
            },
            {
              "sent": "The contributions of our work can be summarized as follows:",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 7,
          "sentences": [
            {
              "sent": "\u2022 To the best of our knowledge, we are the first to explore the faithfulness problem of abstractive summarization.",
              "tag": "Claim"
            },
            {
              "sent": "\u2022 We propose a dual-attention s2s model to push the generation to follow the original facts.",
              "tag": "Claim"
            },
            {
              "sent": "\u2022 Since the fact descriptions often condense the meaning of the source sentence, they also bring the significant benefit to promote informativeness.",
              "tag": "Result"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Fact Description Extraction",
      "selected_sentences": [
        {
          "par_id": 8,
          "sentences": [
            {
              "sent": "Based on our observation, 30% of summaries generated by state-of-the-art s2s models suffer from fact fabrication, such as the mismatch between the predicate and its subject or object.",
              "tag": "Claim"
            },
            {
              "sent": "Therefore, we propose to explicitly encode existing fact descriptions into the model.",
              "tag": "Claim"
            },
            {
              "sent": "OpenIE is able to give a complete description of the entity relations.",
              "tag": "Claim"
            },
            {
              "sent": "However, it is worth noting that, the relation triples are not always extractable, eg, from the imperative sentences.",
              "tag": "Result"
            },
            {
              "sent": "In fact, about 15% of the OpenIE outputs are empty on our dataset.",
              "tag": "Result"
            },
            {
              "sent": "These empty instances are likely to damage the robustness of our model.",
              "tag": "Result"
            },
            {
              "sent": "As observed, although the complete relation triples are not always available, the (subject; predicate) or (predicate; object) tuples are almost present in each sentence.",
              "tag": "Method"
            },
            {
              "sent": "Therefore, we leverage the dependency parser to dig out the appropriate tuples to supplement the fact descriptions.",
              "tag": "Method"
            },
            {
              "sent": "A dependency parser converts a sentence into the labeled (governor; dependent) tuples.",
              "tag": "Method"
            },
            {
              "sent": "We extract the predicate-related tuples according to the labels: nsubj, nsubjpass, csubj, csubjpass and dobj.",
              "tag": "Method"
            },
            {
              "sent": "To acquire more complete fact descriptions, we also reserve the important modifiers including the adjectival (amod), numeric (nummod) and noun compound (compound).",
              "tag": "Method"
            },
            {
              "sent": "We then merge the tuples containing the same words, and order words based on the original sentence to form the fact descriptions.",
              "tag": "Method"
            },
            {
              "sent": "Take the dependency tree in Figure 1 as an example.",
              "tag": "Method"
            },
            {
              "sent": "The output of OpenIE is empty for this sentence.",
              "tag": "Method"
            },
            {
              "sent": "Based on the dependency parser, we firstly filter the following predicate-related tuples: (prices; opened) (opened; tuesday) (dealers; said) and the modify-head tuples: (taiwan; price) (share; price) (lower; tuesday).",
              "tag": "Method"
            },
            {
              "sent": "These tuples are then merged to form two fact descriptions: taiwan share prices opened lower tuesday ||| dealers said.",
              "tag": "Method"
            }
          ]
        },
        {
          "par_id": 9,
          "sentences": [
            {
              "sent": "In the experiments, we employ the popular NLP pipeline Stanford CoreNLP (Manning et al 2014) to handle Ope-nIE and dependency parse at the same time.",
              "tag": "Method"
            },
            {
              "sent": "We combine the fact descriptions derived from both parts, and screen out the fact descriptions with the pattern \"somebody said/declared/announced\", which are usually meaningless  2008).",
              "tag": "Method"
            },
            {
              "sent": "We extract the following two fact descriptions: taiwan share prices opened lower tuesday ||| dealers said and insignificant.",
              "tag": "Result"
            },
            {
              "sent": "Referring to the copy ratios in Table 3, words in fact descriptions are 40% more likely to be used in the summary than the words in the original sentence.",
              "tag": "Result"
            },
            {
              "sent": "It indicates that fact descriptions truly condense the meaning of sentences to a large extent.",
              "tag": "Result"
            },
            {
              "sent": "The above statistics also supports the practice of dependency parse based compressive summarization (Knight and Marcu 2002).",
              "tag": "Result"
            },
            {
              "sent": "However, the length sum of extracted fact descriptions is shorter than the actual summary in 20% of the sentences, and 4% of the sentences even hold empty fact descriptions.",
              "tag": "Result"
            },
            {
              "sent": "In addition, from Table 3 we can find that on average one key source word is missing in the fact descriptions.",
              "tag": "Result"
            },
            {
              "sent": "Thus, without the source sentence, we cannot reply on fact descriptions alone to generate summaries.",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Fact Aware Neural Summarization Model Framework",
      "selected_sentences": [
        {
          "par_id": 14,
          "sentences": [
            {
              "sent": "With the respective attention mechanisms, our model computes the sentence and relation context vectors (c x t and c r t ) at each decoding time step t.",
              "tag": "Method"
            },
            {
              "sent": "The gate network is followed to merge the context vectors according to their relative associations with the current generation.",
              "tag": "Method"
            },
            {
              "sent": "The decoder produces summaries y = (y 1 , \u2022 \u2022 \u2022 y l ) word-by-word conditioned on the tailored context vector which embeds the semantics of both source sentence and fact descriptions.",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Encoders",
      "selected_sentences": [
        {
          "par_id": 15,
          "sentences": [
            {
              "sent": "The input includes the source sentence x and the fact descriptions r.",
              "tag": "Method"
            },
            {
              "sent": "For each sequence, we employ the bidirectional Gated Recurrent Unit (BiGRU) encoder (Cho et al 2014), to construct its semantic representation.",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Dual-Attention Decoder",
      "selected_sentences": []
    },
    {
      "section_name": "Learning",
      "selected_sentences": []
    },
    {
      "section_name": "Experiments Datasets",
      "selected_sentences": [
        {
          "par_id": 31,
          "sentences": [
            {
              "sent": "We conduct experiments on the Annotated English Gigaword corpus, as with (Rush, Chopra, and Weston 2015b).",
              "tag": "Method"
            },
            {
              "sent": "This parallel corpus is produced by pairing the first sentence in the news article and its headline as the summary with heuristic rules.",
              "tag": "Method"
            },
            {
              "sent": "The training and development datasets are built through the script 1 released by (Rush, Chopra, and Weston 2015b).",
              "tag": "Method"
            },
            {
              "sent": "The script also performs various basic text normalization, including tokenization, lower-casing, replacing all digit characters with #, and mask the words appearing less than 5 times with a UNK tag.",
              "tag": "Method"
            },
            {
              "sent": "It comes up with about 3.8M sentence-headline pairs as the training set and 189K pairs as the development set.",
              "tag": "Method"
            },
            {
              "sent": "We use the same Gigaword test set as (Rush, Chopra, and Weston 2015b).",
              "tag": "Method"
            },
            {
              "sent": "Following (Rush, Chopra, and Weston 2015a), we remove pairs with empty titles, leading to slightly different accuracy compared with (Rush, Chopra, and Weston 2015b).",
              "tag": "Method"
            },
            {
              "sent": "The statistics of the Gigaword corpus is presented in Table 4.",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Evaluation Metric",
      "selected_sentences": []
    },
    {
      "section_name": "Implementation Details",
      "selected_sentences": []
    },
    {
      "section_name": "Baselines",
      "selected_sentences": []
    },
    {
      "section_name": "Informativeness Evaluation",
      "selected_sentences": []
    },
    {
      "section_name": "Faithfulness Evaluation",
      "selected_sentences": [
        {
          "par_id": 38,
          "sentences": [
            {
              "sent": "For the sake of a complete comparison, we present the results of our system FTSum g together with the the attentional s2s model s2s+att.",
              "tag": "Result"
            },
            {
              "sent": "As shown in s2s-att outputs gives disinformation.",
              "tag": "Result"
            },
            {
              "sent": "This number greatly reduces to 6% by our model.",
              "tag": "Result"
            },
            {
              "sent": "Nearly 90% of summaries generated by our model is faithful, which makes our model far more practical.",
              "tag": "Result"
            },
            {
              "sent": "We find that s2s-att tends to copy the words closer to the predicate and regard them as its subject and object.",
              "tag": "Result"
            },
            {
              "sent": "However, this is not always reasonable and thus it is actually counterfeiting messages.",
              "tag": "Claim"
            },
            {
              "sent": "In comparison, the fact descriptions indeed designate the relations between a predicate and its subject and object.",
              "tag": "Claim"
            },
            {
              "sent": "As a result, generation in line with the fact descriptions is usually able to keep the faithfulness.",
              "tag": "Result"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Gate Analysis",
      "selected_sentences": [
        {
          "par_id": 45,
          "sentences": [
            {
              "sent": "The extracted fact description itself is already a proper summary.",
              "tag": "Result"
            },
            {
              "sent": "That is why fact descriptions are particularly preferred in generation.",
              "tag": "Claim"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Related Work",
      "selected_sentences": [
        {
          "par_id": 46,
          "sentences": [
            {
              "sent": "Abstractive sentence summarization (Chopra et al 2016) aims to produce a shorter version of a given sentence while preserving its meaning.",
              "tag": "Claim"
            },
            {
              "sent": "Unlike document-level summarization, it is impossible for this task to apply the common extractive techniques (eg, (Cao et al 2015a;).",
              "tag": "Claim"
            },
            {
              "sent": "Early studies for sentence summarization included rule-based methods (Zajic et al 2007), syntactic tree pruning (Knight and Marcu 2002) and statistical machine translation techniques (Banko, Mittal, and Witbrock 2000).",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 49,
          "sentences": [
            {
              "sent": "Notably, previous researches usually focused on the improvement of summary informativeness.",
              "tag": "Claim"
            },
            {
              "sent": "To the best of our knowledge, we are the first to explore the faithfulness problem of abstractive summarization.",
              "tag": "Claim"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Conclusion and Future Work",
      "selected_sentences": [
        {
          "par_id": 50,
          "sentences": [
            {
              "sent": "This paper investigates the faithfulness problem in abstractive summarization.",
              "tag": "Method"
            },
            {
              "sent": "We employ popular OpenIE and dependency parse tools to extract fact descriptions in the source sentence.",
              "tag": "Method"
            },
            {
              "sent": "Then, we propose the dual-attention s2s framework to force the generation conditioned on both source sentence and the fact descriptions.",
              "tag": "Method"
            },
            {
              "sent": "Experiments on the Gigaword benchmark demonstrate that our model greatly reduce fake summaries by 80%.",
              "tag": "Method"
            },
            {
              "sent": "In addition, since the fact descriptions often condense the meaning of the sentence, the import of them also brings significant improvement on informativeness.",
              "tag": "Result"
            }
          ]
        }
      ]
    }
  ],
  "title": "Faithful to the Original: Fact Aware Neural Abstractive Summarization"
}