{
  "paper_id": "1711.02132",
  "sections": [
    {
      "section_name": "Abstract",
      "selected_sentences": [
        {
          "par_id": 0,
          "sentences": [
            {
              "sent": "State-of-the-art results on neural machine translation often use attentional sequence-to-sequence models with some form of convolution or recursion.",
              "tag": "Claim"
            },
            {
              "sent": "Vaswani et al (2017) propose a new architecture that avoids recurrence and convolution completely.",
              "tag": "Claim"
            },
            {
              "sent": "Instead, it uses only self-attention and feed-forward layers.",
              "tag": "Method"
            },
            {
              "sent": "While the proposed architecture achieves state-of-the-art results on several machine translation tasks, it requires a large number of parameters and training iterations to converge.",
              "tag": "Result"
            },
            {
              "sent": "We propose Weighted Transformer, a Transformer with modified attention layers, that not only outperforms the baseline network in BLEU score but also converges 15 \u2212 40% faster.",
              "tag": "Claim"
            },
            {
              "sent": "Specifically, we replace the multi-head attention by multiple self-attention branches that the model learns to combine during the training process.",
              "tag": "Method"
            },
            {
              "sent": "Our model improves the state-of-the-art performance by 0.5 BLEU points on the WMT 2014 English-toGerman translation task and by 0.4 on the English-toFrench translation task.",
              "tag": "Result"
            }
          ]
        }
      ]
    },
    {
      "section_name": "INTRODUCTION",
      "selected_sentences": [
        {
          "par_id": 2,
          "sentences": [
            {
              "sent": "Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs) (Hochreiter & Schmidhuber, 1997), form an important building block for many tasks that require modeling of sequential data.",
              "tag": "Claim"
            },
            {
              "sent": "RNNs have been successfully employed for several such tasks including language modeling (Melis et al, 2017;Merity et al, 2017), speech recognition Graves et al, 2013), and machine translation (Wu et al, 2016;.",
              "tag": "Method"
            },
            {
              "sent": "RNNs make output predictions at each time step by computing a hidden state vector h t based on the current input token and the previous states.",
              "tag": "Claim"
            },
            {
              "sent": "This sequential computation underlies their ability to map arbitrary inputoutput sequence pairs.",
              "tag": "Claim"
            },
            {
              "sent": "However, because of their auto-regressive property of requiring previous hidden states to be computed before the current time step, they cannot benefit from parallelization.",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 4,
          "sentences": [
            {
              "sent": "In Vaswani et al (2017), the authors introduce the Transformer network, a novel architecture that avoids the recurrence equation and maps the input sequences into hidden states solely using attention.",
              "tag": "Claim"
            },
            {
              "sent": "Specifically, the authors use positional encodings in conjunction with a multi-head attention mechanism.",
              "tag": "Method"
            },
            {
              "sent": "This allows for increased parallel computation and reduces time to convergence.",
              "tag": "Method"
            },
            {
              "sent": "The authors report results for neural machine translation that show the Transformer networks achieves state-of-the-art performance on the WMT 2014 English-toGerman and English-toFrench tasks while being orders-of-magnitude faster than prior approaches.",
              "tag": "Result"
            }
          ]
        },
        {
          "par_id": 5,
          "sentences": [
            {
              "sent": "Transformer networks still require a large number of parameters to achieve state-of-the-art performance.",
              "tag": "Claim"
            },
            {
              "sent": "In the case of the newstest2013 English-toGerman translation task, the base model required 65M parameters, and the large model required 213M parameters.",
              "tag": "Claim"
            },
            {
              "sent": "We propose a variant of the Transformer network which we call Weighted Transformer that uses self-attention branches in lieu of the multi-head attention.",
              "tag": "Claim"
            },
            {
              "sent": "The branches replace the multiple heads in the attention mechanism of the original Transformer network, and the model learns to combine these branches during training.",
              "tag": "Method"
            },
            {
              "sent": "This branched architecture enables the network to achieve comparable performance at a significantly lower computational cost.",
              "tag": "Result"
            },
            {
              "sent": "Indeed, through this modification, we improve the state-of-the-art performance by 0.5 and 0.4 BLEU scores on the WMT 2014 English-toGerman and English-toFrench tasks, respectively.",
              "tag": "Result"
            },
            {
              "sent": "Finally, we present evidence that suggests a regularizing effect of the proposed architecture.",
              "tag": "Claim"
            }
          ]
        }
      ]
    },
    {
      "section_name": "RELATED WORK",
      "selected_sentences": [
        {
          "par_id": 6,
          "sentences": [
            {
              "sent": "Most architectures for neural machine translation (NMT) use an encoder and a decoder that rely on deep recurrent neural networks like the LSTM (Luong et al, 2015;Wu et al, 2016;Barone et al, 2017;.",
              "tag": "Claim"
            },
            {
              "sent": "Several architectures have been proposed to reduce the computational load associated with recurrence-based computation (Gehring et al, 2016;2017;Kaiser & Bengio, 2016;Kalchbrenner et al, 2016).",
              "tag": "Claim"
            },
            {
              "sent": "Self-attention, which relies on dot-products between elements of the input sequence to compute a weighted sum (Lin et al, 2017;Parikh et al, 2016;Kim et al, 2017), has also been a critical ingredient in modern NMT architectures.",
              "tag": "Method"
            },
            {
              "sent": "The Transformer network (Vaswani et al, 2017) avoids the recurrence completely and uses only self-attention.",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 7,
          "sentences": [
            {
              "sent": "We propose a modified Transformer network wherein the multi-head attention layer is replaced by a branched self-attention layer.",
              "tag": "Claim"
            },
            {
              "sent": "The contributions of the various branches is learned as part of the training procedure.",
              "tag": "Claim"
            },
            {
              "sent": "The idea of multi-branch networks has been explored in several domains (Ahmed & Torresani, 2017;Gastaldi, 2017;Xie et al, 2016).",
              "tag": "Claim"
            },
            {
              "sent": "To the best of our knowledge, this is the first model using a branched structure in the Transformer network.",
              "tag": "Conclusion"
            },
            {
              "sent": "In , the authors use a large network, with billions of weights, in conjunction with a sparse expert model to achieve competitive performance.",
              "tag": "Method"
            },
            {
              "sent": "Ahmed & Torresani (2017) analyze learned branching, through gates, in the context of computer vision while in Gastaldi (2017), the author analyzes a two-branch model with randomly sampled weights in the context of image classification.",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "TRANSFORMER NETWORK",
      "selected_sentences": [
        {
          "par_id": 9,
          "sentences": [
            {
              "sent": "From the source tokens, learned embeddings of dimension d model are generated which are then modified by an additive positional encoding.",
              "tag": "Method"
            },
            {
              "sent": "The positional encoding is necessary since the network does not otherwise possess any means of leveraging the order of the sequence since it contains no recurrence or convolution.",
              "tag": "Method"
            },
            {
              "sent": "The authors use additive encoding which is defined as: PE(pos, 2i) = sin(pos/10000 2i/dmodel ) PE(pos, 2i + 1) = cos(pos/10000 2i/dmodel ), where pos is the position of a word in the sentence and i is the dimension of the vector.",
              "tag": "Method"
            },
            {
              "sent": "The authors also experiment with learned embeddings (Gehring et al, 2016;2017) but found no benefit in doing so.",
              "tag": "Method"
            },
            {
              "sent": "The encoded word embeddings are then used as input to the encoder which consists of N layers each containing two sub-layers: (a) a multi-head attention mechanism, and (b) a feed-forward network.",
              "tag": "Method"
            }
          ]
        },
        {
          "par_id": 13,
          "sentences": [
            {
              "sent": "Multi-head attention mechanisms obtain h different representations of (Q, K, V ), compute scaled dot-product attention for each representation, concatenate the results, and project the concatenation with a feed-forward layer.",
              "tag": "Method"
            },
            {
              "sent": "This can be expressed in the same notation as Equation ( 1):",
              "tag": "Method"
            }
          ]
        },
        {
          "par_id": 15,
          "sentences": [
            {
              "sent": "where h denotes the number of heads in the multi-head attention.",
              "tag": "Method"
            },
            {
              "sent": "Vaswani et al (2017) proportionally reduce d k = d v = d model so that the computational load of the multi-head attention is the same as simple self-attention.",
              "tag": "Method"
            }
          ]
        },
        {
          "par_id": 16,
          "sentences": [
            {
              "sent": "The second component of each layer of the Transformer network is a feed-forward network.",
              "tag": "Method"
            },
            {
              "sent": "The authors propose using a two-layered network with a ReLU activation.",
              "tag": "Method"
            }
          ]
        },
        {
          "par_id": 17,
          "sentences": [
            {
              "sent": "The dimension of the inner layer is d f f which is set to 2048 in their experiments.",
              "tag": "Method"
            },
            {
              "sent": "For the sake of brevity, we refer the reader to Vaswani et al (2017) for additional details regarding the architecture.",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 19,
          "sentences": [
            {
              "sent": "One natural question regarding the Transformer network is why self-attention should be preferred to recurrent or convolutional models.",
              "tag": "Claim"
            },
            {
              "sent": "Vaswani et al (2017) state three reasons for the preference: (a) computational complexity of each layer, (b) concurrency, and (c) path length between long-range dependencies.",
              "tag": "Claim"
            },
            {
              "sent": "Assuming a sequence length of n and vector dimension d, the complexity of each layer is O(n 2 d) for self-attention layers while it is O(nd 2 ) for recurrent layers.",
              "tag": "Method"
            },
            {
              "sent": "Given that typically d > n, the complexity of self-attention layers is lower than that of recurrent layers.",
              "tag": "Claim"
            },
            {
              "sent": "Further, the number of sequential computations is O(1) for self-attention layers and O(n) for recurrent layers.",
              "tag": "Claim"
            },
            {
              "sent": "This helps improved utilization of parallel computing architectures.",
              "tag": "Method"
            },
            {
              "sent": "Finally, the maximum path length between dependencies is O(1) for the self-attention layer while it is O(n) for the recurrent layer.",
              "tag": "Result"
            },
            {
              "sent": "This difference is instrumental in impeding recurrent models' ability to learn long-range dependencies.",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "PROPOSED NETWORK ARCHITECTURE",
      "selected_sentences": [
        {
          "par_id": 21,
          "sentences": [
            {
              "sent": "In Equations ( 3) and (4), we described the attention layer proposed in Vaswani et al (2017) comprising the multi-head attention sub-layer and a FFN sub-layer.",
              "tag": "Method"
            },
            {
              "sent": "For the Weighted Transformer, we propose a branched attention that modifies the entire attention layer in the Transformer network (including both the multi-head attention and the feed-forward network).",
              "tag": "Method"
            },
            {
              "sent": "The proposed attention layer can be described as:",
              "tag": "Method"
            }
          ]
        },
        {
          "par_id": 24,
          "sentences": [
            {
              "sent": "It can be shown that if \u03ba i = 1 and \u03b1 i = 1 for all i, we recover the equation for the multi-head attention (3).",
              "tag": "Result"
            },
            {
              "sent": "However, given the i \u03ba i = 1 and i \u03b1 i = 1 bounds, these values are not permissible in the Weighted Transformer.",
              "tag": "Result"
            },
            {
              "sent": "One interpretation of our proposed architecture is that it replaces the multi-head attention by a multi-branch attention.",
              "tag": "Method"
            },
            {
              "sent": "Rather than concatenating the contributions of the different heads, they are instead treated as branches that a multi-branch network learns to combine.",
              "tag": "Result"
            }
          ]
        }
      ]
    },
    {
      "section_name": "EXPERIMENTS 4.1 TRAINING DETAILS",
      "selected_sentences": [
        {
          "par_id": 33,
          "sentences": [
            {
              "sent": "We note that the number of iterations required for convergence to the final score is substantially reduced for the Weighted Transformer.",
              "tag": "Result"
            },
            {
              "sent": "We found that Weighted Transformer converges 15-40% faster as measured by the total number of iterations to achieve optimal performance.",
              "tag": "Method"
            },
            {
              "sent": "We train the baseline model for 100K steps for the smaller variant and 300K for the larger.",
              "tag": "Method"
            },
            {
              "sent": "We train the Weighted Transformer for the respective variants for 60K and 250K iterations.",
              "tag": "Method"
            },
            {
              "sent": "We found that the objective did not significantly improve by running it for longer.",
              "tag": "Method"
            },
            {
              "sent": "Further, we do not use any averaging strategies employed in Vaswani et al (2017) and simply return the final model for testing purposes.",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "RESULTS ON BENCHMARK DATA SETS",
      "selected_sentences": [
        {
          "par_id": 36,
          "sentences": [
            {
              "sent": "Results of our experiments are summarized in Table 1.",
              "tag": "Result"
            },
            {
              "sent": "The Weighted Transformer achieves a 1.1 BLEU score improvement over the state-of-the-art on the English-toGerman task for the smaller network and 0.5 BLEU improvement for the larger network.",
              "tag": "Result"
            },
            {
              "sent": "In the case of the larger English-toFrench task, we note a 0.8 BLEU improvement for the smaller model and a 0.4 improvement for the larger model.",
              "tag": "Result"
            },
            {
              "sent": "Also, note that the performance of the smaller model for Weighted Transformer is close to that of the larger baseline model, especially for the English-toGerman task.",
              "tag": "Conclusion"
            },
            {
              "sent": "This suggests that the Weighted Transformer better utilizes available model capacity since it needs only 30% of the parameters as the baseline transformer for matching its performance.",
              "tag": "Result"
            },
            {
              "sent": "Our relative improvements do not hinge on using the BLEU scores for comparison; experiments with the GLEU score proposed in Wu et al (2016) also yielded similar improvements.",
              "tag": "Result"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Model EN-DE BLEU EN-FR BLEU",
      "selected_sentences": [
        {
          "par_id": 38,
          "sentences": [
            {
              "sent": "Transformer (small) (Vaswani et al, 2017) 27.3 38.1 Weighted Transformer (small) 28.4 38.9",
              "tag": "Result"
            }
          ]
        },
        {
          "par_id": 39,
          "sentences": [
            {
              "sent": "Transformer (large) (Vaswani et al, 2017) 28.4 41.0 Weighted Transformer (large)",
              "tag": "Method"
            }
          ]
        },
        {
          "par_id": 42,
          "sentences": [
            {
              "sent": "Table 1: Experimental results on the WMT 2014 English-toGerman (ENDE) and English-toFrench (ENFR) translation tasks.",
              "tag": "Method"
            },
            {
              "sent": "Our proposed model outperforms the state-of-the-art models including the Transformer (Vaswani et al, 2017).",
              "tag": "Result"
            },
            {
              "sent": "The small model corresponds to configuration (A) in Table 2 while large corresponds to configuration (B).",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "SENSITIVITY ANALYSIS",
      "selected_sentences": [
        {
          "par_id": 44,
          "sentences": [
            {
              "sent": "In Table 2, we report sensitivity results on the newstest2013 English-toGerman task.",
              "tag": "Method"
            },
            {
              "sent": "Specifically, we vary the number of layers in the encoder/decoder and compare the performance of the Weighted Transformer and the Transformer baseline.",
              "tag": "Result"
            },
            {
              "sent": "The results clearly demonstrate the benefit of the branched attention; for every experiment, the Weighted Transformer outperforms the baseline transformer, in some cases by up to 1.3 BLEU points.",
              "tag": "Result"
            },
            {
              "sent": "As in the case of the baseline Transformer, increasing the number of layers does not necessarily improve performance; a modest improvement is seen when the number of layers N is increased from 2 to 4 and 4 to 6 but the performance degrades when N is increased to 8. Increasing the number of heads from 8 to 16 in configuration (A) yielded an even better BLEU score.",
              "tag": "Result"
            },
            {
              "sent": "However, preliminary experiments with h = 16 and h = 32, like in the case with N , degrade the performance of the model.",
              "tag": "Result"
            },
            {
              "sent": "Figure 3: Convergence of the (\u03b1, \u03ba) weights for the second encoder layer of Configuration (C) for the English-toGerman newstest2013 task.",
              "tag": "Method"
            },
            {
              "sent": "We smoothen the curves using a mean filter.",
              "tag": "Result"
            },
            {
              "sent": "This shows that the network does prioritize some branches more than others and that the architecture does not exploit a subset of the branches while ignoring others.",
              "tag": "Result"
            }
          ]
        }
      ]
    },
    {
      "section_name": "RANDOMIZATION BASELINE",
      "selected_sentences": []
    },
    {
      "section_name": "GATING",
      "selected_sentences": []
    },
    {
      "section_name": "CONCLUSIONS",
      "selected_sentences": [
        {
          "par_id": 52,
          "sentences": [
            {
              "sent": "We present the Weighted Transformer that trains faster and achieves better performance than the original Transformer network.",
              "tag": "Method"
            },
            {
              "sent": "The proposed architecture replaces the multi-head attention in the Transformer network by a multiple self-attention branches whose contributions are learned as a part of the training process.",
              "tag": "Method"
            },
            {
              "sent": "We report numerical results on the WMT 2014 English-toGerman and English-toFrench tasks and show that the Weighted Transformer improves the state-of-the-art BLEU scores by 0.5 and 0.4 points respectively.",
              "tag": "Method"
            },
            {
              "sent": "Further, our proposed architecture trains 15 \u2212 40% faster than the baseline Transformer.",
              "tag": "Result"
            },
            {
              "sent": "Finally, we present evidence suggesting the regularizing effect of the proposal and emphasize that the relative improvement in BLEU score is observed across various hyper-parameter settings for both small and large models.",
              "tag": "Result"
            }
          ]
        }
      ]
    }
  ],
  "title": "WEIGHTED TRANSFORMER NETWORK FOR MACHINE TRANSLATION"
}