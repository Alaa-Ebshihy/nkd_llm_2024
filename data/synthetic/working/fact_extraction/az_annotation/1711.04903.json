{
  "paper_id": "1711.04903",
  "sections": [
    {
      "section_name": "Abstract",
      "selected_sentences": [
        {
          "par_id": 0,
          "sentences": [
            {
              "sent": "Adversarial training (AT) 1 is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations.",
              "tag": "Claim"
            },
            {
              "sent": "Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing.",
              "tag": "Claim"
            },
            {
              "sent": "In this paper, we propose and analyze a neural POS tagging model that exploits AT.",
              "tag": "Claim"
            },
            {
              "sent": "In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words.",
              "tag": "Method"
            },
            {
              "sent": "We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations.",
              "tag": "Result"
            },
            {
              "sent": "5) The proposed AT model is generally effective in different sequence labeling tasks.",
              "tag": "Conclusion"
            },
            {
              "sent": "These positive results motivate further use of AT for natural language tasks.",
              "tag": "Conclusion"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Introduction",
      "selected_sentences": [
        {
          "par_id": 2,
          "sentences": [
            {
              "sent": "Recently, neural network-based approaches have become popular in many natural language processing (NLP) tasks including tagging, parsing, and translation (Chen and Manning, 2014;Bahdanau et al, 2015;Ma and Hovy, 2016).",
              "tag": "Claim"
            },
            {
              "sent": "However, it has been shown that neural networks tend to be locally unstable and even tiny perturbations to the original inputs can mislead the models (Szegedy et al, 2014).",
              "tag": "Claim"
            },
            {
              "sent": "Such maliciously perturbed inputs are called adversarial examples.",
              "tag": "Claim"
            },
            {
              "sent": "Adversarial training (Goodfellow et al, 2015) aims to improve the robustness of a model to input perturbations by training on both unmodified examples and adversarial examples.",
              "tag": "Claim"
            },
            {
              "sent": "Previous work (Goodfellow Figure 1: Illustration of our architecture for adversarial POS tagging.",
              "tag": "Method"
            },
            {
              "sent": "Given a sentence, we input the normalized word embeddings (w 1 , w 2 , w 3 ) and character embeddings (showing c 1 , c 2 , c 3 for w 1 ).",
              "tag": "Method"
            },
            {
              "sent": "Each word is represented by concatenating its word embedding and its character-level BiLSTM output.",
              "tag": "Method"
            },
            {
              "sent": "They are fed into the main BiLSTMCRF network for POS tagging.",
              "tag": "Method"
            },
            {
              "sent": "In adversarial training, we compute and add the worst-case perturbation \u03b7 to all the input embeddings for regularization.",
              "tag": "Claim"
            },
            {
              "sent": "Shaham et al, 2015) on image recognition has demonstrated the enhanced robustness of their models to unseen images via adversarial training and has provided theoretical explanations of the regularization effects.",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 3,
          "sentences": [
            {
              "sent": "Despite its potential as a powerful regularizer, adversarial training (AT) has yet to be explored extensively in natural language tasks.",
              "tag": "Claim"
            },
            {
              "sent": "Recently, Miyato et al (2017) applied AT on text classification, achieving state-of-the-art accuracy.",
              "tag": "Claim"
            },
            {
              "sent": "Yet, the specific effects of the robustness obtained from AT are still unclear in the context of NLP.",
              "tag": "Claim"
            },
            {
              "sent": "For example, research studies have yet to answer questions such as 1) how can we interpret perturbations or robustness on natural language inputs? related to linguistic factors like vocabulary statistics? 3) are the effects of AT language-dependent?",
              "tag": "Claim"
            },
            {
              "sent": "Answering such questions is crucial to understand and motivate the application of adversarial training on natural language tasks.",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 4,
          "sentences": [
            {
              "sent": "In this paper, spotlighting a well-studied core problem of NLP, we propose and carefully analyze a neural part-of-speech (POS) tagging model that exploits adversarial training.",
              "tag": "Method"
            },
            {
              "sent": "With a BiLSTMCRF model Ma and Hovy, 2016) as our baseline POS tagger, we apply adversarial training by considering perturbations to input word/character embeddings.",
              "tag": "Method"
            },
            {
              "sent": "In order to demystify the effects of adversarial training in the context of NLP, we conduct POS tagging experiments on multiple languages using the Penn Treebank WSJ corpus (Englsih) and the Universal Dependencies dataset (27 languages), with thorough analyses of the following points:",
              "tag": "Method"
            }
          ]
        },
        {
          "par_id": 9,
          "sentences": [
            {
              "sent": "\u2022 AT can boost the tagging performance for rare/ unseen words and increase the sentence-level accuracy.",
              "tag": "Claim"
            },
            {
              "sent": "This positively affects the performance of down-stream tasks such as dependency parsing, where low sentence-level POS accuracy can be a bottleneck (Manning, 2011).",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 11,
          "sentences": [
            {
              "sent": "We argue that the effects of AT can be interpreted from the perspective of natural language.",
              "tag": "Claim"
            },
            {
              "sent": "Finally, we demonstrate that the proposed AT model is generally effective across different sequence labeling tasks.",
              "tag": "Conclusion"
            },
            {
              "sent": "This work therefore provides a strong motivation and basis for utilizing adversarial training in NLP tasks.",
              "tag": "Conclusion"
            }
          ]
        }
      ]
    },
    {
      "section_name": "POS Tagging",
      "selected_sentences": [
        {
          "par_id": 14,
          "sentences": [
            {
              "sent": "It is empirically shown that POS tagging performance can greatly affect downstream tasks such as dependency parsing .",
              "tag": "Result"
            },
            {
              "sent": "In this work, we also demonstrate that the improvements obtained from our AT POS tagger actually contribute to dependency parsing.",
              "tag": "Conclusion"
            },
            {
              "sent": "Nonetheless, parsing with gold POS tags still yields better results, bolstering the view that POS tagging is an essential task in NLP that needs further development.",
              "tag": "Claim"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Adversarial Training",
      "selected_sentences": [
        {
          "par_id": 15,
          "sentences": [
            {
              "sent": "The concept of adversarial training (Szegedy et al, 2014;Goodfellow et al, 2015) was originally introduced in the context of image classification to improve the robustness of a model by training on input images with malicious perturbations.",
              "tag": "Claim"
            },
            {
              "sent": "Previous work (Goodfellow et al, 2015;Shaham et al, 2015; has provided a theoretical framework to understand adversarial examples and the regularization effects of adversarial training (AT) in image recognition.",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 16,
          "sentences": [
            {
              "sent": "Recently, Miyato et al ( 2017) applied AT to a natural language task (text classification) by extending the concept of adversarial perturbations to word embeddings.",
              "tag": "Claim"
            },
            {
              "sent": "Wu et al (2017) further explored the possibility of AT in relation extraction.",
              "tag": "Claim"
            },
            {
              "sent": "Both report improved performance on their tasks via AT, but the specific effects of AT have yet to be analyzed.",
              "tag": "Claim"
            },
            {
              "sent": "In our work, we aim to address this issue by providing detailed analyses on the effects of AT from the perspective of NLP, such as different languages, vocabulary statistics, word embedding distribution, and aim to motivate future research that exploits AT in NLP tasks.",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 27,
          "sentences": [
            {
              "sent": "Adversarial training (Goodfellow et al, 2015) is a powerful regularization method, primarily explored in image recognition to improve the robustness of classifiers to input perturbations.",
              "tag": "Method"
            },
            {
              "sent": "Given a classifier, we first generate input examples that are very close to original inputs (so should yield the same labels) yet are likely to be misclassified by the current model.",
              "tag": "Method"
            },
            {
              "sent": "Specifically, these adversarial examples are generated by adding small perturbations to the inputs in the direction that significantly increases the loss function of the classifier (worstcase perturbations).",
              "tag": "Method"
            },
            {
              "sent": "Then, the classifier is trained on the mixture of clean examples and adversarial examples to improve the stability to input perturbations.",
              "tag": "Method"
            },
            {
              "sent": "In this work, we incorporate adversarial training into our baseline POS tagger, aiming to achieve better regularization effects and to provide their interpretations in the context of NLP.",
              "tag": "Claim"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Method",
      "selected_sentences": [
        {
          "par_id": 19,
          "sentences": [
            {
              "sent": "In this section, we introduce our baseline POS tagging model and explain how we implement adversarial training on top.",
              "tag": "Claim"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Baseline POS Tagging Model",
      "selected_sentences": [
        {
          "par_id": 20,
          "sentences": [
            {
              "sent": "Following the recent top-performing models for sequence labeling tasks (Plank et al, 2016;Lample et al, 2016;Ma and Hovy, 2016), we employ a Bi-directional LSTMCRF model as our baseline (see Figure 1 for an illustration).",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "CRF.",
      "selected_sentences": []
    },
    {
      "section_name": "Experiments",
      "selected_sentences": [
        {
          "par_id": 34,
          "sentences": [
            {
              "sent": "To fully analyze the effects of adversarial training, we train and evaluate our baseline/adversarial POS tagging models on both a standard English dataset and a multilingual dataset.",
              "tag": "Claim"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Datasets",
      "selected_sentences": []
    },
    {
      "section_name": "Results",
      "selected_sentences": []
    },
    {
      "section_name": "Analysis",
      "selected_sentences": [
        {
          "par_id": 39,
          "sentences": [
            {
              "sent": "In the previous sections, we demonstrated the regularization power of adversarial training (AT) on different languages, based on the overall POS tagging performance and learning curves.",
              "tag": "Claim"
            },
            {
              "sent": "In this section, we conduct further analyses on the robustness of AT from NLP specific aspects such as word statistics, sequence modeling, downstream tasks, and word representation learning.",
              "tag": "Claim"
            },
            {
              "sent": "We find that AT can boost tagging accuracy on rare words and neighbors of unseen words ( \u00a75.1).",
              "tag": "Result"
            },
            {
              "sent": "Furthermore, this robustness against rare / unseen words leads to better sentence-level accuracy and downstream dependency parsing ( \u00a75.2).",
              "tag": "Method"
            },
            {
              "sent": "We illustrate these findings using two major languages, English (WSJ) and French (UD), which have substantially large training and testing data to discuss vocabulary statistics and sentence-level performance.",
              "tag": "Method"
            },
            {
              "sent": "Finally, we study the effects of AT on word representation learning ( \u00a75.3), and the applicability of AT to different sequential tasks ( \u00a75.4).",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Word-level Analysis",
      "selected_sentences": [
        {
          "par_id": 40,
          "sentences": [
            {
              "sent": "Poor tagging accuracy on rare/unseen words is one of the bottlenecks in current POS taggers (Manning, 2011;Plank et al, 2016).",
              "tag": "Method"
            },
            {
              "sent": "Aiming to reveal the effects of AT on rare / unseen words, we analyze tagging performance at the word level, considering vocabulary statistics.",
              "tag": "Claim"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Sentence-level & Downstream Analysis",
      "selected_sentences": [
        {
          "par_id": 43,
          "sentences": [
            {
              "sent": "In the word-level analysis, we showed that AT can boost tagging accuracy on rare words and the neighbors of unseen words, enhancing overall robustness on rare/unseen words.",
              "tag": "Claim"
            },
            {
              "sent": "In this section, we discuss the benefit of our improved POS tagger in a major downstream task, dependency parsing.",
              "tag": "Claim"
            },
            {
              "sent": "Most of the recent state-of-the-art dependency parsers take predicted POS tags as input (eg",
              "tag": "Claim"
            },
            {
              "sent": "Chen and Manning (2014); Andor et al (2016); ). empirically show that their dependency parser gains significant improvements by using POS tags predicted by a BiLSTM POS tagger, while POS tags predicted by the UDPipe tagger (Straka et al, 2016) do not contribute to parsing performance as much.",
              "tag": "Claim"
            },
            {
              "sent": "This observation illustrates that POS tagging performance has a great influence on dependency parsing, motivating the hypothesis that the POS tagging improvements gained from our adversarial training help dependency parsing.",
              "tag": "Result"
            }
          ]
        },
        {
          "par_id": 45,
          "sentences": [
            {
              "sent": "Table 5 shows the results of the experiments.",
              "tag": "Result"
            },
            {
              "sent": "We can observe improvements in both languages by using the POS tags predicted by our AT POS tagger.",
              "tag": "Result"
            },
            {
              "sent": "As Manning (2011) points out, when predicted POS tags are used for downstream dependency parsing, a single bad mistake in a sentence can greatly damage the usefulness of the POS tagger.",
              "tag": "Claim"
            },
            {
              "sent": "The robustness of our AT POS tagger against rare/unseen words helps to mitigate such an issue.",
              "tag": "Result"
            },
            {
              "sent": "This advantage can also be observed from the AT POS tagger's notably higher sentence-level accuracy than the baseline (see Table 5 left).",
              "tag": "Result"
            },
            {
              "sent": "Nonetheless, gold POS tags still yield better parsing results as compared to the baseline/AT POS taggers, supporting the claim that POS tagging needs further improvement for downstream tasks.",
              "tag": "Result"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Effects on Representation Learning",
      "selected_sentences": [
        {
          "par_id": 49,
          "sentences": [
            {
              "sent": "The evaluation results are summarized in Table 6.",
              "tag": "Result"
            },
            {
              "sent": "We report the tightness scores for the four major clusters: noun, verb, adjective, and adverb (from left to right).",
              "tag": "Method"
            },
            {
              "sent": "As can be seen from the table, for both languages, adversarial training (AT) results in cleaner word embedding distributions than the baseline, with a higher cosine similarity within each POS cluster, and with a clear advantage in the average tightness across all the clusters.",
              "tag": "Result"
            },
            {
              "sent": "In other words, the learned word vectors show stronger correlations with their POS tags.",
              "tag": "Result"
            },
            {
              "sent": "This result confirms that training with adversarial examples can help to learn cleaner word embeddings so that the meaning / grammatical function of a word cannot be altered by a small perturbation in its embedding.",
              "tag": "Conclusion"
            },
            {
              "sent": "This analysis provides a means to interpret the robustness to input perturbations, from the perspective of NLP.",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Other Sequence Labeling Tasks",
      "selected_sentences": [
        {
          "par_id": 52,
          "sentences": [
            {
              "sent": "Finally, to further confirm the applicability of AT, we experiment with our BiLSTMCRF AT model in different sequence labeling tasks: chunking and named entity recognition (NER).",
              "tag": "Result"
            }
          ]
        },
        {
          "par_id": 55,
          "sentences": [
            {
              "sent": "The results are summarized in Table 8 and 9. AT enhanced F1 score from the baseline BiLSTMCRF model's 95.18 to 95.25 for chunking, and from 91.22 to 91.56 for NER, also significantly outperforming Ma and Hovy (2016).",
              "tag": "Result"
            },
            {
              "sent": "These improvements made by AT are bigger than that for English POS tagging, most likely due to the larger room for improvement in chunking and NER.",
              "tag": "Result"
            },
            {
              "sent": "The improvements are again statistically significant, with p-value < 0.05 on the t-test.",
              "tag": "Result"
            },
            {
              "sent": "The experimental results suggest that the proposed adversarial training scheme is generally effective across different sequence labeling tasks.",
              "tag": "Claim"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Conclusion",
      "selected_sentences": [
        {
          "par_id": 57,
          "sentences": [
            {
              "sent": "We proposed and carefully analyzed a POS tagging model that exploits adversarial training (AT).",
              "tag": "Claim"
            },
            {
              "sent": "In our multilingual experiments, we find that AT achieves substantial improvements on all the languages tested, especially on low resource ones.",
              "tag": "Result"
            },
            {
              "sent": "AT also enhances the robustness to rare/unseen words and sentence-level accuracy, alleviating the major issues of current POS taggers, and contributing to the downstream task, dependency parsing.",
              "tag": "Result"
            },
            {
              "sent": "Furthermore, our analyses on different languages, word / neighbor statistics and word representation learning reveal the effects of AT from the perspective of NLP.",
              "tag": "Conclusion"
            },
            {
              "sent": "The proposed AT model is applicable to general sequence labeling tasks.",
              "tag": "Conclusion"
            },
            {
              "sent": "This work therefore provides a strong basis and motivation for utilizing AT in natural language tasks.",
              "tag": "Conclusion"
            }
          ]
        }
      ]
    }
  ],
  "title": "Robust Multilingual Part-of-Speech Tagging via Adversarial Training"
}