{
  "paper_id": "1711.08028",
  "sections": [
    {
      "section_name": "Abstract",
      "selected_sentences": [
        {
          "par_id": 0,
          "sentences": [
            {
              "sent": "This paper is concerned with learning to solve tasks that require a chain of interdependent steps of relational inference, like answering complex questions about the relationships between objects, or solving puzzles where the smaller elements of a solution mutually constrain each other.",
              "tag": "Claim"
            },
            {
              "sent": "We introduce the recurrent relational network, a general purpose module that operates on a graph representation of objects.",
              "tag": "Claim"
            },
            {
              "sent": "As a generalization of Santoro et al [2017]'s relational network, it can augment any neural network model with the capacity to do many-step relational reasoning.",
              "tag": "Method"
            },
            {
              "sent": "We achieve state of the art results on the bAbI textual question-answering dataset with the recurrent relational network, consistently solving 20/20 tasks.",
              "tag": "Method"
            },
            {
              "sent": "As bAbI is not particularly challenging from a relational reasoning point of view, we introduce PrettyCLEVR, a new diagnostic dataset for relational reasoning.",
              "tag": "Claim"
            },
            {
              "sent": "In the PrettyCLEVR set-up, we can vary the question to control for the number of relational reasoning steps that are required to obtain the answer.",
              "tag": "Claim"
            },
            {
              "sent": "Using PrettyCLEVR, we probe the limitations of multi-layer perceptrons, relational and recurrent relational networks.",
              "tag": "Method"
            },
            {
              "sent": "Finally, we show how recurrent relational networks can learn to solve Sudoku puzzles from supervised training data, a challenging task requiring upwards of 64 steps of relational reasoning.",
              "tag": "Result"
            },
            {
              "sent": "We achieve state-of-the-art results amongst comparable methods by solving 96.6% of the hardest Sudoku puzzles. 1  We invite the reader to solve the Sudoku in the supplementary material to appreciate the difficulty of solving a Sudoku in which 17 cells are initially filled.",
              "tag": "Result"
            },
            {
              "sent": "32nd Conference on Neural Information Processing Systems (NeurIPS 2018),",
              "tag": "Result"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Introduction",
      "selected_sentences": [
        {
          "par_id": 3,
          "sentences": [
            {
              "sent": "A central component of human intelligence is the ability to abstractly reason about objects and their interactions [Spelke et al, 1995, Spelke andKinzler, 2007].",
              "tag": "Claim"
            },
            {
              "sent": "As an illustrative example, consider solving a Sudoku.",
              "tag": "Method"
            },
            {
              "sent": "A Sudoku consists of 81 cells that are arranged in a 9-by-9 grid, which must be filled with digits 1 to 9 so that each digit appears exactly once in each row, column and 3-by-3 non-overlapping box, with a number of digits given 1 .",
              "tag": "Claim"
            },
            {
              "sent": "To solve a Sudoku, one methodically reasons about the puzzle in terms of its cells and their interactions over many steps.",
              "tag": "Claim"
            },
            {
              "sent": "One tries placing digits in cells and see how that affects other cells, iteratively working toward a solution.",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 4,
          "sentences": [
            {
              "sent": "Contrast this with the canonical deep learning approach to solving problems, the multilayer perceptron (MLP), or multilayer convolutional neural net (CNN).",
              "tag": "Claim"
            },
            {
              "sent": "These architectures take the entire Sudoku as an input and output the entire solution in a single forward pass, ignoring the inductive bias that objects exists in the world, and that they affect each other in a consistent manner.",
              "tag": "Claim"
            },
            {
              "sent": "Not surprisingly these models fall short when faced with problems that require even basic relational reasoning [Lake et al, 2016, Santoro et al, 2017.",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 5,
          "sentences": [
            {
              "sent": "The relational network of Santoro et al [2017] is an important first step towards a simple module for reasoning about objects and their interactions but it is limited to performing a single relational operation, and was evaluated on datasets that require a maximum of three steps of reasoning (which, surprisingly, can be solved by a single relational reasoning step as we show).",
              "tag": "Claim"
            },
            {
              "sent": "Looking beyond relational networks, there is a rich literature on logic and reasoning in artificial intelligence and machine learning, which we discuss in section 5.",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 6,
          "sentences": [
            {
              "sent": "Toward generally realizing the ability to methodically reason about objects and their interactions over many steps, this paper introduces a composite function, the recurrent relational network.",
              "tag": "Claim"
            },
            {
              "sent": "It serves as a modular component for many-step relational reasoning in end-to-end differentiable learning systems.",
              "tag": "Method"
            },
            {
              "sent": "It encodes the inductive biases that 1) objects exists in the world 2) they can be sufficiently described by properties 3) properties can change over time 4) objects can affect each other and 5) given the properties, the effects object have on each other is invariant to time.",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 7,
          "sentences": [
            {
              "sent": "An important insight from the work of Santoro et al [2017] is to decompose a function for relational reasoning into two components or \"modules\": a perceptual front-end, which is tasked to recognize objects in the raw input and represent them as vectors, and a relational reasoning module, which uses the representation to reason about the objects and their interactions.",
              "tag": "Method"
            },
            {
              "sent": "Both modules are trained jointly end-to-end.",
              "tag": "Method"
            },
            {
              "sent": "In computer science parlance, the relational reasoning module implements an interface: it operates on a graph of nodes and directed edges, where the nodes are represented by real valued vectors, and is differentiable.",
              "tag": "Claim"
            },
            {
              "sent": "This paper chiefly develops the relational reasoning side of that interface.",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 10,
          "sentences": [
            {
              "sent": "This paper considers many-step relational reasoning, a challenging task for deep learning architectures.",
              "tag": "Claim"
            },
            {
              "sent": "We develop a recurrent relational reasoning module, which constitutes our main contribution.",
              "tag": "Method"
            },
            {
              "sent": "We show that it is a powerful architecture for many-step relational reasoning on three varied datasets, achieving state-of-the-art results on bAbI and Sudoku.",
              "tag": "Claim"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Recurrent Relational Networks",
      "selected_sentences": [
        {
          "par_id": 11,
          "sentences": [
            {
              "sent": "We ground the discussion of a recurrent relational network in something familiar, solving a Sudoku puzzle.",
              "tag": "Claim"
            },
            {
              "sent": "A simple strategy works by noting that if a certain Sudoku cell is given as a \"7\", one can safely remove \"7\" as an option from other cells in the same row, column and box.",
              "tag": "Claim"
            },
            {
              "sent": "In a message passing framework, that cell needs to send a message to each other cell in the same row, column, and box, broadcasting it's value as \"7\", and informing those cells not to take the value \"7\".",
              "tag": "Method"
            },
            {
              "sent": "In an iteration t, these messages are sent simultaneously, in parallel, between all cells.",
              "tag": "Method"
            },
            {
              "sent": "Each cell i should then consider all incoming messages, and update its internal state h t i to h t+1 i .",
              "tag": "Method"
            },
            {
              "sent": "With the updated state each cell should send out new messages, and the process repeats.",
              "tag": "Method"
            }
          ]
        },
        {
          "par_id": 12,
          "sentences": [
            {
              "sent": "The recurrent relational network will learn to pass messages on a graph.",
              "tag": "Method"
            },
            {
              "sent": "For Sudoku, the graph has i \u2208 {1, 2, ..., 81} nodes, one for each cell in the Sudoku.",
              "tag": "Method"
            },
            {
              "sent": "Each node has an input feature vector x i , and edges to and from all nodes that are in the same row, column and box in the Sudoku.",
              "tag": "Method"
            },
            {
              "sent": "The graph is the input to the relational reasoning module, and vectors x i would generally be the output of a perceptual front-end, for instance a convolutional neural network.",
              "tag": "Method"
            },
            {
              "sent": "Keeping with our Sudoku example, each x i encodes the initial cell content (empty or given) and the row and column position of the cell.",
              "tag": "Method"
            }
          ]
        },
        {
          "par_id": 16,
          "sentences": [
            {
              "sent": "where g, the node function, is another learned neural network.",
              "tag": "Method"
            },
            {
              "sent": "The dependence on the previous node hidden state h t\u22121 j allows the network to iteratively work towards a solution instead of starting with a blank slate at every step.",
              "tag": "Method"
            },
            {
              "sent": "Injecting the feature vector x j at each step like this allows the node function to focus on the messages from the other nodes instead of trying to remember the input.",
              "tag": "Method"
            }
          ]
        },
        {
          "par_id": 17,
          "sentences": [
            {
              "sent": "The above equations for sending messages and updating node states define a recurrent relational network's core.",
              "tag": "Method"
            },
            {
              "sent": "To train a recurrent relational network in a supervised manner to solve a Sudoku we introduce an output probability distribution over the digits 1-9 for each of the nodes in the graph.",
              "tag": "Method"
            },
            {
              "sent": "The output distribution o t i for node i at step t is given by",
              "tag": "Result"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Convergent message passing.",
      "selected_sentences": []
    },
    {
      "section_name": "Experiments",
      "selected_sentences": []
    },
    {
      "section_name": "bAbI question-answering tasks",
      "selected_sentences": [
        {
          "par_id": 26,
          "sentences": [
            {
              "sent": "Our trained network solves 20 of 20 tasks in 13 out of 15 runs.",
              "tag": "Result"
            },
            {
              "sent": "This is state-of-the-art and markedly more stable than competing methods.",
              "tag": "Method"
            },
            {
              "sent": "We perform ablation experiment to see which parts of the model are important, including varying the number of steps.",
              "tag": "Result"
            },
            {
              "sent": "We find that using dropout and appending the question encoding to the fact encodings is important for the performance.",
              "tag": "Result"
            },
            {
              "sent": "See the supplementary material for details.",
              "tag": "Result"
            }
          ]
        },
        {
          "par_id": 27,
          "sentences": [
            {
              "sent": "Surprisingly, we find that we only need a single step of relational reasoning to solve all the bAbI tasks.",
              "tag": "Result"
            },
            {
              "sent": "This is surprising since the hardest tasks requires reasoning about three facts.",
              "tag": "Claim"
            },
            {
              "sent": "It's possible that there are superficial correlations in the tasks that the model learns to exploit.",
              "tag": "Method"
            },
            {
              "sent": "Alternatively the model learns to compress all the relevant fact-relations into the 128 floats resulting from the sum over the node hidden states, and perform the remaining reasoning steps in the output MLP.",
              "tag": "Conclusion"
            },
            {
              "sent": "Regardless, it appears multiple steps of relational reasoning are not important for the bAbI dataset.",
              "tag": "Claim"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Pretty-CLEVR",
      "selected_sentences": [
        {
          "par_id": 28,
          "sentences": [
            {
              "sent": "Given that bAbI did not require multiple steps of relational reasoning and in order to test our hypothesis that our proposed model is better suited for tasks requiring more steps of relational reasoning we create a diagnostic dataset \"PrettyCLEVER\".",
              "tag": "Claim"
            },
            {
              "sent": "It can be seen as an extension of the \"Sort-ofCLEVR\" data set by [Santoro et al, 2017] which has questions of a non-relational and relational nature.",
              "tag": "Claim"
            },
            {
              "sent": "\"PrettyCLEVR\" takes this a step further and has non-relational questions as well as questions requiring varying degrees of relational reasoning.",
              "tag": "Method"
            },
            {
              "sent": "For the topmost sample the solution to the question: \"green, 3 jumps\", which is \"plus\", is shown with arrows.",
              "tag": "Method"
            },
            {
              "sent": "2b Random corresponds to picking one of the eight possible outputs at random (colors or shapes, depending on the input).",
              "tag": "Method"
            },
            {
              "sent": "The RRN is trained for four steps but since it predicts at each step we can evaluate the performance for each step.",
              "tag": "Method"
            },
            {
              "sent": "The the number of steps is stated in parentheses.",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Sudoku",
      "selected_sentences": [
        {
          "par_id": 31,
          "sentences": [
            {
              "sent": "We create training, validation and testing sets totaling 216,000 Sudoku puzzles with a uniform distribution of givens between 17 and 34.",
              "tag": "Method"
            },
            {
              "sent": "We consider each of the 81 cells in the 9x9 Sudoku grid a node in a graph, with edges to and from each other cell in the same row, column and box.",
              "tag": "Method"
            },
            {
              "sent": "The node features x i are the output of a MLP which takes as input the digit for the cell (0-9, 0 if not given), and the row and column position (1-9).",
              "tag": "Method"
            },
            {
              "sent": "We run the network for 32 steps and at every step the output function r maps each node hidden state to nine output logits corresponding to the nine possible digits.",
              "tag": "Method"
            },
            {
              "sent": "For details see the supplementary material.",
              "tag": "Result"
            },
            {
              "sent": "Our network learns to solve 94.1% of even the hardest 17-givens Sudokus after 32 steps.",
              "tag": "Method"
            },
            {
              "sent": "We only consider a puzzled solved if all the digits are correct, ie no partial credit is given for getting individual digits correct.",
              "tag": "Method"
            },
            {
              "sent": "For more givens the accuracy (fraction of test puzzles solved) quickly approaches 100%.",
              "tag": "Method"
            },
            {
              "sent": "Since the network outputs a probability distribution for each step, we can visualize how the network arrives at the solution step by step.",
              "tag": "Method"
            },
            {
              "sent": "For an example of this see figure 3.",
              "tag": "Method"
            }
          ]
        },
        {
          "par_id": 32,
          "sentences": [
            {
              "sent": "To examine our hypothesis that multiple steps are required we plot the accuracy as a function of the number of steps.",
              "tag": "Result"
            },
            {
              "sent": "See figure 4. We can see that even simple Sudokus with 33 givens require upwards of 10 steps of relational reasoning, whereas the harder 17 givens continue to improve even after 32 steps.",
              "tag": "Result"
            },
            {
              "sent": "Figure 4 also shows that the model has learned a convergent algorithm.",
              "tag": "Method"
            },
            {
              "sent": "The model was trained for 32 steps, but seeing that the accuracy increased with more steps, we ran the model for 64 steps during testing.",
              "tag": "Method"
            },
            {
              "sent": "At 64 steps the accuracy for the 17 givens puzzles increases to 96.6%.",
              "tag": "Method"
            }
          ]
        },
        {
          "par_id": 33,
          "sentences": [
            {
              "sent": "We also examined the importance of the row and column features by multiplying the row and column embeddings by zero and re-tested our trained network.",
              "tag": "Method"
            },
            {
              "sent": "We compare our network to several other differentiable methods.",
              "tag": "Method"
            },
            {
              "sent": "See table 2. We train two relational networks: a node and a graph centric.",
              "tag": "Method"
            },
            {
              "sent": "For details see the supplementary material.",
              "tag": "Result"
            },
            {
              "sent": "Of the two, the node centric was considerably better.",
              "tag": "Result"
            },
            {
              "sent": "The node centric correspond exactly to our proposed network with a single step, yet fails to solve any Sudoku.",
              "tag": "Result"
            },
            {
              "sent": "This shows that multiple steps are crucial for complex relational reasoning.",
              "tag": "Result"
            },
            {
              "sent": "Our network outperforms loopy belief propagation, with parallel and random messages passing updates [Bauke, 2008].",
              "tag": "Result"
            },
            {
              "sent": "It also outperforms a version of loopy belief propagation modified specifically for solving Sudokus that uses 250 steps, Sinkhorn balancing every two steps and iteratively picks the most probable digit [Khan et al, 2014].",
              "tag": "Result"
            },
            {
              "sent": "We also compare to learning the messages in parallel loopy BP as presented in Lin et al [2015].",
              "tag": "Method"
            },
            {
              "sent": "We tried a few variants including a single step as presented and 32 steps with and without a loss on every step, but could not get it to solve any 17 given Sudokus.",
              "tag": "Method"
            },
            {
              "sent": "Finally we outperform Park [2016] which treats the Sudoku as a 9x9 image, uses 10 convolutional layers, iteratively picks the most probable digit, and evaluate on easier Sudokus with 24-36 givens.",
              "tag": "Method"
            },
            {
              "sent": "We also tried to train a version of our network that only had a loss at the last step.",
              "tag": "Result"
            },
            {
              "sent": "It was harder to train, performed worse and didn't learn a convergent algorithm.",
              "tag": "Result"
            },
            {
              "sent": "Loopy BP, parallel [Bauke, 2008] 17 53.2%",
              "tag": "Claim"
            },
            {
              "sent": "Deeply Learned Messages* [Lin et al, 2015] 17 0% Relational Network, node* [Santoro et al, 2017] 17 0% Relational Network, graph* [Santoro et al, 2017] 17 0% Deep Convolutional Network [Park, 2016] 24-36 70%",
              "tag": "Claim"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Age arithmetic",
      "selected_sentences": []
    },
    {
      "section_name": "Discussion",
      "selected_sentences": [
        {
          "par_id": 35,
          "sentences": [
            {
              "sent": "We have proposed a general relational reasoning model for solving tasks requiring an order of magnitude more complex relational reasoning than the current state-of-the art.",
              "tag": "Claim"
            },
            {
              "sent": "BaBi and Sort-ofCLEVR require a few steps, PrettyCLEVR requires up to eight steps and Sudoku requires more than ten steps.",
              "tag": "Conclusion"
            },
            {
              "sent": "Our relational reasoning module can be added to any deep learning model to add a powerful relational reasoning capacity.",
              "tag": "Result"
            },
            {
              "sent": "We get state-of-the-art results on Sudokus solving 96.6% of the hardest Sudokus with 17 givens.",
              "tag": "Result"
            },
            {
              "sent": "We also markedly improve state-of-the-art on the BaBi dataset solving 20/20 tasks in 13 out of 15 runs with a single model trained jointly on all tasks.",
              "tag": "Result"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Related work",
      "selected_sentences": [
        {
          "par_id": 38,
          "sentences": [
            {
              "sent": "Relational networks [Santoro et al, 2017] and interaction networks [Battaglia et al, 2016] are the most directly comparable to ours.",
              "tag": "Result"
            },
            {
              "sent": "These models correspond to using a single step of equation 3.",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 39,
          "sentences": [
            {
              "sent": "Since it only does one step it cannot naturally do complex multi-step relational reasoning.",
              "tag": "Method"
            },
            {
              "sent": "In order to solve the tasks that require more than a single step it must compress all the relevant relations into a fixed size vector, then perform the remaining relational reasoning in the last forward layers.",
              "tag": "Claim"
            },
            {
              "sent": "Relational networks, interaction networks and our proposed model can all be seen as an instance of Graph Neural Networks [Scarselli et al, 2009, Gilmer et al, 2017.",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 44,
          "sentences": [
            {
              "sent": "There is rich literature on combining symbolic reasoning and logic with sub-symbolic distributed representations which goes all the way back to the birth of the idea of parallel distributed processing McCulloch and Pitts [1943].",
              "tag": "Claim"
            },
            {
              "sent": "See [Raedt et al, 2016, Besold et al, 2017 for two recent surveys.",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 45,
          "sentences": [
            {
              "sent": "Here we describe only a few recent methods.",
              "tag": "Claim"
            },
            {
              "sent": "Serafini and Garcez [2016] introduces the Logic Tensor Network (LTN) which describes a first order logic in which symbols are grounded as vector embeddings, and predicates and functions are grounded as tensor networks.",
              "tag": "Method"
            },
            {
              "sent": "The embeddings and tensor networks are then optimized jointly to maximize a fuzzy satisfiability measure over a set of known facts and fuzzy constraints.",
              "tag": "Claim"
            },
            {
              "sent": "\u0160ourek et al [2015] introduces the Lifted Relational Network which combines relational logic with neural networks by creating neural networks from lifted rules and training examples, such that the connections between neurons created from the same lifted rules shares weights.",
              "tag": "Claim"
            },
            {
              "sent": "Our approach differs fundamentally in that we do not aim to bridge symbolic and sub-symbolic methods.",
              "tag": "Claim"
            },
            {
              "sent": "Instead we stay completely in the sub-symbolic realm.",
              "tag": "Claim"
            },
            {
              "sent": "We do not introduce or consider any explicit logic, aim to discover (fuzzy) logic rules, or attempt to include prior knowledge in the form of logical constraints.",
              "tag": "Claim"
            },
            {
              "sent": "Amos and Kolter [2017] Introduces OptNet, a neural network layer that solve quadratic programs using an efficient differentiable solver.",
              "tag": "Method"
            },
            {
              "sent": "OptNet is trained to solve 4x4 Sudokus amongst other problems and beats the deep convolutional network baseline as described in Park [2016].",
              "tag": "Method"
            },
            {
              "sent": "Unfortunately we cannot compare to OptNet directly as it has computational issues scaling to 9x9 Sudokus (Brandon Amos, 2018, personal communication).",
              "tag": "Claim"
            }
          ]
        },
        {
          "par_id": 46,
          "sentences": [
            {
              "sent": "Sukhbaatar et al [2016] proposes the Communication Network (CommNet) for learning multi-agent cooperation and communication using back-propagation.",
              "tag": "Claim"
            },
            {
              "sent": "It is similar to our recurrent relational network, but differs in key aspects.",
              "tag": "Method"
            },
            {
              "sent": "The messages passed between all nodes at a given step are the same, corresponding to the average of all the node hidden states.",
              "tag": "Method"
            },
            {
              "sent": "Also, it is not trained to minimize the loss on every step of the algorithm.",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "bAbI experimental details",
      "selected_sentences": []
    },
    {
      "section_name": "bAbI ablation experiments",
      "selected_sentences": [
        {
          "par_id": 51,
          "sentences": [
            {
              "sent": "To test which parts of the proposed model is important to solving the bAbI tasks we perform ablation experiments.",
              "tag": "Method"
            },
            {
              "sent": "One of the main differences between the relational network and our proposed model, aside from the recurrent steps, is that we encode the sentences and question together.",
              "tag": "Method"
            },
            {
              "sent": "We ablate the model in two ways to test how important this is. 1) Using a single linear layer instead of the 4-layer MLP baseline, and 2) Not encoding them together.",
              "tag": "Method"
            },
            {
              "sent": "In this case the node hidden states are initialized to the fact encodings.",
              "tag": "Method"
            },
            {
              "sent": "We found dropout to be important, so we also perform an ablation experiment without dropout.",
              "tag": "Method"
            },
            {
              "sent": "We run each ablation experiment eight times.",
              "tag": "Method"
            },
            {
              "sent": "We also do pseudo-ablation experiments with fewer steps by measuring at each step of the RRN.",
              "tag": "Method"
            },
            {
              "sent": "See table 3 3: BaBi ablation results.",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Pretty-CLEVR experimental details",
      "selected_sentences": [
        {
          "par_id": 56,
          "sentences": [
            {
              "sent": "Our output function r is a MLP with a dropout fraction of 0.5 in the penultimate layer.",
              "tag": "Method"
            },
            {
              "sent": "The last layer has 16 hidden linear units.",
              "tag": "Method"
            },
            {
              "sent": "We run our recurrent relational network for 4 steps.",
              "tag": "Method"
            }
          ]
        }
      ]
    },
    {
      "section_name": "Sudoku dataset",
      "selected_sentences": []
    },
    {
      "section_name": "Sudoku experimental details",
      "selected_sentences": []
    },
    {
      "section_name": "Sudoku relational network baseline details",
      "selected_sentences": []
    },
    {
      "section_name": "Age arithmetic task details",
      "selected_sentences": [
        {
          "par_id": 72,
          "sentences": [
            {
              "sent": "Recurrent relational network on a fully connected graph with 3 nodes.",
              "tag": "Claim"
            },
            {
              "sent": "Subscripts denote node indices and superscripts denote steps t.",
              "tag": "Method"
            },
            {
              "sent": "The dashed lines indicate the recurrent connections.",
              "tag": "Method"
            }
          ]
        }
      ]
    }
  ],
  "title": "Recurrent Relational Networks"
}