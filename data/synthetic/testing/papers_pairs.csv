,Unnamed: 0,paper_1,paper_2,paper_1_id,paper_2_id,arxiv_id_1,arxiv_id_2,task,datasets_diff,approach_diff
0,0,1_paper_2,1_paper_6,20,21,1602.0241,1711.00066,Language Modelling,"sent: Article 1 uses One Billion Word dataset, while Article 2 uses multiple datasets:  Penn Treebank  Word Level , and  WikiText-2.","sent: Article 1 uses LSTM-8192-1024 approach for Language Modelling, while Article 2 uses AWD-LSTM 3-layer with Fraternal dropout for Language Modelling. sent: Article 1 uses LSTM-8192-1024   CNN Input approach for Language Modelling, while Article 2 uses AWD-LSTM 3-layer with Fraternal dropout for Language Modelling."
1,1,1_paper_2,1_paper_12,20,24,1602.0241,1701.06538,Language Modelling,"sent: Article 1 and Article 2 use One Billion Word dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-French, and  WMT2014 English-German.","sent: Article 1 uses LSTM-8192-1024 approach for Language Modelling, while Article 2 uses MoE for Language Modelling. sent: Article 1 uses LSTM-8192-1024 approach for Language Modelling, while Article 2 uses Low-Budget MoE for Language Modelling. sent: Article 1 uses LSTM-8192-1024 approach for Language Modelling, while Article 2 uses High-Budget MoE for Language Modelling. sent: Article 1 uses LSTM-8192-1024   CNN Input approach for Language Modelling, while Article 2 uses MoE for Language Modelling. sent: Article 1 uses LSTM-8192-1024   CNN Input approach for Language Modelling, while Article 2 uses Low-Budget MoE for Language Modelling. sent: Article 1 uses LSTM-8192-1024   CNN Input approach for Language Modelling, while Article 2 uses High-Budget MoE for Language Modelling."
2,2,1_paper_2,1_paper_4,20,26,1602.0241,1412.1454,Language Modelling,sent: Article 1 and Article 2 use One Billion Word dataset.,"sent: Article 1 uses LSTM-8192-1024 approach for Language Modelling, while Article 2 uses Sparse Non-Negative for Language Modelling. sent: Article 1 uses LSTM-8192-1024   CNN Input approach for Language Modelling, while Article 2 uses Sparse Non-Negative for Language Modelling."
3,3,1_paper_6,1_paper_3,21,23,1711.00066,1602.0241,Language Modelling,"sent: Article 1 uses multiple datasets:  Penn Treebank  Word Level , and  WikiText-2, while Article 2 uses One Billion Word dataset.","sent: Article 1 uses AWD-LSTM 3-layer with Fraternal dropout approach for Language Modelling, while Article 2 uses LSTM-8192-1024 for Language Modelling. sent: Article 1 uses AWD-LSTM 3-layer with Fraternal dropout approach for Language Modelling, while Article 2 uses LSTM-8192-1024   CNN Input for Language Modelling."
4,4,1_paper_6,1_paper_12,21,24,1711.00066,1701.06538,Language Modelling,"sent: Article 1 uses multiple datasets:  Penn Treebank  Word Level , and  WikiText-2, while Article 2 uses multiple datasets:  WMT2014 English-French,  One Billion Word, and  WMT2014 English-German.","sent: Article 1 uses AWD-LSTM 3-layer with Fraternal dropout approach for Language Modelling, while Article 2 uses MoE for Language Modelling. sent: Article 1 uses AWD-LSTM 3-layer with Fraternal dropout approach for Language Modelling, while Article 2 uses Low-Budget MoE for Language Modelling. sent: Article 1 uses AWD-LSTM 3-layer with Fraternal dropout approach for Language Modelling, while Article 2 uses High-Budget MoE for Language Modelling."
5,5,1_paper_6,1_paper_4,21,26,1711.00066,1412.1454,Language Modelling,"sent: Article 1 uses multiple datasets:  Penn Treebank  Word Level , and  WikiText-2, while Article 2 uses One Billion Word dataset.","sent: Article 1 uses AWD-LSTM 3-layer with Fraternal dropout approach for Language Modelling, while Article 2 uses Sparse Non-Negative for Language Modelling."
6,6,1_paper_12,1_paper_9,24,25,1701.06538,1711.00066,Language Modelling,"sent: Article 1 uses multiple datasets:  WMT2014 English-French,  One Billion Word, and  WMT2014 English-German, while Article 2 uses multiple datasets:  Penn Treebank  Word Level , and  WikiText-2.","sent: Article 1 uses MoE approach for Language Modelling, while Article 2 uses AWD-LSTM 3-layer with Fraternal dropout for Language Modelling. sent: Article 1 uses Low-Budget MoE approach for Language Modelling, while Article 2 uses AWD-LSTM 3-layer with Fraternal dropout for Language Modelling. sent: Article 1 uses High-Budget MoE approach for Language Modelling, while Article 2 uses AWD-LSTM 3-layer with Fraternal dropout for Language Modelling."
7,7,1_paper_12,1_paper_4,24,26,1701.06538,1412.1454,Language Modelling,"sent: Article 1 and Article 2 use One Billion Word dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-French, and  WMT2014 English-German, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses MoE approach for Language Modelling, while Article 2 uses Sparse Non-Negative for Language Modelling. sent: Article 1 uses Low-Budget MoE approach for Language Modelling, while Article 2 uses Sparse Non-Negative for Language Modelling. sent: Article 1 uses High-Budget MoE approach for Language Modelling, while Article 2 uses Sparse Non-Negative for Language Modelling."
8,8,1_paper_12,1_paper_0,24,27,1701.06538,1602.0241,Language Modelling,"sent: Article 1 and Article 2 use One Billion Word dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-French, and  WMT2014 English-German, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses MoE approach for Language Modelling, while Article 2 uses LSTM-8192-1024 for Language Modelling. sent: Article 1 uses MoE approach for Language Modelling, while Article 2 uses LSTM-8192-1024   CNN Input for Language Modelling. sent: Article 1 uses Low-Budget MoE approach for Language Modelling, while Article 2 uses LSTM-8192-1024 for Language Modelling. sent: Article 1 uses Low-Budget MoE approach for Language Modelling, while Article 2 uses LSTM-8192-1024   CNN Input for Language Modelling. sent: Article 1 uses High-Budget MoE approach for Language Modelling, while Article 2 uses LSTM-8192-1024 for Language Modelling. sent: Article 1 uses High-Budget MoE approach for Language Modelling, while Article 2 uses LSTM-8192-1024   CNN Input for Language Modelling."
9,9,1_paper_4,1_paper_0,26,27,1412.1454,1602.0241,Language Modelling,sent: Article 1 and Article 2 use One Billion Word dataset.,"sent: Article 1 uses Sparse Non-Negative approach for Language Modelling, while Article 2 uses LSTM-8192-1024 for Language Modelling. sent: Article 1 uses Sparse Non-Negative approach for Language Modelling, while Article 2 uses LSTM-8192-1024   CNN Input for Language Modelling."
10,10,1_paper_4,1_paper_10,26,28,1412.1454,1711.00066,Language Modelling,"sent: Article 1 uses One Billion Word dataset, while Article 2 uses multiple datasets:  Penn Treebank  Word Level , and  WikiText-2.","sent: Article 1 uses Sparse Non-Negative approach for Language Modelling, while Article 2 uses AWD-LSTM 3-layer with Fraternal dropout for Language Modelling."
11,11,1_paper_4,1_paper_13,26,211,1412.1454,1701.06538,Language Modelling,"sent: Article 1 and Article 2 use One Billion Word dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-French, and  WMT2014 English-German.","sent: Article 1 uses Sparse Non-Negative approach for Language Modelling, while Article 2 uses MoE for Language Modelling. sent: Article 1 uses Sparse Non-Negative approach for Language Modelling, while Article 2 uses Low-Budget MoE for Language Modelling. sent: Article 1 uses Sparse Non-Negative approach for Language Modelling, while Article 2 uses High-Budget MoE for Language Modelling."
12,12,2_paper_0,2_paper_2,30,31,1703.0129,1807.03342,Weakly Supervised Object Detection,"sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses Self-paced curriculum learning approach for Weakly Supervised Object Detection, while Article 2 uses PCL-OB-G-Ens   FRCNN for Weakly Supervised Object Detection."
13,13,2_paper_0,2_paper_1,30,33,1703.0129,1511.03776,Weakly Supervised Object Detection,"sent: Article 1 uses PASCAL VOC 2007 dataset, while Article 2 uses COCO dataset.","sent: Article 1 uses Self-paced curriculum learning approach for Weakly Supervised Object Detection, while Article 2 uses ProNet for Weakly Supervised Object Detection."
14,14,2_paper_2,2_paper_1,31,33,1807.03342,1511.03776,Weakly Supervised Object Detection,"sent: Article 1 uses multiple datasets:  ImageNet,  PASCAL VOC 2007, and  PASCAL VOC 2012, while Article 2 uses COCO dataset.","sent: Article 1 uses PCL-OB-G-Ens   FRCNN approach for Weakly Supervised Object Detection, while Article 2 uses ProNet for Weakly Supervised Object Detection."
15,15,2_paper_1,2_paper_3,33,34,1511.03776,1807.03342,Weakly Supervised Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses multiple datasets:  ImageNet,  PASCAL VOC 2007, and  PASCAL VOC 2012.","sent: Article 1 uses ProNet approach for Weakly Supervised Object Detection, while Article 2 uses PCL-OB-G-Ens   FRCNN for Weakly Supervised Object Detection."
16,16,3_paper_0,3_paper_7,40,42,1604.08242,1609.03528,Speech Recognition,sent: Article 1 and Article 2 use swb hub 500 WER fullSWBCH dataset.sent: Article 1 and Article 2 use Switchboard   Hub500 dataset.,"sent: Article 1 uses IBM 2016 approach for Speech Recognition, while Article 2 uses RNNLM for Speech Recognition. sent: Article 1 uses IBM 2016 approach for Speech Recognition, while Article 2 uses VGG Resnet LACE BiLSTM acoustic model trained on SWB Fisher CH  N-gram   RNNLM language model trained on Switchboard Fisher Gigaword Broadcast for Speech Recognition. sent: Article 1 uses IBM 2016 approach for Speech Recognition, while Article 2 uses Microsoft 2016 for Speech Recognition. sent: Article 1 uses RNN   VGG   LSTM acoustic model trained on SWB Fisher CH  N-gram    model M    NNLM language model approach for Speech Recognition, while Article 2 uses RNNLM for Speech Recognition. sent: Article 1 uses RNN   VGG   LSTM acoustic model trained on SWB Fisher CH  N-gram    model M    NNLM language model approach for Speech Recognition, while Article 2 uses VGG Resnet LACE BiLSTM acoustic model trained on SWB Fisher CH  N-gram   RNNLM language model trained on Switchboard Fisher Gigaword Broadcast for Speech Recognition. sent: Article 1 uses RNN   VGG   LSTM acoustic model trained on SWB Fisher CH  N-gram    model M    NNLM language model approach for Speech Recognition, while Article 2 uses Microsoft 2016 for Speech Recognition."
17,17,3_paper_0,3_paper_3,40,44,1604.08242,1303.5778,Speech Recognition,"sent: Article 1 uses multiple datasets:  swb hub 500 WER fullSWBCH, and  Switchboard   Hub500, while Article 2 uses TIMIT dataset.","sent: Article 1 uses IBM 2016 approach for Speech Recognition, while Article 2 uses Bi-LSTM   skip connections w  CTC for Speech Recognition. sent: Article 1 uses RNN   VGG   LSTM acoustic model trained on SWB Fisher CH  N-gram    model M    NNLM language model approach for Speech Recognition, while Article 2 uses Bi-LSTM   skip connections w  CTC for Speech Recognition."
18,18,3_paper_7,3_paper_3,42,44,1609.03528,1303.5778,Speech Recognition,"sent: Article 1 uses multiple datasets:  swb hub 500 WER fullSWBCH, and  Switchboard   Hub500, while Article 2 uses TIMIT dataset.","sent: Article 1 uses RNNLM approach for Speech Recognition, while Article 2 uses Bi-LSTM   skip connections w  CTC for Speech Recognition. sent: Article 1 uses VGG Resnet LACE BiLSTM acoustic model trained on SWB Fisher CH  N-gram   RNNLM language model trained on Switchboard Fisher Gigaword Broadcast approach for Speech Recognition, while Article 2 uses Bi-LSTM   skip connections w  CTC for Speech Recognition. sent: Article 1 uses Microsoft 2016 approach for Speech Recognition, while Article 2 uses Bi-LSTM   skip connections w  CTC for Speech Recognition."
19,19,3_paper_7,3_paper_2,42,46,1609.03528,1604.08242,Speech Recognition,sent: Article 1 and Article 2 use swb hub 500 WER fullSWBCH dataset.sent: Article 1 and Article 2 use Switchboard   Hub500 dataset.,"sent: Article 1 uses RNNLM approach for Speech Recognition, while Article 2 uses IBM 2016 for Speech Recognition. sent: Article 1 uses RNNLM approach for Speech Recognition, while Article 2 uses RNN   VGG   LSTM acoustic model trained on SWB Fisher CH  N-gram    model M    NNLM language model for Speech Recognition. sent: Article 1 uses VGG Resnet LACE BiLSTM acoustic model trained on SWB Fisher CH  N-gram   RNNLM language model trained on Switchboard Fisher Gigaword Broadcast approach for Speech Recognition, while Article 2 uses IBM 2016 for Speech Recognition. sent: Article 1 uses VGG Resnet LACE BiLSTM acoustic model trained on SWB Fisher CH  N-gram   RNNLM language model trained on Switchboard Fisher Gigaword Broadcast approach for Speech Recognition, while Article 2 uses RNN   VGG   LSTM acoustic model trained on SWB Fisher CH  N-gram    model M    NNLM language model for Speech Recognition. sent: Article 1 uses Microsoft 2016 approach for Speech Recognition, while Article 2 uses IBM 2016 for Speech Recognition. sent: Article 1 uses Microsoft 2016 approach for Speech Recognition, while Article 2 uses RNN   VGG   LSTM acoustic model trained on SWB Fisher CH  N-gram    model M    NNLM language model for Speech Recognition."
20,20,3_paper_3,3_paper_4,44,45,1303.5778,1609.03528,Speech Recognition,"sent: Article 1 uses TIMIT dataset, while Article 2 uses multiple datasets:  swb hub 500 WER fullSWBCH, and  Switchboard   Hub500.","sent: Article 1 uses Bi-LSTM   skip connections w  CTC approach for Speech Recognition, while Article 2 uses RNNLM for Speech Recognition. sent: Article 1 uses Bi-LSTM   skip connections w  CTC approach for Speech Recognition, while Article 2 uses VGG Resnet LACE BiLSTM acoustic model trained on SWB Fisher CH  N-gram   RNNLM language model trained on Switchboard Fisher Gigaword Broadcast for Speech Recognition. sent: Article 1 uses Bi-LSTM   skip connections w  CTC approach for Speech Recognition, while Article 2 uses Microsoft 2016 for Speech Recognition."
21,21,3_paper_3,3_paper_2,44,46,1303.5778,1604.08242,Speech Recognition,"sent: Article 1 uses TIMIT dataset, while Article 2 uses multiple datasets:  swb hub 500 WER fullSWBCH, and  Switchboard   Hub500.","sent: Article 1 uses Bi-LSTM   skip connections w  CTC approach for Speech Recognition, while Article 2 uses IBM 2016 for Speech Recognition. sent: Article 1 uses Bi-LSTM   skip connections w  CTC approach for Speech Recognition, while Article 2 uses RNN   VGG   LSTM acoustic model trained on SWB Fisher CH  N-gram    model M    NNLM language model for Speech Recognition."
22,22,4_paper_0,4_paper_2,50,52,1512.00567,1409.4842,Image Classification,"sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses ImageNet Detection dataset.","sent: Article 1 uses Inception V3 approach for Image Classification, while Article 2 uses Inception V1 for Image Classification."
23,23,4_paper_0,4_paper_6,50,53,1512.00567,1406.6909,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  STL-10, and  CIFAR-10.","sent: Article 1 uses Inception V3 approach for Image Classification, while Article 2 uses Discriminative Unsupervised Feature Learning with Convolutional Neural Networks for Image Classification."
24,24,4_paper_0,4_paper_4,50,54,1512.00567,1412.7149,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses MNIST dataset.","sent: Article 1 uses Inception V3 approach for Image Classification, while Article 2 uses Deep Fried Convnets for Image Classification."
25,25,4_paper_0,4_paper_5,50,56,1512.00567,1412.6572,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses MNIST dataset.","sent: Article 1 uses Inception V3 approach for Image Classification, while Article 2 uses Explaining and Harnessing Adversarial Examples for Image Classification."
26,26,4_paper_2,4_paper_6,52,53,1409.4842,1406.6909,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  ImageNet Detection, while Article 2 uses multiple datasets:  STL-10, and  CIFAR-10.","sent: Article 1 uses Inception V1 approach for Image Classification, while Article 2 uses Discriminative Unsupervised Feature Learning with Convolutional Neural Networks for Image Classification."
27,27,4_paper_2,4_paper_4,52,54,1409.4842,1412.7149,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  ImageNet Detection, while Article 2 uses MNIST dataset.","sent: Article 1 uses Inception V1 approach for Image Classification, while Article 2 uses Deep Fried Convnets for Image Classification."
28,28,4_paper_2,4_paper_5,52,56,1409.4842,1412.6572,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  ImageNet Detection, while Article 2 uses MNIST dataset.","sent: Article 1 uses Inception V1 approach for Image Classification, while Article 2 uses Explaining and Harnessing Adversarial Examples for Image Classification."
29,29,4_paper_6,4_paper_4,53,54,1406.6909,1412.7149,Image Classification,"sent: Article 1 uses multiple datasets:  STL-10, and  CIFAR-10, while Article 2 uses MNIST dataset.","sent: Article 1 uses Discriminative Unsupervised Feature Learning with Convolutional Neural Networks approach for Image Classification, while Article 2 uses Deep Fried Convnets for Image Classification."
30,30,4_paper_6,4_paper_3,53,55,1406.6909,1409.4842,Image Classification,"sent: Article 1 uses multiple datasets:  STL-10, and  CIFAR-10, while Article 2 uses multiple datasets:  ImageNet, and  ImageNet Detection.","sent: Article 1 uses Discriminative Unsupervised Feature Learning with Convolutional Neural Networks approach for Image Classification, while Article 2 uses Inception V1 for Image Classification."
31,31,4_paper_6,4_paper_5,53,56,1406.6909,1412.6572,Image Classification,"sent: Article 1 uses multiple datasets:  STL-10, and  CIFAR-10, while Article 2 uses MNIST dataset.","sent: Article 1 uses Discriminative Unsupervised Feature Learning with Convolutional Neural Networks approach for Image Classification, while Article 2 uses Explaining and Harnessing Adversarial Examples for Image Classification."
32,32,4_paper_4,4_paper_3,54,55,1412.7149,1409.4842,Image Classification,"sent: Article 1 uses MNIST dataset, while Article 2 uses multiple datasets:  ImageNet, and  ImageNet Detection.","sent: Article 1 uses Deep Fried Convnets approach for Image Classification, while Article 2 uses Inception V1 for Image Classification."
33,33,4_paper_4,4_paper_5,54,56,1412.7149,1412.6572,Image Classification,sent: Article 1 and Article 2 use MNIST dataset.,"sent: Article 1 uses Deep Fried Convnets approach for Image Classification, while Article 2 uses Explaining and Harnessing Adversarial Examples for Image Classification."
34,34,4_paper_4,4_paper_7,54,57,1412.7149,1406.6909,Image Classification,"sent: Article 1 uses MNIST dataset, while Article 2 uses multiple datasets:  STL-10, and  CIFAR-10.","sent: Article 1 uses Deep Fried Convnets approach for Image Classification, while Article 2 uses Discriminative Unsupervised Feature Learning with Convolutional Neural Networks for Image Classification."
35,35,4_paper_5,4_paper_7,56,57,1412.6572,1406.6909,Image Classification,"sent: Article 1 uses MNIST dataset, while Article 2 uses multiple datasets:  STL-10, and  CIFAR-10.","sent: Article 1 uses Explaining and Harnessing Adversarial Examples approach for Image Classification, while Article 2 uses Discriminative Unsupervised Feature Learning with Convolutional Neural Networks for Image Classification."
36,36,11_paper_1,11_paper_5,120,121,1809.0837,1603.04351,Dependency Parsing,"sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 uses multiple datasets:  CCGBank,  Ontonotes v5  English , and  CoNLL 2003  English , while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses CVT   Multi-Task approach for Dependency Parsing, while Article 2 uses BIST graph-based parser for Dependency Parsing. sent: Article 1 uses CVT   Multi-Task approach for Dependency Parsing, while Article 2 uses BIST transition-based parser for Dependency Parsing. sent: Article 1 uses Clark et al  approach for Dependency Parsing, while Article 2 uses BIST graph-based parser for Dependency Parsing. sent: Article 1 uses Clark et al  approach for Dependency Parsing, while Article 2 uses BIST transition-based parser for Dependency Parsing."
37,37,11_paper_5,11_paper_2,121,122,1603.04351,1809.0837,Dependency Parsing,"sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  CCGBank,  Ontonotes v5  English , and  CoNLL 2003  English .","sent: Article 1 uses BIST graph-based parser approach for Dependency Parsing, while Article 2 uses CVT   Multi-Task for Dependency Parsing. sent: Article 1 uses BIST graph-based parser approach for Dependency Parsing, while Article 2 uses Clark et al  for Dependency Parsing. sent: Article 1 uses BIST transition-based parser approach for Dependency Parsing, while Article 2 uses CVT   Multi-Task for Dependency Parsing. sent: Article 1 uses BIST transition-based parser approach for Dependency Parsing, while Article 2 uses Clark et al  for Dependency Parsing."
38,38,12_paper_5,12_paper_3,130,131,1809.02279,1505.07818,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  SST-2 Binary classification,  SNLI,  SST-5 Fine-grained classification, and  Quora Question Pairs, while Article 2 uses Multi-Domain Sentiment Dataset dataset.","sent: Article 1 uses Bi-CAS-LSTM approach for Sentiment Analysis, while Article 2 uses DANN for Sentiment Analysis. sent: Article 1 uses 300D 2-layer Bi-CAS-LSTM approach for Sentiment Analysis, while Article 2 uses DANN for Sentiment Analysis."
39,39,12_paper_5,12_paper_8,130,132,1809.02279,1804.0953,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  SST-2 Binary classification,  SNLI,  SST-5 Fine-grained classification, and  Quora Question Pairs, while Article 2 uses Multi-Domain Sentiment Dataset dataset.","sent: Article 1 uses Bi-CAS-LSTM approach for Sentiment Analysis, while Article 2 uses Multi-task tri-training for Sentiment Analysis. sent: Article 1 uses 300D 2-layer Bi-CAS-LSTM approach for Sentiment Analysis, while Article 2 uses Multi-task tri-training for Sentiment Analysis."
40,40,12_paper_3,12_paper_8,131,132,1505.07818,1804.0953,Sentiment Analysis,sent: Article 1 and Article 2 use Multi-Domain Sentiment Dataset dataset.,"sent: Article 1 uses DANN approach for Sentiment Analysis, while Article 2 uses Multi-task tri-training for Sentiment Analysis."
41,41,12_paper_3,12_paper_6,131,1310,1505.07818,1809.02279,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses multiple datasets:  SST-2 Binary classification,  SNLI,  SST-5 Fine-grained classification, and  Quora Question Pairs.","sent: Article 1 uses DANN approach for Sentiment Analysis, while Article 2 uses Bi-CAS-LSTM for Sentiment Analysis. sent: Article 1 uses DANN approach for Sentiment Analysis, while Article 2 uses 300D 2-layer Bi-CAS-LSTM for Sentiment Analysis."
42,42,12_paper_8,12_paper_2,132,133,1804.0953,1505.07818,Sentiment Analysis,sent: Article 1 and Article 2 use Multi-Domain Sentiment Dataset dataset.,"sent: Article 1 uses Multi-task tri-training approach for Sentiment Analysis, while Article 2 uses DANN for Sentiment Analysis."
43,43,12_paper_8,12_paper_6,132,1310,1804.0953,1809.02279,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses multiple datasets:  SST-2 Binary classification,  SNLI,  SST-5 Fine-grained classification, and  Quora Question Pairs.","sent: Article 1 uses Multi-task tri-training approach for Sentiment Analysis, while Article 2 uses Bi-CAS-LSTM for Sentiment Analysis. sent: Article 1 uses Multi-task tri-training approach for Sentiment Analysis, while Article 2 uses 300D 2-layer Bi-CAS-LSTM for Sentiment Analysis."
44,44,16_paper_15,16_paper_4,170,171,1607.04492,1806.05645,Natural Language Inference,"sent: Article 1 uses SNLI dataset, while Article 2 uses V-SNLI dataset.","sent: Article 1 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention approach for Natural Language Inference, while Article 2 uses BiMPM for Natural Language Inference. sent: Article 1 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention approach for Natural Language Inference, while Article 2 uses V-BiMPM for Natural Language Inference. sent: Article 1 uses 300D NTI-SLSTM-LSTM encoders approach for Natural Language Inference, while Article 2 uses BiMPM for Natural Language Inference. sent: Article 1 uses 300D NTI-SLSTM-LSTM encoders approach for Natural Language Inference, while Article 2 uses V-BiMPM for Natural Language Inference."
45,45,16_paper_15,16_paper_5,170,172,1607.04492,1709.04348,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Quora Question Pairs dataset.","sent: Article 1 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention approach for Natural Language Inference, while Article 2 uses 448D Densely Interactive Inference Network  DIIN  code  Ensemble for Natural Language Inference. sent: Article 1 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention approach for Natural Language Inference, while Article 2 uses DIIN for Natural Language Inference. sent: Article 1 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention approach for Natural Language Inference, while Article 2 uses 448D Densely Interactive Inference Network  DIIN  code  for Natural Language Inference. sent: Article 1 uses 300D NTI-SLSTM-LSTM encoders approach for Natural Language Inference, while Article 2 uses 448D Densely Interactive Inference Network  DIIN  code  Ensemble for Natural Language Inference. sent: Article 1 uses 300D NTI-SLSTM-LSTM encoders approach for Natural Language Inference, while Article 2 uses DIIN for Natural Language Inference. sent: Article 1 uses 300D NTI-SLSTM-LSTM encoders approach for Natural Language Inference, while Article 2 uses 448D Densely Interactive Inference Network  DIIN  code  for Natural Language Inference."
46,46,16_paper_15,16_paper_0,170,173,1607.04492,1512.08422,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention approach for Natural Language Inference, while Article 2 uses 300D Tree-based CNN encoders for Natural Language Inference. sent: Article 1 uses 300D NTI-SLSTM-LSTM encoders approach for Natural Language Inference, while Article 2 uses 300D Tree-based CNN encoders for Natural Language Inference."
47,47,16_paper_15,16_paper_11,170,1713,1607.04492,1809.02279,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SST-2 Binary classification,  SST-5 Fine-grained classification, and  Quora Question Pairs.","sent: Article 1 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention approach for Natural Language Inference, while Article 2 uses Bi-CAS-LSTM for Natural Language Inference. sent: Article 1 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention approach for Natural Language Inference, while Article 2 uses 300D 2-layer Bi-CAS-LSTM for Natural Language Inference. sent: Article 1 uses 300D NTI-SLSTM-LSTM encoders approach for Natural Language Inference, while Article 2 uses Bi-CAS-LSTM for Natural Language Inference. sent: Article 1 uses 300D NTI-SLSTM-LSTM encoders approach for Natural Language Inference, while Article 2 uses 300D 2-layer Bi-CAS-LSTM for Natural Language Inference."
48,48,16_paper_4,16_paper_5,171,172,1806.05645,1709.04348,Natural Language Inference,"sent: Article 1 uses V-SNLI dataset, while Article 2 uses multiple datasets:  SNLI, and  Quora Question Pairs.","sent: Article 1 uses BiMPM approach for Natural Language Inference, while Article 2 uses 448D Densely Interactive Inference Network  DIIN  code  Ensemble for Natural Language Inference. sent: Article 1 uses BiMPM approach for Natural Language Inference, while Article 2 uses DIIN for Natural Language Inference. sent: Article 1 uses BiMPM approach for Natural Language Inference, while Article 2 uses 448D Densely Interactive Inference Network  DIIN  code  for Natural Language Inference. sent: Article 1 uses V-BiMPM approach for Natural Language Inference, while Article 2 uses 448D Densely Interactive Inference Network  DIIN  code  Ensemble for Natural Language Inference. sent: Article 1 uses V-BiMPM approach for Natural Language Inference, while Article 2 uses DIIN for Natural Language Inference. sent: Article 1 uses V-BiMPM approach for Natural Language Inference, while Article 2 uses 448D Densely Interactive Inference Network  DIIN  code  for Natural Language Inference."
49,49,16_paper_4,16_paper_0,171,173,1806.05645,1512.08422,Natural Language Inference,"sent: Article 1 uses V-SNLI dataset, while Article 2 uses SNLI dataset.","sent: Article 1 uses BiMPM approach for Natural Language Inference, while Article 2 uses 300D Tree-based CNN encoders for Natural Language Inference. sent: Article 1 uses V-BiMPM approach for Natural Language Inference, while Article 2 uses 300D Tree-based CNN encoders for Natural Language Inference."
50,50,16_paper_4,16_paper_14,171,174,1806.05645,1607.04492,Natural Language Inference,"sent: Article 1 uses V-SNLI dataset, while Article 2 uses SNLI dataset.","sent: Article 1 uses BiMPM approach for Natural Language Inference, while Article 2 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention for Natural Language Inference. sent: Article 1 uses BiMPM approach for Natural Language Inference, while Article 2 uses 300D NTI-SLSTM-LSTM encoders for Natural Language Inference. sent: Article 1 uses V-BiMPM approach for Natural Language Inference, while Article 2 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention for Natural Language Inference. sent: Article 1 uses V-BiMPM approach for Natural Language Inference, while Article 2 uses 300D NTI-SLSTM-LSTM encoders for Natural Language Inference."
51,51,16_paper_4,16_paper_11,171,1713,1806.05645,1809.02279,Natural Language Inference,"sent: Article 1 uses V-SNLI dataset, while Article 2 uses multiple datasets:  SST-2 Binary classification,  SNLI,  SST-5 Fine-grained classification, and  Quora Question Pairs.","sent: Article 1 uses BiMPM approach for Natural Language Inference, while Article 2 uses Bi-CAS-LSTM for Natural Language Inference. sent: Article 1 uses BiMPM approach for Natural Language Inference, while Article 2 uses 300D 2-layer Bi-CAS-LSTM for Natural Language Inference. sent: Article 1 uses V-BiMPM approach for Natural Language Inference, while Article 2 uses Bi-CAS-LSTM for Natural Language Inference. sent: Article 1 uses V-BiMPM approach for Natural Language Inference, while Article 2 uses 300D 2-layer Bi-CAS-LSTM for Natural Language Inference."
52,52,16_paper_5,16_paper_0,172,173,1709.04348,1512.08422,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses Quora Question Pairs dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses 448D Densely Interactive Inference Network  DIIN  code  Ensemble approach for Natural Language Inference, while Article 2 uses 300D Tree-based CNN encoders for Natural Language Inference. sent: Article 1 uses DIIN approach for Natural Language Inference, while Article 2 uses 300D Tree-based CNN encoders for Natural Language Inference. sent: Article 1 uses 448D Densely Interactive Inference Network  DIIN  code  approach for Natural Language Inference, while Article 2 uses 300D Tree-based CNN encoders for Natural Language Inference."
53,53,16_paper_5,16_paper_14,172,174,1709.04348,1607.04492,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses Quora Question Pairs dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses 448D Densely Interactive Inference Network  DIIN  code  Ensemble approach for Natural Language Inference, while Article 2 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention for Natural Language Inference. sent: Article 1 uses 448D Densely Interactive Inference Network  DIIN  code  Ensemble approach for Natural Language Inference, while Article 2 uses 300D NTI-SLSTM-LSTM encoders for Natural Language Inference. sent: Article 1 uses DIIN approach for Natural Language Inference, while Article 2 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention for Natural Language Inference. sent: Article 1 uses DIIN approach for Natural Language Inference, while Article 2 uses 300D NTI-SLSTM-LSTM encoders for Natural Language Inference. sent: Article 1 uses 448D Densely Interactive Inference Network  DIIN  code  approach for Natural Language Inference, while Article 2 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention for Natural Language Inference. sent: Article 1 uses 448D Densely Interactive Inference Network  DIIN  code  approach for Natural Language Inference, while Article 2 uses 300D NTI-SLSTM-LSTM encoders for Natural Language Inference."
54,54,16_paper_5,16_paper_3,172,178,1709.04348,1806.05645,Natural Language Inference,"sent: Article 1 uses multiple datasets:  SNLI, and  Quora Question Pairs, while Article 2 uses V-SNLI dataset.","sent: Article 1 uses 448D Densely Interactive Inference Network  DIIN  code  Ensemble approach for Natural Language Inference, while Article 2 uses BiMPM for Natural Language Inference. sent: Article 1 uses 448D Densely Interactive Inference Network  DIIN  code  Ensemble approach for Natural Language Inference, while Article 2 uses V-BiMPM for Natural Language Inference. sent: Article 1 uses DIIN approach for Natural Language Inference, while Article 2 uses BiMPM for Natural Language Inference. sent: Article 1 uses DIIN approach for Natural Language Inference, while Article 2 uses V-BiMPM for Natural Language Inference. sent: Article 1 uses 448D Densely Interactive Inference Network  DIIN  code  approach for Natural Language Inference, while Article 2 uses BiMPM for Natural Language Inference. sent: Article 1 uses 448D Densely Interactive Inference Network  DIIN  code  approach for Natural Language Inference, while Article 2 uses V-BiMPM for Natural Language Inference."
55,55,16_paper_5,16_paper_11,172,1713,1709.04348,1809.02279,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset.sent: Article 1 and Article 2 use Quora Question Pairs dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SST-2 Binary classification, and  SST-5 Fine-grained classification.","sent: Article 1 uses DIIN approach for Paraphrase Identification, while Article 2 uses Bi-CAS-LSTM for Paraphrase Identification. sent: Article 1 uses 448D Densely Interactive Inference Network  DIIN  code  Ensemble approach for Natural Language Inference, while Article 2 uses Bi-CAS-LSTM for Natural Language Inference. sent: Article 1 uses 448D Densely Interactive Inference Network  DIIN  code  Ensemble approach for Natural Language Inference, while Article 2 uses 300D 2-layer Bi-CAS-LSTM for Natural Language Inference. sent: Article 1 uses DIIN approach for Natural Language Inference, while Article 2 uses Bi-CAS-LSTM for Natural Language Inference. sent: Article 1 uses DIIN approach for Natural Language Inference, while Article 2 uses 300D 2-layer Bi-CAS-LSTM for Natural Language Inference. sent: Article 1 uses 448D Densely Interactive Inference Network  DIIN  code  approach for Natural Language Inference, while Article 2 uses Bi-CAS-LSTM for Natural Language Inference. sent: Article 1 uses 448D Densely Interactive Inference Network  DIIN  code  approach for Natural Language Inference, while Article 2 uses 300D 2-layer Bi-CAS-LSTM for Natural Language Inference."
56,56,16_paper_0,16_paper_14,173,174,1512.08422,1607.04492,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 300D Tree-based CNN encoders approach for Natural Language Inference, while Article 2 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention for Natural Language Inference. sent: Article 1 uses 300D Tree-based CNN encoders approach for Natural Language Inference, while Article 2 uses 300D NTI-SLSTM-LSTM encoders for Natural Language Inference."
57,57,16_paper_0,16_paper_8,173,175,1512.08422,1709.04348,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Quora Question Pairs dataset.","sent: Article 1 uses 300D Tree-based CNN encoders approach for Natural Language Inference, while Article 2 uses 448D Densely Interactive Inference Network  DIIN  code  Ensemble for Natural Language Inference. sent: Article 1 uses 300D Tree-based CNN encoders approach for Natural Language Inference, while Article 2 uses DIIN for Natural Language Inference. sent: Article 1 uses 300D Tree-based CNN encoders approach for Natural Language Inference, while Article 2 uses 448D Densely Interactive Inference Network  DIIN  code  for Natural Language Inference."
58,58,16_paper_0,16_paper_3,173,178,1512.08422,1806.05645,Natural Language Inference,"sent: Article 1 uses SNLI dataset, while Article 2 uses V-SNLI dataset.","sent: Article 1 uses 300D Tree-based CNN encoders approach for Natural Language Inference, while Article 2 uses BiMPM for Natural Language Inference. sent: Article 1 uses 300D Tree-based CNN encoders approach for Natural Language Inference, while Article 2 uses V-BiMPM for Natural Language Inference."
59,59,16_paper_0,16_paper_11,173,1713,1512.08422,1809.02279,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SST-2 Binary classification,  SST-5 Fine-grained classification, and  Quora Question Pairs.","sent: Article 1 uses 300D Tree-based CNN encoders approach for Natural Language Inference, while Article 2 uses Bi-CAS-LSTM for Natural Language Inference. sent: Article 1 uses 300D Tree-based CNN encoders approach for Natural Language Inference, while Article 2 uses 300D 2-layer Bi-CAS-LSTM for Natural Language Inference."
60,60,16_paper_11,16_paper_12,1713,1714,1809.02279,1607.04492,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  SST-2 Binary classification,  SST-5 Fine-grained classification, and  Quora Question Pairs, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Bi-CAS-LSTM approach for Natural Language Inference, while Article 2 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention for Natural Language Inference. sent: Article 1 uses Bi-CAS-LSTM approach for Natural Language Inference, while Article 2 uses 300D NTI-SLSTM-LSTM encoders for Natural Language Inference. sent: Article 1 uses 300D 2-layer Bi-CAS-LSTM approach for Natural Language Inference, while Article 2 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention for Natural Language Inference. sent: Article 1 uses 300D 2-layer Bi-CAS-LSTM approach for Natural Language Inference, while Article 2 uses 300D NTI-SLSTM-LSTM encoders for Natural Language Inference."
61,61,16_paper_11,16_paper_2,1713,1715,1809.02279,1512.08422,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  SST-2 Binary classification,  SST-5 Fine-grained classification, and  Quora Question Pairs, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Bi-CAS-LSTM approach for Natural Language Inference, while Article 2 uses 300D Tree-based CNN encoders for Natural Language Inference. sent: Article 1 uses 300D 2-layer Bi-CAS-LSTM approach for Natural Language Inference, while Article 2 uses 300D Tree-based CNN encoders for Natural Language Inference."
62,62,16_paper_11,16_paper_10,1713,1716,1809.02279,1709.04348,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset.sent: Article 1 and Article 2 use Quora Question Pairs dataset. sent: Article 1 uses multiple datasets:  SST-2 Binary classification, and  SST-5 Fine-grained classification, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Bi-CAS-LSTM approach for Paraphrase Identification, while Article 2 uses DIIN for Paraphrase Identification. sent: Article 1 uses Bi-CAS-LSTM approach for Natural Language Inference, while Article 2 uses 448D Densely Interactive Inference Network  DIIN  code  Ensemble for Natural Language Inference. sent: Article 1 uses Bi-CAS-LSTM approach for Natural Language Inference, while Article 2 uses DIIN for Natural Language Inference. sent: Article 1 uses Bi-CAS-LSTM approach for Natural Language Inference, while Article 2 uses 448D Densely Interactive Inference Network  DIIN  code  for Natural Language Inference. sent: Article 1 uses 300D 2-layer Bi-CAS-LSTM approach for Natural Language Inference, while Article 2 uses 448D Densely Interactive Inference Network  DIIN  code  Ensemble for Natural Language Inference. sent: Article 1 uses 300D 2-layer Bi-CAS-LSTM approach for Natural Language Inference, while Article 2 uses DIIN for Natural Language Inference. sent: Article 1 uses 300D 2-layer Bi-CAS-LSTM approach for Natural Language Inference, while Article 2 uses 448D Densely Interactive Inference Network  DIIN  code  for Natural Language Inference."
63,63,17_paper_4,17_paper_0,180,181,1406.4729,1409.4842,Object Detection,"sent: Article 1 uses PASCAL VOC 2007 dataset, while Article 2 uses multiple datasets:  ImageNet, and  ImageNet Detection.","sent: Article 1 uses SPP  Overfeat-7  approach for Object Detection, while Article 2 uses Inception V1 for Object Detection."
64,64,17_paper_4,17_paper_1,180,182,1406.4729,1506.0264,Object Detection,sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset.,"sent: Article 1 uses SPP  Overfeat-7  approach for Object Detection, while Article 2 uses YOLO for Object Detection."
65,65,17_paper_4,17_paper_2,180,183,1406.4729,1604.0354,Object Detection,sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset.,"sent: Article 1 uses SPP  Overfeat-7  approach for Object Detection, while Article 2 uses OHEM for Object Detection."
66,66,17_paper_4,17_paper_3,180,184,1406.4729,1708.02863,Object Detection,sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset.,"sent: Article 1 uses SPP  Overfeat-7  approach for Object Detection, while Article 2 uses CoupleNet for Object Detection."
67,67,17_paper_0,17_paper_1,181,182,1409.4842,1506.0264,Object Detection,"sent: Article 1 uses multiple datasets:  ImageNet, and  ImageNet Detection, while Article 2 uses PASCAL VOC 2007 dataset.","sent: Article 1 uses Inception V1 approach for Object Detection, while Article 2 uses YOLO for Object Detection."
68,68,17_paper_0,17_paper_2,181,183,1409.4842,1604.0354,Object Detection,"sent: Article 1 uses multiple datasets:  ImageNet, and  ImageNet Detection, while Article 2 uses PASCAL VOC 2007 dataset.","sent: Article 1 uses Inception V1 approach for Object Detection, while Article 2 uses OHEM for Object Detection."
69,69,17_paper_0,17_paper_3,181,184,1409.4842,1708.02863,Object Detection,"sent: Article 1 uses multiple datasets:  ImageNet, and  ImageNet Detection, while Article 2 uses PASCAL VOC 2007 dataset.","sent: Article 1 uses Inception V1 approach for Object Detection, while Article 2 uses CoupleNet for Object Detection."
70,70,17_paper_1,17_paper_2,182,183,1506.0264,1604.0354,Object Detection,sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset.,"sent: Article 1 uses YOLO approach for Object Detection, while Article 2 uses OHEM for Object Detection."
71,71,17_paper_1,17_paper_3,182,184,1506.0264,1708.02863,Object Detection,sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset.,"sent: Article 1 uses YOLO approach for Object Detection, while Article 2 uses CoupleNet for Object Detection."
72,72,17_paper_2,17_paper_3,183,184,1604.0354,1708.02863,Object Detection,sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset.,"sent: Article 1 uses OHEM approach for Object Detection, while Article 2 uses CoupleNet for Object Detection."
73,73,18_paper_3,18_paper_15,190,197,1805.02704,1708.092,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses Urban100 - 4x upscaling dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DSRN approach for Image Super-Resolution, while Article 2 uses JMPF  for Image Super-Resolution."
74,74,18_paper_3,18_paper_0,190,1911,1805.02704,1712.05248,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses Urban100 - 4x upscaling dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DSRN approach for Image Super-Resolution, while Article 2 uses FARF  for Image Super-Resolution. sent: Article 1 uses DSRN approach for Image Super-Resolution, while Article 2 uses FAFR  for Image Super-Resolution."
75,75,18_paper_15,18_paper_6,197,198,1708.092,1805.02704,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Urban100 - 4x upscaling dataset.","sent: Article 1 uses JMPF  approach for Image Super-Resolution, while Article 2 uses DSRN for Image Super-Resolution."
76,76,18_paper_15,18_paper_0,197,1911,1708.092,1712.05248,Image Super-Resolution,sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset.,"sent: Article 1 uses JMPF  approach for Image Super-Resolution, while Article 2 uses FARF  for Image Super-Resolution. sent: Article 1 uses JMPF  approach for Image Super-Resolution, while Article 2 uses FAFR  for Image Super-Resolution."
77,77,23_paper_3,23_paper_1,240,242,1709.08325,1611.05666,Person Re-Identification,sent: Article 1 and Article 2 use Market-1501 dataset.,"sent: Article 1 uses PDF approach for Person Re-Identification, while Article 2 uses DLCE for Person Re-Identification."
78,78,29_paper_1,29_paper_5,300,302,1811.11662,1407.4023,Face Detection,"sent: Article 1 and Article 2 use WIDER Face  Hard  dataset. sent: Article 1 uses multiple datasets:  Annotated Faces in the Wild,  PASCAL Face, and  FDDB, while Article 2 uses multiple datasets:  WIDER Face  Medium , and  WIDER Face  Easy .","sent: Article 1 uses Anchor-based approach for Face Detection, while Article 2 uses ACF-WIDER for Face Detection."
79,79,29_paper_5,29_paper_2,302,305,1407.4023,1811.11662,Face Detection,"sent: Article 1 and Article 2 use WIDER Face  Hard  dataset. sent: Article 1 uses multiple datasets:  WIDER Face  Medium , and  WIDER Face  Easy , while Article 2 uses multiple datasets:  Annotated Faces in the Wild,  PASCAL Face, and  FDDB.","sent: Article 1 uses ACF-WIDER approach for Face Detection, while Article 2 uses Anchor-based for Face Detection."
80,80,31_paper_4,31_paper_3,320,321,1801.01641,1406.3676,Question Answering,"sent: Article 1 uses TrecQA dataset, while Article 2 uses WebQuestions dataset.","sent: Article 1 uses aNMM approach for Question Answering, while Article 2 uses Subgraph embeddings for Question Answering."
81,81,31_paper_3,31_paper_5,321,322,1406.3676,1801.01641,Question Answering,"sent: Article 1 uses WebQuestions dataset, while Article 2 uses TrecQA dataset.","sent: Article 1 uses Subgraph embeddings approach for Question Answering, while Article 2 uses aNMM for Question Answering."
82,82,32_paper_2,32_paper_3,330,332,1804.03287,1709.03612,Multi-Human Parsing,"sent: Article 1 and Article 2 use PASCAL-Person-Part dataset. sent: Article 1 uses multiple datasets:  MHP v2 0, and  MHP v1 0, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses NAN approach for Multi-Human Parsing, while Article 2 uses Holistic instance-level for Multi-Human Parsing."
83,83,32_paper_3,32_paper_0,332,333,1709.03612,1804.03287,Multi-Human Parsing,"sent: Article 1 and Article 2 use PASCAL-Person-Part dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MHP v2 0, and  MHP v1 0.","sent: Article 1 uses Holistic instance-level approach for Multi-Human Parsing, while Article 2 uses NAN for Multi-Human Parsing."
