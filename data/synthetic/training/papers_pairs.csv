,Unnamed: 0,paper_1,paper_2,paper_1_id,paper_2_id,arxiv_id_1,arxiv_id_2,task,datasets_diff,approach_diff
0,0,0_paper_2,0_paper_12,10,11,1611.08323,1511.07122,Real-Time Semantic Segmentation,"sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  CamVid,  PASCAL VOC 2012, and  ADE20K.","sent: Article 1 uses FRRN approach for Real-Time Semantic Segmentation, while Article 2 uses Dilation10 for Real-Time Semantic Segmentation. sent: Article 1 uses FRRN approach for Real-Time Semantic Segmentation, while Article 2 uses Dilated Convolutions for Real-Time Semantic Segmentation. sent: Article 1 uses FRRN approach for Real-Time Semantic Segmentation, while Article 2 uses DilatedNet for Real-Time Semantic Segmentation. sent: Article 1 uses FRRN approach for Semantic Segmentation, while Article 2 uses Dilation10 for Semantic Segmentation. sent: Article 1 uses FRRN approach for Semantic Segmentation, while Article 2 uses Dilated Convolutions for Semantic Segmentation. sent: Article 1 uses FRRN approach for Semantic Segmentation, while Article 2 uses DilatedNet for Semantic Segmentation."
1,1,0_paper_2,0_paper_7,10,12,1611.08323,1704.08545,Real-Time Semantic Segmentation,"sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CamVid dataset.","sent: Article 1 uses FRRN approach for Real-Time Semantic Segmentation, while Article 2 uses ICNet for Real-Time Semantic Segmentation. sent: Article 1 uses FRRN approach for Semantic Segmentation, while Article 2 uses ICNet for Semantic Segmentation."
2,2,0_paper_2,0_paper_9,10,18,1611.08323,1502.0324,Real-Time Semantic Segmentation,"sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  PASCAL VOC 2012, and  PASCAL Context.","sent: Article 1 uses FRRN approach for Real-Time Semantic Segmentation, while Article 2 uses CRF-RNN for Real-Time Semantic Segmentation. sent: Article 1 uses FRRN approach for Semantic Segmentation, while Article 2 uses CRF-RNN for Semantic Segmentation."
3,3,0_paper_12,0_paper_7,11,12,1511.07122,1704.08545,Real-Time Semantic Segmentation,"sent: Article 1 and Article 2 use CamVid dataset.sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses multiple datasets:  PASCAL VOC 2012, and  ADE20K, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Dilation10 approach for Real-Time Semantic Segmentation, while Article 2 uses ICNet for Real-Time Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Real-Time Semantic Segmentation, while Article 2 uses ICNet for Real-Time Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Real-Time Semantic Segmentation, while Article 2 uses ICNet for Real-Time Semantic Segmentation. sent: Article 1 uses Dilation10 approach for Semantic Segmentation, while Article 2 uses ICNet for Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Semantic Segmentation, while Article 2 uses ICNet for Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Semantic Segmentation, while Article 2 uses ICNet for Semantic Segmentation."
4,4,0_paper_12,0_paper_9,11,18,1511.07122,1502.0324,Real-Time Semantic Segmentation,"sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset.sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses multiple datasets:  CamVid, and  ADE20K, while Article 2 uses PASCAL Context dataset.","sent: Article 1 uses Dilation10 approach for Real-Time Semantic Segmentation, while Article 2 uses CRF-RNN for Real-Time Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Real-Time Semantic Segmentation, while Article 2 uses CRF-RNN for Real-Time Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Real-Time Semantic Segmentation, while Article 2 uses CRF-RNN for Real-Time Semantic Segmentation. sent: Article 1 uses Dilation10 approach for Semantic Segmentation, while Article 2 uses CRF-RNN for Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Semantic Segmentation, while Article 2 uses CRF-RNN for Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Semantic Segmentation, while Article 2 uses CRF-RNN for Semantic Segmentation."
5,5,0_paper_12,0_paper_1,11,114,1511.07122,1611.08323,Real-Time Semantic Segmentation,"sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses multiple datasets:  CamVid,  PASCAL VOC 2012, and  ADE20K, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Dilation10 approach for Real-Time Semantic Segmentation, while Article 2 uses FRRN for Real-Time Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Real-Time Semantic Segmentation, while Article 2 uses FRRN for Real-Time Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Real-Time Semantic Segmentation, while Article 2 uses FRRN for Real-Time Semantic Segmentation. sent: Article 1 uses Dilation10 approach for Semantic Segmentation, while Article 2 uses FRRN for Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Semantic Segmentation, while Article 2 uses FRRN for Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Semantic Segmentation, while Article 2 uses FRRN for Semantic Segmentation."
6,6,0_paper_7,0_paper_16,12,13,1704.08545,1511.07122,Real-Time Semantic Segmentation,"sent: Article 1 and Article 2 use CamVid dataset.sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  PASCAL VOC 2012, and  ADE20K.","sent: Article 1 uses ICNet approach for Real-Time Semantic Segmentation, while Article 2 uses Dilation10 for Real-Time Semantic Segmentation. sent: Article 1 uses ICNet approach for Real-Time Semantic Segmentation, while Article 2 uses Dilated Convolutions for Real-Time Semantic Segmentation. sent: Article 1 uses ICNet approach for Real-Time Semantic Segmentation, while Article 2 uses DilatedNet for Real-Time Semantic Segmentation. sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses Dilation10 for Semantic Segmentation. sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses Dilated Convolutions for Semantic Segmentation. sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses DilatedNet for Semantic Segmentation."
7,7,0_paper_7,0_paper_9,12,18,1704.08545,1502.0324,Real-Time Semantic Segmentation,"sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses CamVid dataset, while Article 2 uses multiple datasets:  PASCAL VOC 2012, and  PASCAL Context.","sent: Article 1 uses ICNet approach for Real-Time Semantic Segmentation, while Article 2 uses CRF-RNN for Real-Time Semantic Segmentation. sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses CRF-RNN for Semantic Segmentation."
8,8,0_paper_7,0_paper_1,12,114,1704.08545,1611.08323,Real-Time Semantic Segmentation,"sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses CamVid dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ICNet approach for Real-Time Semantic Segmentation, while Article 2 uses FRRN for Real-Time Semantic Segmentation. sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses FRRN for Semantic Segmentation."
9,9,0_paper_9,0_paper_5,18,19,1502.0324,1704.08545,Real-Time Semantic Segmentation,"sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses multiple datasets:  PASCAL VOC 2012, and  PASCAL Context, while Article 2 uses CamVid dataset.","sent: Article 1 uses CRF-RNN approach for Real-Time Semantic Segmentation, while Article 2 uses ICNet for Real-Time Semantic Segmentation. sent: Article 1 uses CRF-RNN approach for Semantic Segmentation, while Article 2 uses ICNet for Semantic Segmentation."
10,10,0_paper_9,0_paper_1,18,114,1502.0324,1611.08323,Real-Time Semantic Segmentation,"sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses multiple datasets:  PASCAL VOC 2012, and  PASCAL Context, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses CRF-RNN approach for Real-Time Semantic Segmentation, while Article 2 uses FRRN for Real-Time Semantic Segmentation. sent: Article 1 uses CRF-RNN approach for Semantic Segmentation, while Article 2 uses FRRN for Semantic Segmentation."
11,11,0_paper_9,0_paper_13,18,116,1502.0324,1511.07122,Real-Time Semantic Segmentation,"sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset.sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses PASCAL Context dataset, while Article 2 uses multiple datasets:  CamVid, and  ADE20K.","sent: Article 1 uses CRF-RNN approach for Real-Time Semantic Segmentation, while Article 2 uses Dilation10 for Real-Time Semantic Segmentation. sent: Article 1 uses CRF-RNN approach for Real-Time Semantic Segmentation, while Article 2 uses Dilated Convolutions for Real-Time Semantic Segmentation. sent: Article 1 uses CRF-RNN approach for Real-Time Semantic Segmentation, while Article 2 uses DilatedNet for Real-Time Semantic Segmentation. sent: Article 1 uses CRF-RNN approach for Semantic Segmentation, while Article 2 uses Dilation10 for Semantic Segmentation. sent: Article 1 uses CRF-RNN approach for Semantic Segmentation, while Article 2 uses Dilated Convolutions for Semantic Segmentation. sent: Article 1 uses CRF-RNN approach for Semantic Segmentation, while Article 2 uses DilatedNet for Semantic Segmentation."
12,12,1_paper_2,1_paper_6,20,21,1804.09337,1506.04579,Semantic Segmentation,"sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset. sent: Article 1 uses Cityscapes dataset, while Article 2 uses PASCAL Context dataset.","sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses ParseNet  for Semantic Segmentation. sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses ParseNet for Semantic Segmentation."
13,13,1_paper_2,1_paper_8,20,22,1804.09337,1712.02616,Semantic Segmentation,"sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses PASCAL VOC 2012 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses Mapillary for Semantic Segmentation."
14,14,1_paper_2,1_paper_3,20,23,1804.09337,1704.08545,Semantic Segmentation,"sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses PASCAL VOC 2012 dataset, while Article 2 uses CamVid dataset.","sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses ICNet for Semantic Segmentation."
15,15,1_paper_2,1_paper_12,20,24,1804.09337,1511.07122,Semantic Segmentation,"sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset.sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  CamVid, and  ADE20K.","sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses Dilation10 for Semantic Segmentation. sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses Dilated Convolutions for Semantic Segmentation. sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses DilatedNet for Semantic Segmentation."
16,16,1_paper_2,1_paper_9,20,25,1804.09337,1502.0324,Semantic Segmentation,"sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset.sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses PASCAL Context dataset.","sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses CRF-RNN for Semantic Segmentation."
17,17,1_paper_2,1_paper_4,20,26,1804.09337,1504.01013,Semantic Segmentation,"sent: Article 1 uses multiple datasets:  PASCAL VOC 2012, and  Cityscapes, while Article 2 uses PASCAL Context dataset.","sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation."
18,18,1_paper_2,1_paper_0,20,27,1804.09337,1611.08323,Semantic Segmentation,"sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses PASCAL VOC 2012 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses FRRN for Semantic Segmentation."
19,19,1_paper_2,1_paper_7,20,215,1804.09337,1707.02968,Semantic Segmentation,"sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset. sent: Article 1 uses Cityscapes dataset, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation."
20,20,1_paper_6,1_paper_8,21,22,1506.04579,1712.02616,Semantic Segmentation,"sent: Article 1 uses multiple datasets:  PASCAL VOC 2012, and  PASCAL Context, while Article 2 uses Cityscapes dataset.","sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses Mapillary for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses Mapillary for Semantic Segmentation."
21,21,1_paper_6,1_paper_3,21,23,1506.04579,1704.08545,Semantic Segmentation,"sent: Article 1 uses multiple datasets:  PASCAL VOC 2012, and  PASCAL Context, while Article 2 uses multiple datasets:  CamVid, and  Cityscapes.","sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses ICNet for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses ICNet for Semantic Segmentation."
22,22,1_paper_6,1_paper_12,21,24,1506.04579,1511.07122,Semantic Segmentation,"sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset. sent: Article 1 uses PASCAL Context dataset, while Article 2 uses multiple datasets:  CamVid,  ADE20K, and  Cityscapes.","sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses Dilation10 for Semantic Segmentation. sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses Dilated Convolutions for Semantic Segmentation. sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses DilatedNet for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses Dilation10 for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses Dilated Convolutions for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses DilatedNet for Semantic Segmentation."
23,23,1_paper_6,1_paper_9,21,25,1506.04579,1502.0324,Semantic Segmentation,"sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset.sent: Article 1 and Article 2 use PASCAL Context dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Cityscapes dataset.","sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses CRF-RNN for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses CRF-RNN for Semantic Segmentation."
24,24,1_paper_6,1_paper_4,21,26,1506.04579,1504.01013,Semantic Segmentation,"sent: Article 1 and Article 2 use PASCAL Context dataset. sent: Article 1 uses PASCAL VOC 2012 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation."
25,25,1_paper_6,1_paper_0,21,27,1506.04579,1611.08323,Semantic Segmentation,"sent: Article 1 uses multiple datasets:  PASCAL VOC 2012, and  PASCAL Context, while Article 2 uses Cityscapes dataset.","sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses FRRN for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses FRRN for Semantic Segmentation."
26,26,1_paper_6,1_paper_1,21,213,1506.04579,1804.09337,Semantic Segmentation,"sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset. sent: Article 1 uses PASCAL Context dataset, while Article 2 uses Cityscapes dataset.","sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses Smooth Network with Channel Attention Block for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses Smooth Network with Channel Attention Block for Semantic Segmentation."
27,27,1_paper_6,1_paper_7,21,215,1506.04579,1707.02968,Semantic Segmentation,"sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset. sent: Article 1 uses PASCAL Context dataset, while Article 2 uses ImageNet dataset.","sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation."
28,28,1_paper_8,1_paper_3,22,23,1712.02616,1704.08545,Semantic Segmentation,"sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CamVid dataset.","sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses ICNet for Semantic Segmentation."
29,29,1_paper_8,1_paper_12,22,24,1712.02616,1511.07122,Semantic Segmentation,"sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  CamVid,  PASCAL VOC 2012, and  ADE20K.","sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses Dilation10 for Semantic Segmentation. sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses Dilated Convolutions for Semantic Segmentation. sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses DilatedNet for Semantic Segmentation."
30,30,1_paper_8,1_paper_9,22,25,1712.02616,1502.0324,Semantic Segmentation,"sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  PASCAL VOC 2012, and  PASCAL Context.","sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses CRF-RNN for Semantic Segmentation."
31,31,1_paper_8,1_paper_4,22,26,1712.02616,1504.01013,Semantic Segmentation,"sent: Article 1 uses Cityscapes dataset, while Article 2 uses PASCAL Context dataset.","sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation."
32,32,1_paper_8,1_paper_0,22,27,1712.02616,1611.08323,Semantic Segmentation,sent: Article 1 and Article 2 use Cityscapes dataset.,"sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses FRRN for Semantic Segmentation."
33,33,1_paper_8,1_paper_5,22,29,1712.02616,1506.04579,Semantic Segmentation,"sent: Article 1 uses Cityscapes dataset, while Article 2 uses multiple datasets:  PASCAL VOC 2012, and  PASCAL Context.","sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses ParseNet  for Semantic Segmentation. sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses ParseNet for Semantic Segmentation."
34,34,1_paper_8,1_paper_1,22,213,1712.02616,1804.09337,Semantic Segmentation,"sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses PASCAL VOC 2012 dataset.","sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses Smooth Network with Channel Attention Block for Semantic Segmentation."
35,35,1_paper_8,1_paper_7,22,215,1712.02616,1707.02968,Semantic Segmentation,"sent: Article 1 uses Cityscapes dataset, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation."
36,36,1_paper_3,1_paper_4,23,26,1704.08545,1504.01013,Semantic Segmentation,"sent: Article 1 uses multiple datasets:  CamVid, and  Cityscapes, while Article 2 uses PASCAL Context dataset.","sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation."
37,37,1_paper_3,1_paper_5,23,29,1704.08545,1506.04579,Semantic Segmentation,"sent: Article 1 uses multiple datasets:  CamVid, and  Cityscapes, while Article 2 uses multiple datasets:  PASCAL VOC 2012, and  PASCAL Context.","sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses ParseNet  for Semantic Segmentation. sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses ParseNet for Semantic Segmentation."
38,38,1_paper_3,1_paper_1,23,213,1704.08545,1804.09337,Semantic Segmentation,"sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses CamVid dataset, while Article 2 uses PASCAL VOC 2012 dataset.","sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses Smooth Network with Channel Attention Block for Semantic Segmentation."
39,39,1_paper_3,1_paper_7,23,215,1704.08545,1707.02968,Semantic Segmentation,"sent: Article 1 uses multiple datasets:  CamVid, and  Cityscapes, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation."
40,40,1_paper_12,1_paper_4,24,26,1511.07122,1504.01013,Semantic Segmentation,"sent: Article 1 uses multiple datasets:  CamVid,  PASCAL VOC 2012,  ADE20K, and  Cityscapes, while Article 2 uses PASCAL Context dataset.","sent: Article 1 uses Dilation10 approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation."
41,41,1_paper_12,1_paper_5,24,29,1511.07122,1506.04579,Semantic Segmentation,"sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset. sent: Article 1 uses multiple datasets:  CamVid,  ADE20K, and  Cityscapes, while Article 2 uses PASCAL Context dataset.","sent: Article 1 uses Dilation10 approach for Semantic Segmentation, while Article 2 uses ParseNet  for Semantic Segmentation. sent: Article 1 uses Dilation10 approach for Semantic Segmentation, while Article 2 uses ParseNet for Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Semantic Segmentation, while Article 2 uses ParseNet  for Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Semantic Segmentation, while Article 2 uses ParseNet for Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Semantic Segmentation, while Article 2 uses ParseNet  for Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Semantic Segmentation, while Article 2 uses ParseNet for Semantic Segmentation."
42,42,1_paper_12,1_paper_1,24,213,1511.07122,1804.09337,Semantic Segmentation,"sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset.sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses multiple datasets:  CamVid, and  ADE20K, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Dilation10 approach for Semantic Segmentation, while Article 2 uses Smooth Network with Channel Attention Block for Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Semantic Segmentation, while Article 2 uses Smooth Network with Channel Attention Block for Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Semantic Segmentation, while Article 2 uses Smooth Network with Channel Attention Block for Semantic Segmentation."
43,43,1_paper_12,1_paper_7,24,215,1511.07122,1707.02968,Semantic Segmentation,"sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset. sent: Article 1 uses multiple datasets:  CamVid,  ADE20K, and  Cityscapes, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Dilation10 approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses Dilation10 approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation."
44,44,1_paper_9,1_paper_4,25,26,1502.0324,1504.01013,Semantic Segmentation,"sent: Article 1 and Article 2 use PASCAL Context dataset. sent: Article 1 uses multiple datasets:  PASCAL VOC 2012, and  Cityscapes, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses CRF-RNN approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation."
45,45,1_paper_9,1_paper_5,25,29,1502.0324,1506.04579,Semantic Segmentation,"sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset.sent: Article 1 and Article 2 use PASCAL Context dataset. sent: Article 1 uses Cityscapes dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses CRF-RNN approach for Semantic Segmentation, while Article 2 uses ParseNet  for Semantic Segmentation. sent: Article 1 uses CRF-RNN approach for Semantic Segmentation, while Article 2 uses ParseNet for Semantic Segmentation."
46,46,1_paper_9,1_paper_1,25,213,1502.0324,1804.09337,Semantic Segmentation,"sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset.sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses PASCAL Context dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses CRF-RNN approach for Semantic Segmentation, while Article 2 uses Smooth Network with Channel Attention Block for Semantic Segmentation."
47,47,1_paper_9,1_paper_7,25,215,1502.0324,1707.02968,Semantic Segmentation,"sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset. sent: Article 1 uses multiple datasets:  PASCAL Context, and  Cityscapes, while Article 2 uses ImageNet dataset.","sent: Article 1 uses CRF-RNN approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses CRF-RNN approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation."
48,48,1_paper_4,1_paper_0,26,27,1504.01013,1611.08323,Semantic Segmentation,"sent: Article 1 uses PASCAL Context dataset, while Article 2 uses Cityscapes dataset.","sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses FRRN for Semantic Segmentation."
49,49,1_paper_4,1_paper_10,26,28,1504.01013,1502.0324,Semantic Segmentation,"sent: Article 1 and Article 2 use PASCAL Context dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  PASCAL VOC 2012, and  Cityscapes.","sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses CRF-RNN for Semantic Segmentation."
50,50,1_paper_4,1_paper_5,26,29,1504.01013,1506.04579,Semantic Segmentation,"sent: Article 1 and Article 2 use PASCAL Context dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses PASCAL VOC 2012 dataset.","sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses ParseNet  for Semantic Segmentation. sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses ParseNet for Semantic Segmentation."
51,51,1_paper_4,1_paper_13,26,211,1504.01013,1511.07122,Semantic Segmentation,"sent: Article 1 uses PASCAL Context dataset, while Article 2 uses multiple datasets:  CamVid,  PASCAL VOC 2012,  ADE20K, and  Cityscapes.","sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses Dilation10 for Semantic Segmentation. sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses Dilated Convolutions for Semantic Segmentation. sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses DilatedNet for Semantic Segmentation."
52,52,1_paper_4,1_paper_1,26,213,1504.01013,1804.09337,Semantic Segmentation,"sent: Article 1 uses PASCAL Context dataset, while Article 2 uses multiple datasets:  PASCAL VOC 2012, and  Cityscapes.","sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses Smooth Network with Channel Attention Block for Semantic Segmentation."
53,53,1_paper_4,1_paper_7,26,215,1504.01013,1707.02968,Semantic Segmentation,"sent: Article 1 uses PASCAL Context dataset, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation."
54,54,1_paper_0,1_paper_5,27,29,1611.08323,1506.04579,Semantic Segmentation,"sent: Article 1 uses Cityscapes dataset, while Article 2 uses multiple datasets:  PASCAL VOC 2012, and  PASCAL Context.","sent: Article 1 uses FRRN approach for Semantic Segmentation, while Article 2 uses ParseNet  for Semantic Segmentation. sent: Article 1 uses FRRN approach for Semantic Segmentation, while Article 2 uses ParseNet for Semantic Segmentation."
55,55,1_paper_0,1_paper_1,27,213,1611.08323,1804.09337,Semantic Segmentation,"sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses PASCAL VOC 2012 dataset.","sent: Article 1 uses FRRN approach for Semantic Segmentation, while Article 2 uses Smooth Network with Channel Attention Block for Semantic Segmentation."
56,56,1_paper_0,1_paper_7,27,215,1611.08323,1707.02968,Semantic Segmentation,"sent: Article 1 uses Cityscapes dataset, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses FRRN approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses FRRN approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation."
57,57,3_paper_21,3_paper_14,40,41,1511.06038,1805.0222,Question Answering,"sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses MS MARCO dataset.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses VNET for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses VNET for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses VNET for Question Answering."
58,58,3_paper_21,3_paper_60,40,42,1511.06038,1606.02858,Question Answering,"sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering."
59,59,3_paper_21,3_paper_84,40,43,1511.06038,1802.05365,Question Answering,"sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  ACL-ARC,  SQuAD2 0,  SST-5 Fine-grained classification, and  SQuAD1 1.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering."
60,60,3_paper_21,3_paper_31,40,45,1511.06038,1603.01547,Question Answering,"sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering."
61,61,3_paper_21,3_paper_41,40,47,1511.06038,1707.09098,Question Answering,"sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses MEMEN for Question Answering."
62,62,3_paper_21,3_paper_71,40,48,1511.06038,1609.05284,Question Answering,"sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses multiple datasets:  CNN   Daily Mail, and  SQuAD1 1.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering."
63,63,3_paper_21,3_paper_77,40,410,1511.06038,1706.02596,Question Answering,"sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses TriviaQA dataset.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering."
64,64,3_paper_21,3_paper_48,40,414,1511.06038,1606.01549,Question Answering,"sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses GA for Question Answering."
65,65,3_paper_21,3_paper_69,40,415,1511.06038,1506.0334,Question Answering,"sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering."
66,66,3_paper_21,3_paper_62,40,416,1511.06038,1610.09027,Question Answering,"sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses bAbi dataset.","sent: Article 1 and Article 2 use LSTMapproach for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses SDNC for Question Answering."
67,67,3_paper_21,3_paper_81,40,421,1511.06038,1710.10723,Question Answering,"sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses S-Norm for Question Answering."
68,68,3_paper_21,3_paper_13,40,422,1511.06038,1503.03244,Question Answering,"sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses SemEvalCQA dataset.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses ARC-II for Question Answering."
69,69,3_paper_21,3_paper_64,40,448,1511.06038,1901.02262,Question Answering,"sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses MS MARCO dataset.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering."
70,70,3_paper_21,3_paper_79,40,450,1511.06038,1711.08028,Question Answering,"sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses bAbi dataset.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses  RR for Question Answering."
71,71,3_paper_21,3_paper_54,40,454,1511.06038,1606.03126,Question Answering,"sent: Article 1 and Article 2 use WikiQA dataset. sent: Article 1 uses QASent dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering."
72,72,3_paper_14,3_paper_60,41,42,1805.0222,1606.02858,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering."
73,73,3_paper_14,3_paper_84,41,43,1805.0222,1802.05365,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering."
74,74,3_paper_14,3_paper_31,41,45,1805.0222,1603.01547,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test.","sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering."
75,75,3_paper_14,3_paper_41,41,47,1805.0222,1707.09098,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.","sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses MEMEN for Question Answering."
76,76,3_paper_14,3_paper_71,41,48,1805.0222,1609.05284,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail, and  SQuAD1 1.","sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering."
77,77,3_paper_14,3_paper_23,41,49,1805.0222,1511.06038,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses multiple datasets:  WikiQA, and  QASent.","sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering."
78,78,3_paper_14,3_paper_77,41,410,1805.0222,1706.02596,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses TriviaQA dataset.","sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering."
79,79,3_paper_14,3_paper_48,41,414,1805.0222,1606.01549,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.","sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses GA for Question Answering."
80,80,3_paper_14,3_paper_69,41,415,1805.0222,1506.0334,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering."
81,81,3_paper_14,3_paper_62,41,416,1805.0222,1610.09027,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses bAbi dataset.","sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses SDNC for Question Answering."
82,82,3_paper_14,3_paper_81,41,421,1805.0222,1710.10723,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.","sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses S-Norm for Question Answering."
83,83,3_paper_14,3_paper_13,41,422,1805.0222,1503.03244,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses SemEvalCQA dataset.","sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses ARC-II for Question Answering."
84,84,3_paper_14,3_paper_64,41,448,1805.0222,1901.02262,Question Answering,sent: Article 1 and Article 2 use MS MARCO dataset.,"sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering."
85,85,3_paper_14,3_paper_79,41,450,1805.0222,1711.08028,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses bAbi dataset.","sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses  RR for Question Answering."
86,86,3_paper_14,3_paper_54,41,454,1805.0222,1606.03126,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses WikiQA dataset.","sent: Article 1 uses VNET approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering."
87,87,3_paper_60,3_paper_84,42,43,1606.02858,1802.05365,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering."
88,88,3_paper_60,3_paper_15,42,44,1606.02858,1805.0222,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses MS MARCO dataset.","sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses VNET for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses VNET for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses VNET for Question Answering."
89,89,3_paper_60,3_paper_31,42,45,1606.02858,1603.01547,Question Answering,"sent: Article 1 and Article 2 use CNN   Daily Mail dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SearchQA, and  Children s Book Test.","sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering."
90,90,3_paper_60,3_paper_41,42,47,1606.02858,1707.09098,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.","sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses MEMEN for Question Answering."
91,91,3_paper_60,3_paper_71,42,48,1606.02858,1609.05284,Question Answering,"sent: Article 1 and Article 2 use CNN   Daily Mail dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses SQuAD1 1 dataset.","sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering."
92,92,3_paper_60,3_paper_23,42,49,1606.02858,1511.06038,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses multiple datasets:  WikiQA, and  QASent.","sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering."
93,93,3_paper_60,3_paper_77,42,410,1606.02858,1706.02596,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses TriviaQA dataset.","sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering."
94,94,3_paper_60,3_paper_48,42,414,1606.02858,1606.01549,Question Answering,"sent: Article 1 and Article 2 use CNN   Daily Mail dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Quasar, and  Children s Book Test.","sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses GA for Question Answering."
95,95,3_paper_60,3_paper_69,42,415,1606.02858,1506.0334,Question Answering,sent: Article 1 and Article 2 use CNN   Daily Mail dataset.,"sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering."
96,96,3_paper_60,3_paper_62,42,416,1606.02858,1610.09027,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses bAbi dataset.","sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses SDNC for Question Answering."
97,97,3_paper_60,3_paper_81,42,421,1606.02858,1710.10723,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.","sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses S-Norm for Question Answering."
98,98,3_paper_60,3_paper_13,42,422,1606.02858,1503.03244,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses SemEvalCQA dataset.","sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses ARC-II for Question Answering."
99,99,3_paper_60,3_paper_64,42,448,1606.02858,1901.02262,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses MS MARCO dataset.","sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering."
100,100,3_paper_60,3_paper_79,42,450,1606.02858,1711.08028,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses bAbi dataset.","sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses  RR for Question Answering."
101,101,3_paper_60,3_paper_54,42,454,1606.02858,1606.03126,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses WikiQA dataset.","sent: Article 1 uses Classifier approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses Attentive   relabling   ensemble approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses AttentiveReader   bilinear attention approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering."
102,102,3_paper_84,3_paper_15,43,44,1802.05365,1805.0222,Question Answering,"sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 uses MS MARCO dataset.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses VNET for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses VNET for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses VNET for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses VNET for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses VNET for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses VNET for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses VNET for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses VNET for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses VNET for Question Answering."
103,103,3_paper_84,3_paper_31,43,45,1802.05365,1603.01547,Question Answering,"sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  ACL-ARC,  SQuAD2 0,  SST-5 Fine-grained classification, and  SQuAD1 1, while Article 2 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering."
104,104,3_paper_84,3_paper_56,43,46,1802.05365,1606.02858,Question Answering,"sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering."
105,105,3_paper_84,3_paper_41,43,47,1802.05365,1707.09098,Question Answering,"sent: Article 1 and Article 2 use SQuAD1 1 dataset. sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 uses TriviaQA dataset.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses MEMEN for Question Answering."
106,106,3_paper_84,3_paper_71,43,48,1802.05365,1609.05284,Question Answering,"sent: Article 1 and Article 2 use SQuAD1 1 dataset. sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering."
107,107,3_paper_84,3_paper_23,43,49,1802.05365,1511.06038,Question Answering,"sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  ACL-ARC,  SQuAD2 0,  SST-5 Fine-grained classification, and  SQuAD1 1, while Article 2 uses multiple datasets:  WikiQA, and  QASent.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering."
108,108,3_paper_84,3_paper_77,43,410,1802.05365,1706.02596,Question Answering,"sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 uses TriviaQA dataset.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering."
109,109,3_paper_84,3_paper_48,43,414,1802.05365,1606.01549,Question Answering,"sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  ACL-ARC,  SQuAD2 0,  SST-5 Fine-grained classification, and  SQuAD1 1, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses GA for Question Answering."
110,110,3_paper_84,3_paper_69,43,415,1802.05365,1506.0334,Question Answering,"sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering."
111,111,3_paper_84,3_paper_62,43,416,1802.05365,1610.09027,Question Answering,"sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 uses bAbi dataset.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses SDNC for Question Answering."
112,112,3_paper_84,3_paper_81,43,421,1802.05365,1710.10723,Question Answering,"sent: Article 1 and Article 2 use SQuAD1 1 dataset. sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 uses TriviaQA dataset.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses S-Norm for Question Answering."
113,113,3_paper_84,3_paper_13,43,422,1802.05365,1503.03244,Question Answering,"sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 uses SemEvalCQA dataset.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses ARC-II for Question Answering."
114,114,3_paper_84,3_paper_64,43,448,1802.05365,1901.02262,Question Answering,"sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 uses MS MARCO dataset.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering."
115,115,3_paper_84,3_paper_79,43,450,1802.05365,1711.08028,Question Answering,"sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 uses bAbi dataset.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses  RR for Question Answering."
116,116,3_paper_84,3_paper_54,43,454,1802.05365,1606.03126,Question Answering,"sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 uses WikiQA dataset.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses  Lee et al   2017  ELMo approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses BCN ELMo approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses ESIM   ELMo Ensemble approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses  He et al   2017    ELMo approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses ESIM   ELMo approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering."
117,117,3_paper_31,3_paper_56,45,46,1603.01547,1606.02858,Question Answering,"sent: Article 1 and Article 2 use CNN   Daily Mail dataset. sent: Article 1 uses multiple datasets:  SearchQA, and  Children s Book Test, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering."
118,118,3_paper_31,3_paper_41,45,47,1603.01547,1707.09098,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.","sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses MEMEN for Question Answering."
119,119,3_paper_31,3_paper_71,45,48,1603.01547,1609.05284,Question Answering,"sent: Article 1 and Article 2 use CNN   Daily Mail dataset. sent: Article 1 uses multiple datasets:  SearchQA, and  Children s Book Test, while Article 2 uses SQuAD1 1 dataset.","sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering."
120,120,3_paper_31,3_paper_23,45,49,1603.01547,1511.06038,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test, while Article 2 uses multiple datasets:  WikiQA, and  QASent.","sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering."
121,121,3_paper_31,3_paper_77,45,410,1603.01547,1706.02596,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test, while Article 2 uses TriviaQA dataset.","sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering."
122,122,3_paper_31,3_paper_48,45,414,1603.01547,1606.01549,Question Answering,"sent: Article 1 and Article 2 use CNN   Daily Mail dataset.sent: Article 1 and Article 2 use Children s Book Test dataset. sent: Article 1 uses SearchQA dataset, while Article 2 uses Quasar dataset.","sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses ASR approach for Open-Domain Question Answering, while Article 2 uses GA for Open-Domain Question Answering."
123,123,3_paper_31,3_paper_69,45,415,1603.01547,1506.0334,Question Answering,"sent: Article 1 and Article 2 use CNN   Daily Mail dataset. sent: Article 1 uses multiple datasets:  SearchQA, and  Children s Book Test, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering."
124,124,3_paper_31,3_paper_62,45,416,1603.01547,1610.09027,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test, while Article 2 uses bAbi dataset.","sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses SDNC for Question Answering."
125,125,3_paper_31,3_paper_88,45,417,1603.01547,1802.05365,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  ACL-ARC,  SQuAD2 0,  SST-5 Fine-grained classification, and  SQuAD1 1.","sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering."
126,126,3_paper_31,3_paper_81,45,421,1603.01547,1710.10723,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.","sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses S-Norm for Question Answering."
127,127,3_paper_31,3_paper_13,45,422,1603.01547,1503.03244,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test, while Article 2 uses SemEvalCQA dataset.","sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses ARC-II for Question Answering."
128,128,3_paper_31,3_paper_64,45,448,1603.01547,1901.02262,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test, while Article 2 uses MS MARCO dataset.","sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering."
129,129,3_paper_31,3_paper_79,45,450,1603.01547,1711.08028,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test, while Article 2 uses bAbi dataset.","sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses  RR for Question Answering."
130,130,3_paper_31,3_paper_54,45,454,1603.01547,1606.03126,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test, while Article 2 uses WikiQA dataset.","sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering."
131,131,3_paper_41,3_paper_71,47,48,1707.09098,1609.05284,Question Answering,"sent: Article 1 and Article 2 use SQuAD1 1 dataset. sent: Article 1 uses TriviaQA dataset, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering."
132,132,3_paper_41,3_paper_23,47,49,1707.09098,1511.06038,Question Answering,"sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses multiple datasets:  WikiQA, and  QASent.","sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering."
133,133,3_paper_41,3_paper_77,47,410,1707.09098,1706.02596,Question Answering,"sent: Article 1 and Article 2 use TriviaQA dataset. sent: Article 1 uses SQuAD1 1 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering."
134,134,3_paper_41,3_paper_29,47,412,1707.09098,1603.01547,Question Answering,"sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test.","sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering."
135,135,3_paper_41,3_paper_48,47,414,1707.09098,1606.01549,Question Answering,"sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.","sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses GA for Question Answering."
136,136,3_paper_41,3_paper_69,47,415,1707.09098,1506.0334,Question Answering,"sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering."
137,137,3_paper_41,3_paper_62,47,416,1707.09098,1610.09027,Question Answering,"sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses bAbi dataset.","sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses SDNC for Question Answering."
138,138,3_paper_41,3_paper_88,47,417,1707.09098,1802.05365,Question Answering,"sent: Article 1 and Article 2 use SQuAD1 1 dataset. sent: Article 1 uses TriviaQA dataset, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering."
139,139,3_paper_41,3_paper_81,47,421,1707.09098,1710.10723,Question Answering,sent: Article 1 and Article 2 use TriviaQA dataset.sent: Article 1 and Article 2 use SQuAD1 1 dataset.,"sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses S-Norm for Question Answering."
140,140,3_paper_41,3_paper_13,47,422,1707.09098,1503.03244,Question Answering,"sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses SemEvalCQA dataset.","sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses ARC-II for Question Answering."
141,141,3_paper_41,3_paper_59,47,440,1707.09098,1606.02858,Question Answering,"sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering."
142,142,3_paper_41,3_paper_64,47,448,1707.09098,1901.02262,Question Answering,"sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses MS MARCO dataset.","sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering."
143,143,3_paper_41,3_paper_79,47,450,1707.09098,1711.08028,Question Answering,"sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses bAbi dataset.","sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses  RR for Question Answering."
144,144,3_paper_41,3_paper_54,47,454,1707.09098,1606.03126,Question Answering,"sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses WikiQA dataset.","sent: Article 1 uses MEMEN  ensemble  approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses MEMEN   single model  approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses MEMEN approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering."
145,145,3_paper_71,3_paper_23,48,49,1609.05284,1511.06038,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail, and  SQuAD1 1, while Article 2 uses multiple datasets:  WikiQA, and  QASent.","sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering."
146,146,3_paper_71,3_paper_77,48,410,1609.05284,1706.02596,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail, and  SQuAD1 1, while Article 2 uses TriviaQA dataset.","sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering."
147,147,3_paper_71,3_paper_29,48,412,1609.05284,1603.01547,Question Answering,"sent: Article 1 and Article 2 use CNN   Daily Mail dataset. sent: Article 1 uses SQuAD1 1 dataset, while Article 2 uses multiple datasets:  SearchQA, and  Children s Book Test.","sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering."
148,148,3_paper_71,3_paper_48,48,414,1609.05284,1606.01549,Question Answering,"sent: Article 1 and Article 2 use CNN   Daily Mail dataset. sent: Article 1 uses SQuAD1 1 dataset, while Article 2 uses multiple datasets:  Quasar, and  Children s Book Test.","sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses GA for Question Answering."
149,149,3_paper_71,3_paper_69,48,415,1609.05284,1506.0334,Question Answering,"sent: Article 1 and Article 2 use CNN   Daily Mail dataset. sent: Article 1 uses SQuAD1 1 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering."
150,150,3_paper_71,3_paper_62,48,416,1609.05284,1610.09027,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail, and  SQuAD1 1, while Article 2 uses bAbi dataset.","sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses SDNC for Question Answering."
151,151,3_paper_71,3_paper_88,48,417,1609.05284,1802.05365,Question Answering,"sent: Article 1 and Article 2 use SQuAD1 1 dataset. sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering."
152,152,3_paper_71,3_paper_81,48,421,1609.05284,1710.10723,Question Answering,"sent: Article 1 and Article 2 use SQuAD1 1 dataset. sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses TriviaQA dataset.","sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses S-Norm for Question Answering."
153,153,3_paper_71,3_paper_13,48,422,1609.05284,1503.03244,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail, and  SQuAD1 1, while Article 2 uses SemEvalCQA dataset.","sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses ARC-II for Question Answering."
154,154,3_paper_71,3_paper_43,48,427,1609.05284,1707.09098,Question Answering,"sent: Article 1 and Article 2 use SQuAD1 1 dataset. sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses TriviaQA dataset.","sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses MEMEN for Question Answering."
155,155,3_paper_71,3_paper_59,48,440,1609.05284,1606.02858,Question Answering,"sent: Article 1 and Article 2 use CNN   Daily Mail dataset. sent: Article 1 uses SQuAD1 1 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering."
156,156,3_paper_71,3_paper_64,48,448,1609.05284,1901.02262,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail, and  SQuAD1 1, while Article 2 uses MS MARCO dataset.","sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering."
157,157,3_paper_71,3_paper_79,48,450,1609.05284,1711.08028,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail, and  SQuAD1 1, while Article 2 uses bAbi dataset.","sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses  RR for Question Answering."
158,158,3_paper_71,3_paper_54,48,454,1609.05284,1606.03126,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail, and  SQuAD1 1, while Article 2 uses WikiQA dataset.","sent: Article 1 uses ReasoNet  ensemble  approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses ReasoNet approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses ReasoNet  single model  approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering."
159,159,3_paper_77,3_paper_18,410,411,1706.02596,1511.06038,Question Answering,"sent: Article 1 uses TriviaQA dataset, while Article 2 uses multiple datasets:  WikiQA, and  QASent.","sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering."
160,160,3_paper_77,3_paper_29,410,412,1706.02596,1603.01547,Question Answering,"sent: Article 1 uses TriviaQA dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test.","sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering."
161,161,3_paper_77,3_paper_73,410,413,1706.02596,1609.05284,Question Answering,"sent: Article 1 uses TriviaQA dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail, and  SQuAD1 1.","sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering."
162,162,3_paper_77,3_paper_48,410,414,1706.02596,1606.01549,Question Answering,"sent: Article 1 uses TriviaQA dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.","sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses GA for Question Answering."
163,163,3_paper_77,3_paper_69,410,415,1706.02596,1506.0334,Question Answering,"sent: Article 1 uses TriviaQA dataset, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering."
164,164,3_paper_77,3_paper_62,410,416,1706.02596,1610.09027,Question Answering,"sent: Article 1 uses TriviaQA dataset, while Article 2 uses bAbi dataset.","sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses SDNC for Question Answering."
165,165,3_paper_77,3_paper_88,410,417,1706.02596,1802.05365,Question Answering,"sent: Article 1 uses TriviaQA dataset, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering."
166,166,3_paper_77,3_paper_81,410,421,1706.02596,1710.10723,Question Answering,"sent: Article 1 and Article 2 use TriviaQA dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses SQuAD1 1 dataset.","sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses S-Norm for Question Answering."
167,167,3_paper_77,3_paper_13,410,422,1706.02596,1503.03244,Question Answering,"sent: Article 1 uses TriviaQA dataset, while Article 2 uses SemEvalCQA dataset.","sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses ARC-II for Question Answering."
168,168,3_paper_77,3_paper_43,410,427,1706.02596,1707.09098,Question Answering,"sent: Article 1 and Article 2 use TriviaQA dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses SQuAD1 1 dataset.","sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses MEMEN for Question Answering."
169,169,3_paper_77,3_paper_59,410,440,1706.02596,1606.02858,Question Answering,"sent: Article 1 uses TriviaQA dataset, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering."
170,170,3_paper_77,3_paper_64,410,448,1706.02596,1901.02262,Question Answering,"sent: Article 1 uses TriviaQA dataset, while Article 2 uses MS MARCO dataset.","sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering."
171,171,3_paper_77,3_paper_79,410,450,1706.02596,1711.08028,Question Answering,"sent: Article 1 uses TriviaQA dataset, while Article 2 uses bAbi dataset.","sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses  RR for Question Answering."
172,172,3_paper_77,3_paper_54,410,454,1706.02596,1606.03126,Question Answering,"sent: Article 1 uses TriviaQA dataset, while Article 2 uses WikiQA dataset.","sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering."
173,173,3_paper_48,3_paper_69,414,415,1606.01549,1506.0334,Question Answering,"sent: Article 1 and Article 2 use CNN   Daily Mail dataset. sent: Article 1 uses multiple datasets:  Quasar, and  Children s Book Test, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering."
174,174,3_paper_48,3_paper_62,414,416,1606.01549,1610.09027,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses bAbi dataset.","sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses SDNC for Question Answering."
175,175,3_paper_48,3_paper_88,414,417,1606.01549,1802.05365,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  ACL-ARC,  SQuAD2 0,  SST-5 Fine-grained classification, and  SQuAD1 1.","sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering."
176,176,3_paper_48,3_paper_27,414,418,1606.01549,1511.06038,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses multiple datasets:  WikiQA, and  QASent.","sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering."
177,177,3_paper_48,3_paper_81,414,421,1606.01549,1710.10723,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.","sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses S-Norm for Question Answering."
178,178,3_paper_48,3_paper_13,414,422,1606.01549,1503.03244,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses SemEvalCQA dataset.","sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses ARC-II for Question Answering."
179,179,3_paper_48,3_paper_43,414,427,1606.01549,1707.09098,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.","sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses MEMEN for Question Answering."
180,180,3_paper_48,3_paper_32,414,428,1606.01549,1603.01547,Question Answering,"sent: Article 1 and Article 2 use CNN   Daily Mail dataset.sent: Article 1 and Article 2 use Children s Book Test dataset. sent: Article 1 uses Quasar dataset, while Article 2 uses SearchQA dataset.","sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses GA approach for Open-Domain Question Answering, while Article 2 uses ASR for Open-Domain Question Answering."
181,181,3_paper_48,3_paper_78,414,439,1606.01549,1706.02596,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses TriviaQA dataset.","sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering."
182,182,3_paper_48,3_paper_59,414,440,1606.01549,1606.02858,Question Answering,"sent: Article 1 and Article 2 use CNN   Daily Mail dataset. sent: Article 1 uses multiple datasets:  Quasar, and  Children s Book Test, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering."
183,183,3_paper_48,3_paper_76,414,442,1606.01549,1609.05284,Question Answering,"sent: Article 1 and Article 2 use CNN   Daily Mail dataset. sent: Article 1 uses multiple datasets:  Quasar, and  Children s Book Test, while Article 2 uses SQuAD1 1 dataset.","sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering."
184,184,3_paper_48,3_paper_64,414,448,1606.01549,1901.02262,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses MS MARCO dataset.","sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering."
185,185,3_paper_48,3_paper_79,414,450,1606.01549,1711.08028,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses bAbi dataset.","sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses  RR for Question Answering."
186,186,3_paper_48,3_paper_54,414,454,1606.01549,1606.03126,Question Answering,"sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses WikiQA dataset.","sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering."
187,187,3_paper_69,3_paper_62,415,416,1506.0334,1610.09027,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses bAbi dataset.","sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses SDNC for Question Answering."
188,188,3_paper_69,3_paper_88,415,417,1506.0334,1802.05365,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering."
189,189,3_paper_69,3_paper_27,415,418,1506.0334,1511.06038,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses multiple datasets:  WikiQA, and  QASent.","sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering."
190,190,3_paper_69,3_paper_49,415,419,1506.0334,1606.01549,Question Answering,"sent: Article 1 and Article 2 use CNN   Daily Mail dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Quasar, and  Children s Book Test.","sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses GA for Question Answering."
191,191,3_paper_69,3_paper_81,415,421,1506.0334,1710.10723,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.","sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses S-Norm for Question Answering."
192,192,3_paper_69,3_paper_13,415,422,1506.0334,1503.03244,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses SemEvalCQA dataset.","sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses ARC-II for Question Answering."
193,193,3_paper_69,3_paper_43,415,427,1506.0334,1707.09098,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.","sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses MEMEN for Question Answering."
194,194,3_paper_69,3_paper_32,415,428,1506.0334,1603.01547,Question Answering,"sent: Article 1 and Article 2 use CNN   Daily Mail dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SearchQA, and  Children s Book Test.","sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering."
195,195,3_paper_69,3_paper_78,415,439,1506.0334,1706.02596,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses TriviaQA dataset.","sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering."
196,196,3_paper_69,3_paper_59,415,440,1506.0334,1606.02858,Question Answering,sent: Article 1 and Article 2 use CNN   Daily Mail dataset.,"sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering."
197,197,3_paper_69,3_paper_76,415,442,1506.0334,1609.05284,Question Answering,"sent: Article 1 and Article 2 use CNN   Daily Mail dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses SQuAD1 1 dataset.","sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering."
198,198,3_paper_69,3_paper_64,415,448,1506.0334,1901.02262,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses MS MARCO dataset.","sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering."
199,199,3_paper_69,3_paper_79,415,450,1506.0334,1711.08028,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses bAbi dataset.","sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses  RR for Question Answering."
200,200,3_paper_69,3_paper_54,415,454,1506.0334,1606.03126,Question Answering,"sent: Article 1 uses CNN   Daily Mail dataset, while Article 2 uses WikiQA dataset.","sent: Article 1 uses Attentive Reader approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses Impatient Reader approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses MemNNs  ensemble  approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering."
201,201,3_paper_62,3_paper_88,416,417,1610.09027,1802.05365,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering."
202,202,3_paper_62,3_paper_27,416,418,1610.09027,1511.06038,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses multiple datasets:  WikiQA, and  QASent.","sent: Article 1 and Article 2 use LSTMapproach for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering."
203,203,3_paper_62,3_paper_49,416,419,1610.09027,1606.01549,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses GA for Question Answering."
204,204,3_paper_62,3_paper_68,416,420,1610.09027,1506.0334,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering."
205,205,3_paper_62,3_paper_81,416,421,1610.09027,1710.10723,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses S-Norm for Question Answering."
206,206,3_paper_62,3_paper_13,416,422,1610.09027,1503.03244,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses SemEvalCQA dataset.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses ARC-II for Question Answering."
207,207,3_paper_62,3_paper_43,416,427,1610.09027,1707.09098,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses MEMEN for Question Answering."
208,208,3_paper_62,3_paper_32,416,428,1610.09027,1603.01547,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering."
209,209,3_paper_62,3_paper_78,416,439,1610.09027,1706.02596,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses TriviaQA dataset.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering."
210,210,3_paper_62,3_paper_59,416,440,1610.09027,1606.02858,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering."
211,211,3_paper_62,3_paper_76,416,442,1610.09027,1609.05284,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail, and  SQuAD1 1.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering."
212,212,3_paper_62,3_paper_64,416,448,1610.09027,1901.02262,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses MS MARCO dataset.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering."
213,213,3_paper_62,3_paper_79,416,450,1610.09027,1711.08028,Question Answering,sent: Article 1 and Article 2 use bAbi dataset.,"sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses  RR for Question Answering."
214,214,3_paper_62,3_paper_54,416,454,1610.09027,1606.03126,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses WikiQA dataset.","sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering."
215,215,3_paper_81,3_paper_13,421,422,1710.10723,1503.03244,Question Answering,"sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses SemEvalCQA dataset.","sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses ARC-II for Question Answering."
216,216,3_paper_81,3_paper_63,421,425,1710.10723,1610.09027,Question Answering,"sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses bAbi dataset.","sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses SDNC for Question Answering."
217,217,3_paper_81,3_paper_52,421,426,1710.10723,1606.01549,Question Answering,"sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.","sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses GA for Question Answering."
218,218,3_paper_81,3_paper_43,421,427,1710.10723,1707.09098,Question Answering,sent: Article 1 and Article 2 use TriviaQA dataset.sent: Article 1 and Article 2 use SQuAD1 1 dataset.,"sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses MEMEN for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses MEMEN for Question Answering."
219,219,3_paper_81,3_paper_32,421,428,1710.10723,1603.01547,Question Answering,"sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test.","sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering."
220,220,3_paper_81,3_paper_24,421,431,1710.10723,1511.06038,Question Answering,"sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses multiple datasets:  WikiQA, and  QASent.","sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering."
221,221,3_paper_81,3_paper_87,421,434,1710.10723,1802.05365,Question Answering,"sent: Article 1 and Article 2 use SQuAD1 1 dataset. sent: Article 1 uses TriviaQA dataset, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering."
222,222,3_paper_81,3_paper_66,421,436,1710.10723,1506.0334,Question Answering,"sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering."
223,223,3_paper_81,3_paper_78,421,439,1710.10723,1706.02596,Question Answering,"sent: Article 1 and Article 2 use TriviaQA dataset. sent: Article 1 uses SQuAD1 1 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering."
224,224,3_paper_81,3_paper_59,421,440,1710.10723,1606.02858,Question Answering,"sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering."
225,225,3_paper_81,3_paper_76,421,442,1710.10723,1609.05284,Question Answering,"sent: Article 1 and Article 2 use SQuAD1 1 dataset. sent: Article 1 uses TriviaQA dataset, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering."
226,226,3_paper_81,3_paper_64,421,448,1710.10723,1901.02262,Question Answering,"sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses MS MARCO dataset.","sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering."
227,227,3_paper_81,3_paper_79,421,450,1710.10723,1711.08028,Question Answering,"sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses bAbi dataset.","sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses  RR for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses  RR for Question Answering."
228,228,3_paper_81,3_paper_54,421,454,1710.10723,1606.03126,Question Answering,"sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses WikiQA dataset.","sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering."
229,229,3_paper_13,3_paper_83,422,423,1503.03244,1710.10723,Question Answering,"sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.","sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses S-Norm for Question Answering."
230,230,3_paper_13,3_paper_63,422,425,1503.03244,1610.09027,Question Answering,"sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses bAbi dataset.","sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses SDNC for Question Answering."
231,231,3_paper_13,3_paper_52,422,426,1503.03244,1606.01549,Question Answering,"sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.","sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses GA for Question Answering."
232,232,3_paper_13,3_paper_43,422,427,1503.03244,1707.09098,Question Answering,"sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.","sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses MEMEN  ensemble  for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses MEMEN   single model  for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses MEMEN for Question Answering."
233,233,3_paper_13,3_paper_32,422,428,1503.03244,1603.01547,Question Answering,"sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test.","sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering."
234,234,3_paper_13,3_paper_24,422,431,1503.03244,1511.06038,Question Answering,"sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses multiple datasets:  WikiQA, and  QASent.","sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering."
235,235,3_paper_13,3_paper_87,422,434,1503.03244,1802.05365,Question Answering,"sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering."
236,236,3_paper_13,3_paper_66,422,436,1503.03244,1506.0334,Question Answering,"sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering."
237,237,3_paper_13,3_paper_78,422,439,1503.03244,1706.02596,Question Answering,"sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses TriviaQA dataset.","sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering."
238,238,3_paper_13,3_paper_59,422,440,1503.03244,1606.02858,Question Answering,"sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering."
239,239,3_paper_13,3_paper_76,422,442,1503.03244,1609.05284,Question Answering,"sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail, and  SQuAD1 1.","sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering."
240,240,3_paper_13,3_paper_64,422,448,1503.03244,1901.02262,Question Answering,"sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses MS MARCO dataset.","sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering."
241,241,3_paper_13,3_paper_79,422,450,1503.03244,1711.08028,Question Answering,"sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses bAbi dataset.","sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses  RR for Question Answering."
242,242,3_paper_13,3_paper_54,422,454,1503.03244,1606.03126,Question Answering,"sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses WikiQA dataset.","sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering."
243,243,3_paper_64,3_paper_70,448,449,1901.02262,1506.0334,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering."
244,244,3_paper_64,3_paper_79,448,450,1901.02262,1711.08028,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses bAbi dataset.","sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses  RR for Question Answering."
245,245,3_paper_64,3_paper_89,448,451,1901.02262,1802.05365,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering."
246,246,3_paper_64,3_paper_72,448,452,1901.02262,1609.05284,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail, and  SQuAD1 1.","sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering."
247,247,3_paper_64,3_paper_58,448,453,1901.02262,1606.02858,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering."
248,248,3_paper_64,3_paper_54,448,454,1901.02262,1606.03126,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses WikiQA dataset.","sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering."
249,249,3_paper_64,3_paper_34,448,455,1901.02262,1603.01547,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test.","sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering."
250,250,3_paper_64,3_paper_26,448,456,1901.02262,1511.06038,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses multiple datasets:  WikiQA, and  QASent.","sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering."
251,251,3_paper_64,3_paper_51,448,458,1901.02262,1606.01549,Question Answering,"sent: Article 1 uses MS MARCO dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.","sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses Masque Q A Style approach for Question Answering, while Article 2 uses GA for Question Answering."
252,252,3_paper_79,3_paper_89,450,451,1711.08028,1802.05365,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering."
253,253,3_paper_79,3_paper_72,450,452,1711.08028,1609.05284,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail, and  SQuAD1 1.","sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering."
254,254,3_paper_79,3_paper_58,450,453,1711.08028,1606.02858,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering."
255,255,3_paper_79,3_paper_54,450,454,1711.08028,1606.03126,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses WikiQA dataset.","sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses Key-Value Memory Network for Question Answering."
256,256,3_paper_79,3_paper_34,450,455,1711.08028,1603.01547,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test.","sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering."
257,257,3_paper_79,3_paper_26,450,456,1711.08028,1511.06038,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses multiple datasets:  WikiQA, and  QASent.","sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering."
258,258,3_paper_79,3_paper_51,450,458,1711.08028,1606.01549,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.","sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses GA for Question Answering."
259,259,3_paper_79,3_paper_65,450,467,1711.08028,1901.02262,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses MS MARCO dataset.","sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering."
260,260,3_paper_79,3_paper_67,450,473,1711.08028,1506.0334,Question Answering,"sent: Article 1 uses bAbi dataset, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses  RR approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering."
261,261,3_paper_54,3_paper_34,454,455,1606.03126,1603.01547,Question Answering,"sent: Article 1 uses WikiQA dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test.","sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering."
262,262,3_paper_54,3_paper_26,454,456,1606.03126,1511.06038,Question Answering,"sent: Article 1 and Article 2 use WikiQA dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses QASent dataset.","sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering."
263,263,3_paper_54,3_paper_51,454,458,1606.03126,1606.01549,Question Answering,"sent: Article 1 uses WikiQA dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.","sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses GA for Question Answering."
264,264,3_paper_54,3_paper_57,454,460,1606.03126,1606.02858,Question Answering,"sent: Article 1 uses WikiQA dataset, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses Classifier for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses Attentive   relabling   ensemble for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses AttentiveReader   bilinear attention for Question Answering."
265,265,3_paper_54,3_paper_85,454,463,1606.03126,1802.05365,Question Answering,"sent: Article 1 uses WikiQA dataset, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses BiLSTM-CRF ELMo for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses  Lee et al   2017  ELMo for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses BCN ELMo for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses ESIM   ELMo Ensemble for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses  He et al   2017    ELMo for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses ESIM   ELMo for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses BiLSTM-Attention   ELMo for Question Answering."
266,266,3_paper_54,3_paper_65,454,467,1606.03126,1901.02262,Question Answering,"sent: Article 1 uses WikiQA dataset, while Article 2 uses MS MARCO dataset.","sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses Masque Q A Style for Question Answering."
267,267,3_paper_54,3_paper_75,454,469,1606.03126,1609.05284,Question Answering,"sent: Article 1 uses WikiQA dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail, and  SQuAD1 1.","sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses ReasoNet  ensemble  for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses ReasoNet for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses ReasoNet  single model  for Question Answering."
268,268,3_paper_54,3_paper_67,454,473,1606.03126,1506.0334,Question Answering,"sent: Article 1 uses WikiQA dataset, while Article 2 uses CNN   Daily Mail dataset.","sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses Attentive Reader for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses Impatient Reader for Question Answering. sent: Article 1 uses Key-Value Memory Network approach for Question Answering, while Article 2 uses MemNNs  ensemble  for Question Answering."
269,269,5_paper_37,5_paper_57,60,62,1901.11504,1812.01216,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses CBS-1   ESIM for Natural Language Inference."
270,270,5_paper_37,5_paper_7,60,63,1901.11504,1707.02786,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference."
271,271,5_paper_37,5_paper_46,60,64,1901.11504,1802.05577,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference."
272,272,5_paper_37,5_paper_23,60,65,1901.11504,1603.06021,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference."
273,273,5_paper_37,5_paper_42,60,67,1901.11504,1711.04289,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference."
274,274,5_paper_37,5_paper_12,60,612,1901.11504,1508.05326,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference."
275,275,5_paper_37,5_paper_52,60,618,1901.11504,1802.05365,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  CoNLL 2003  English ,  ACL-ARC,  SQuAD2 0,  SST-5 Fine-grained classification, and  SQuAD1 1.","sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses BiLSTM-CRF ELMo for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses  Lee et al   2017  ELMo for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses BCN ELMo for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses ESIM   ELMo Ensemble for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses  He et al   2017    ELMo for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses ESIM   ELMo for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses BiLSTM-Attention   ELMo for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses BCN ELMo for Sentiment Analysis."
276,276,5_paper_37,5_paper_0,60,619,1901.11504,1605.05573,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference."
277,277,5_paper_37,5_paper_18,60,629,1901.11504,1509.06664,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference."
278,278,5_paper_37,5_paper_32,60,634,1901.11504,1708.02312,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference."
279,279,5_paper_57,5_paper_7,62,63,1812.01216,1707.02786,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference."
280,280,5_paper_57,5_paper_46,62,64,1812.01216,1802.05577,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference."
281,281,5_paper_57,5_paper_23,62,65,1812.01216,1603.06021,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference."
282,282,5_paper_57,5_paper_38,62,66,1812.01216,1901.11504,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.","sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference."
283,283,5_paper_57,5_paper_42,62,67,1812.01216,1711.04289,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference."
284,284,5_paper_57,5_paper_12,62,612,1812.01216,1508.05326,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference."
285,285,5_paper_57,5_paper_52,62,618,1812.01216,1802.05365,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses BiLSTM-CRF ELMo for Natural Language Inference. sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Natural Language Inference. sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses  Lee et al   2017  ELMo for Natural Language Inference. sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Natural Language Inference. sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses BCN ELMo for Natural Language Inference. sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses ESIM   ELMo Ensemble for Natural Language Inference. sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses  He et al   2017    ELMo for Natural Language Inference. sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses ESIM   ELMo for Natural Language Inference. sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses BiLSTM-Attention   ELMo for Natural Language Inference."
286,286,5_paper_57,5_paper_0,62,619,1812.01216,1605.05573,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference."
287,287,5_paper_57,5_paper_18,62,629,1812.01216,1509.06664,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference."
288,288,5_paper_57,5_paper_32,62,634,1812.01216,1708.02312,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses CBS-1   ESIM approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference."
289,289,5_paper_7,5_paper_46,63,64,1707.02786,1802.05577,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference."
290,290,5_paper_7,5_paper_23,63,65,1707.02786,1603.06021,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference."
291,291,5_paper_7,5_paper_38,63,66,1707.02786,1901.11504,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.","sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference."
292,292,5_paper_7,5_paper_42,63,67,1707.02786,1711.04289,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference."
293,293,5_paper_7,5_paper_12,63,612,1707.02786,1508.05326,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference."
294,294,5_paper_7,5_paper_52,63,618,1707.02786,1802.05365,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses BiLSTM-CRF ELMo for Natural Language Inference. sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Natural Language Inference. sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Natural Language Inference. sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses  Lee et al   2017  ELMo for Natural Language Inference. sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses BCN ELMo for Natural Language Inference. sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses ESIM   ELMo Ensemble for Natural Language Inference. sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses  He et al   2017    ELMo for Natural Language Inference. sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses ESIM   ELMo for Natural Language Inference. sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses BiLSTM-Attention   ELMo for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses BiLSTM-CRF ELMo for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses  Lee et al   2017  ELMo for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses BCN ELMo for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses ESIM   ELMo Ensemble for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses  He et al   2017    ELMo for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses ESIM   ELMo for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses BiLSTM-Attention   ELMo for Natural Language Inference."
295,295,5_paper_7,5_paper_0,63,619,1707.02786,1605.05573,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference."
296,296,5_paper_7,5_paper_18,63,629,1707.02786,1509.06664,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference."
297,297,5_paper_7,5_paper_32,63,634,1707.02786,1708.02312,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference."
298,298,5_paper_46,5_paper_23,64,65,1802.05577,1603.06021,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference."
299,299,5_paper_46,5_paper_38,64,66,1802.05577,1901.11504,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.","sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference."
300,300,5_paper_46,5_paper_42,64,67,1802.05577,1711.04289,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference."
301,301,5_paper_46,5_paper_12,64,612,1802.05577,1508.05326,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference."
302,302,5_paper_46,5_paper_6,64,615,1802.05577,1707.02786,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference."
303,303,5_paper_46,5_paper_52,64,618,1802.05577,1802.05365,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses BiLSTM-CRF ELMo for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses  Lee et al   2017  ELMo for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses BCN ELMo for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses ESIM   ELMo Ensemble for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses  He et al   2017    ELMo for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses ESIM   ELMo for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses BiLSTM-Attention   ELMo for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses BiLSTM-CRF ELMo for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses  Lee et al   2017  ELMo for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses BCN ELMo for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses ESIM   ELMo Ensemble for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses  He et al   2017    ELMo for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses ESIM   ELMo for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses BiLSTM-Attention   ELMo for Natural Language Inference."
304,304,5_paper_46,5_paper_0,64,619,1802.05577,1605.05573,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference."
305,305,5_paper_46,5_paper_18,64,629,1802.05577,1509.06664,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference."
306,306,5_paper_46,5_paper_32,64,634,1802.05577,1708.02312,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference."
307,307,5_paper_23,5_paper_38,65,66,1603.06021,1901.11504,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.","sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference."
308,308,5_paper_23,5_paper_42,65,67,1603.06021,1711.04289,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference."
309,309,5_paper_23,5_paper_12,65,612,1603.06021,1508.05326,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference."
310,310,5_paper_23,5_paper_6,65,615,1603.06021,1707.02786,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference."
311,311,5_paper_23,5_paper_52,65,618,1603.06021,1802.05365,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses BiLSTM-CRF ELMo for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses  Lee et al   2017  ELMo for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses BCN ELMo for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses ESIM   ELMo Ensemble for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses  He et al   2017    ELMo for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses ESIM   ELMo for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses BiLSTM-Attention   ELMo for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses BiLSTM-CRF ELMo for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses  Lee et al   2017  ELMo for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses BCN ELMo for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses ESIM   ELMo Ensemble for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses  He et al   2017    ELMo for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses ESIM   ELMo for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses BiLSTM-Attention   ELMo for Natural Language Inference."
312,312,5_paper_23,5_paper_0,65,619,1603.06021,1605.05573,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference."
313,313,5_paper_23,5_paper_47,65,623,1603.06021,1802.05577,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference."
314,314,5_paper_23,5_paper_18,65,629,1603.06021,1509.06664,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference."
315,315,5_paper_23,5_paper_32,65,634,1603.06021,1708.02312,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference."
316,316,5_paper_42,5_paper_21,67,68,1711.04289,1603.06021,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference."
317,317,5_paper_42,5_paper_36,67,610,1711.04289,1901.11504,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.","sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference."
318,318,5_paper_42,5_paper_12,67,612,1711.04289,1508.05326,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference."
319,319,5_paper_42,5_paper_6,67,615,1711.04289,1707.02786,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference."
320,320,5_paper_42,5_paper_52,67,618,1711.04289,1802.05365,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses BiLSTM-CRF ELMo for Natural Language Inference. sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Natural Language Inference. sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Natural Language Inference. sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses  Lee et al   2017  ELMo for Natural Language Inference. sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses BCN ELMo for Natural Language Inference. sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses ESIM   ELMo Ensemble for Natural Language Inference. sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses  He et al   2017    ELMo for Natural Language Inference. sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses ESIM   ELMo for Natural Language Inference. sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses BiLSTM-Attention   ELMo for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses BiLSTM-CRF ELMo for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses  Lee et al   2017  ELMo for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses BCN ELMo for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses ESIM   ELMo Ensemble for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses  He et al   2017    ELMo for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses ESIM   ELMo for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses BiLSTM-Attention   ELMo for Natural Language Inference."
321,321,5_paper_42,5_paper_0,67,619,1711.04289,1605.05573,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference."
322,322,5_paper_42,5_paper_47,67,623,1711.04289,1802.05577,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference."
323,323,5_paper_42,5_paper_18,67,629,1711.04289,1509.06664,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference."
324,324,5_paper_42,5_paper_32,67,634,1711.04289,1708.02312,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses KIM Ensemble approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses KIM approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference."
325,325,5_paper_12,5_paper_6,612,615,1508.05326,1707.02786,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference."
326,326,5_paper_12,5_paper_22,612,616,1508.05326,1603.06021,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference."
327,327,5_paper_12,5_paper_52,612,618,1508.05326,1802.05365,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses BiLSTM-CRF ELMo for Natural Language Inference. sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Natural Language Inference. sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Natural Language Inference. sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses  Lee et al   2017  ELMo for Natural Language Inference. sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses BCN ELMo for Natural Language Inference. sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses ESIM   ELMo Ensemble for Natural Language Inference. sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses  He et al   2017    ELMo for Natural Language Inference. sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses ESIM   ELMo for Natural Language Inference. sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses BiLSTM-Attention   ELMo for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses BiLSTM-CRF ELMo for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses  Lee et al   2017  ELMo for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses BCN ELMo for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses ESIM   ELMo Ensemble for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses  He et al   2017    ELMo for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses ESIM   ELMo for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses BiLSTM-Attention   ELMo for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses BiLSTM-CRF ELMo for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses  Lee et al   2017  ELMo for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses BCN ELMo for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses ESIM   ELMo Ensemble for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses  He et al   2017    ELMo for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses ESIM   ELMo for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses BiLSTM-Attention   ELMo for Natural Language Inference."
328,328,5_paper_12,5_paper_0,612,619,1508.05326,1605.05573,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference."
329,329,5_paper_12,5_paper_47,612,623,1508.05326,1802.05577,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference."
330,330,5_paper_12,5_paper_18,612,629,1508.05326,1509.06664,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference."
331,331,5_paper_12,5_paper_32,612,634,1508.05326,1708.02312,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference."
332,332,5_paper_12,5_paper_41,612,636,1508.05326,1711.04289,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference."
333,333,5_paper_12,5_paper_35,612,641,1508.05326,1901.11504,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.","sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference."
334,334,5_paper_52,5_paper_0,618,619,1802.05365,1605.05573,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference. sent: Article 1 uses  Lee et al   2017  ELMo approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference. sent: Article 1 uses BCN ELMo approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference. sent: Article 1 uses ESIM   ELMo Ensemble approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference. sent: Article 1 uses  He et al   2017    ELMo approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference. sent: Article 1 uses ESIM   ELMo approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference."
335,335,5_paper_52,5_paper_11,618,620,1802.05365,1508.05326,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses BiLSTM-CRF ELMo approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses BiLSTM-CRF ELMo approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference. sent: Article 1 uses  Lee et al   2017  ELMo approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses  Lee et al   2017  ELMo approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses  Lee et al   2017  ELMo approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference. sent: Article 1 uses BCN ELMo approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses BCN ELMo approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses BCN ELMo approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference. sent: Article 1 uses ESIM   ELMo Ensemble approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses ESIM   ELMo Ensemble approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses ESIM   ELMo Ensemble approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference. sent: Article 1 uses  He et al   2017    ELMo approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses  He et al   2017    ELMo approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses  He et al   2017    ELMo approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference. sent: Article 1 uses ESIM   ELMo approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses ESIM   ELMo approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses ESIM   ELMo approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference."
336,336,5_paper_52,5_paper_47,618,623,1802.05365,1802.05577,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses BiLSTM-CRF ELMo approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference. sent: Article 1 uses  Lee et al   2017  ELMo approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses  Lee et al   2017  ELMo approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference. sent: Article 1 uses BCN ELMo approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses BCN ELMo approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference. sent: Article 1 uses ESIM   ELMo Ensemble approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses ESIM   ELMo Ensemble approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference. sent: Article 1 uses  He et al   2017    ELMo approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses  He et al   2017    ELMo approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference. sent: Article 1 uses ESIM   ELMo approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses ESIM   ELMo approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference."
337,337,5_paper_52,5_paper_4,618,626,1802.05365,1707.02786,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses BiLSTM-CRF ELMo approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses  Lee et al   2017  ELMo approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses  Lee et al   2017  ELMo approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses BCN ELMo approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses BCN ELMo approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses ESIM   ELMo Ensemble approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses ESIM   ELMo Ensemble approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses  He et al   2017    ELMo approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses  He et al   2017    ELMo approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses ESIM   ELMo approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses ESIM   ELMo approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference."
338,338,5_paper_52,5_paper_18,618,629,1802.05365,1509.06664,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses  Lee et al   2017  ELMo approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses BCN ELMo approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses ESIM   ELMo Ensemble approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses  He et al   2017    ELMo approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses ESIM   ELMo approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference."
339,339,5_paper_52,5_paper_32,618,634,1802.05365,1708.02312,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses BiLSTM-CRF ELMo approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses  Lee et al   2017  ELMo approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses  Lee et al   2017  ELMo approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses BCN ELMo approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses BCN ELMo approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses ESIM   ELMo Ensemble approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses ESIM   ELMo Ensemble approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses  He et al   2017    ELMo approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses  He et al   2017    ELMo approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses ESIM   ELMo approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses ESIM   ELMo approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference."
340,340,5_paper_52,5_paper_41,618,636,1802.05365,1711.04289,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses BiLSTM-CRF ELMo approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference. sent: Article 1 uses  Lee et al   2017  ELMo approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses  Lee et al   2017  ELMo approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference. sent: Article 1 uses BCN ELMo approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses BCN ELMo approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference. sent: Article 1 uses ESIM   ELMo Ensemble approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses ESIM   ELMo Ensemble approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference. sent: Article 1 uses  He et al   2017    ELMo approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses  He et al   2017    ELMo approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference. sent: Article 1 uses ESIM   ELMo approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses ESIM   ELMo approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference."
341,341,5_paper_52,5_paper_25,618,637,1802.05365,1603.06021,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses BiLSTM-CRF ELMo approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses  Lee et al   2017  ELMo approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses  Lee et al   2017  ELMo approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses BCN ELMo approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses BCN ELMo approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses ESIM   ELMo Ensemble approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses ESIM   ELMo Ensemble approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses  He et al   2017    ELMo approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses  He et al   2017    ELMo approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses ESIM   ELMo approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses ESIM   ELMo approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference."
342,342,5_paper_52,5_paper_35,618,641,1802.05365,1901.11504,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  CoNLL 2003  English ,  ACL-ARC,  SQuAD2 0,  SST-5 Fine-grained classification, and  SQuAD1 1, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses  Lee et al   2017  ELMo approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses BCN ELMo approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses ESIM   ELMo Ensemble approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses  He et al   2017    ELMo approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses ESIM   ELMo approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses BCN ELMo approach for Sentiment Analysis, while Article 2 uses MT-DNN for Sentiment Analysis."
343,343,5_paper_0,5_paper_11,619,620,1605.05573,1508.05326,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference."
344,344,5_paper_0,5_paper_56,619,621,1605.05573,1802.05365,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses BiLSTM-CRF ELMo for Natural Language Inference. sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Natural Language Inference. sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses  Lee et al   2017  ELMo for Natural Language Inference. sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Natural Language Inference. sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses BCN ELMo for Natural Language Inference. sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses ESIM   ELMo Ensemble for Natural Language Inference. sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses  He et al   2017    ELMo for Natural Language Inference. sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses ESIM   ELMo for Natural Language Inference. sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses BiLSTM-Attention   ELMo for Natural Language Inference."
345,345,5_paper_0,5_paper_47,619,623,1605.05573,1802.05577,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference."
346,346,5_paper_0,5_paper_4,619,626,1605.05573,1707.02786,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference."
347,347,5_paper_0,5_paper_18,619,629,1605.05573,1509.06664,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference."
348,348,5_paper_0,5_paper_32,619,634,1605.05573,1708.02312,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference."
349,349,5_paper_0,5_paper_41,619,636,1605.05573,1711.04289,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference."
350,350,5_paper_0,5_paper_25,619,637,1605.05573,1603.06021,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference."
351,351,5_paper_0,5_paper_35,619,641,1605.05573,1901.11504,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.","sent: Article 1 uses 50D stacked TC-LSTMs approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference."
352,352,5_paper_18,5_paper_2,629,630,1509.06664,1605.05573,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference."
353,353,5_paper_18,5_paper_45,629,631,1509.06664,1802.05577,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference."
354,354,5_paper_18,5_paper_3,629,632,1509.06664,1707.02786,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference."
355,355,5_paper_18,5_paper_32,629,634,1509.06664,1708.02312,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 600D Residual stacked encoders for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 300D Residual stacked encoders for Natural Language Inference."
356,356,5_paper_18,5_paper_53,629,635,1509.06664,1802.05365,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses BiLSTM-CRF ELMo for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses  Lee et al   2017  ELMo for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses BCN ELMo for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses ESIM   ELMo Ensemble for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses  He et al   2017    ELMo for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses ESIM   ELMo for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses BiLSTM-Attention   ELMo for Natural Language Inference."
357,357,5_paper_18,5_paper_41,629,636,1509.06664,1711.04289,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference."
358,358,5_paper_18,5_paper_25,629,637,1509.06664,1603.06021,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference."
359,359,5_paper_18,5_paper_13,629,638,1509.06664,1508.05326,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference."
360,360,5_paper_18,5_paper_35,629,641,1509.06664,1901.11504,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.","sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference."
361,361,5_paper_32,5_paper_53,634,635,1708.02312,1802.05365,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses BiLSTM-CRF ELMo for Natural Language Inference. sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Natural Language Inference. sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Natural Language Inference. sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses  Lee et al   2017  ELMo for Natural Language Inference. sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses BCN ELMo for Natural Language Inference. sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses ESIM   ELMo Ensemble for Natural Language Inference. sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses  He et al   2017    ELMo for Natural Language Inference. sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses ESIM   ELMo for Natural Language Inference. sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses BiLSTM-Attention   ELMo for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses BiLSTM-CRF ELMo for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses  Lee et al   2017  ELMo for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses BCN ELMo for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses ESIM   ELMo Ensemble for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses  He et al   2017    ELMo for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses ESIM   ELMo for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses BiLSTM-Attention   ELMo for Natural Language Inference."
362,362,5_paper_32,5_paper_41,634,636,1708.02312,1711.04289,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses KIM Ensemble for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses KIM for Natural Language Inference."
363,363,5_paper_32,5_paper_25,634,637,1708.02312,1603.06021,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference."
364,364,5_paper_32,5_paper_13,634,638,1708.02312,1508.05326,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference."
365,365,5_paper_32,5_paper_50,634,639,1708.02312,1802.05577,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference."
366,366,5_paper_32,5_paper_35,634,641,1708.02312,1901.11504,Natural Language Inference,"sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.","sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference."
367,367,5_paper_32,5_paper_19,634,643,1708.02312,1509.06664,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference."
368,368,5_paper_32,5_paper_5,634,651,1708.02312,1707.02786,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference."
369,369,5_paper_32,5_paper_1,634,657,1708.02312,1605.05573,Natural Language Inference,sent: Article 1 and Article 2 use SNLI dataset.,"sent: Article 1 uses 600D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference. sent: Article 1 uses 300D Residual stacked encoders approach for Natural Language Inference, while Article 2 uses 50D stacked TC-LSTMs for Natural Language Inference."
370,370,8_paper_3,8_paper_0,90,92,1804.01005,1703.07834,3D Face Reconstruction,"sent: Article 1 and Article 2 use Florence dataset. sent: Article 1 uses multiple datasets:  AFLW2000,  BIWI, and  AFLW2000-3D, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses 3DDFA approach for 3D Face Reconstruction, while Article 2 uses VRN-Guided for 3D Face Reconstruction. sent: Article 1 uses 3DDFA   SDM approach for 3D Face Reconstruction, while Article 2 uses VRN-Guided for 3D Face Reconstruction."
371,371,8_paper_3,8_paper_1,90,93,1804.01005,1701.0536,3D Face Reconstruction,"sent: Article 1 and Article 2 use Florence dataset. sent: Article 1 uses multiple datasets:  AFLW2000,  BIWI, and  AFLW2000-3D, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses 3DDFA approach for 3D Face Reconstruction, while Article 2 uses itwmm for 3D Face Reconstruction. sent: Article 1 uses 3DDFA   SDM approach for 3D Face Reconstruction, while Article 2 uses itwmm for 3D Face Reconstruction."
372,372,8_paper_0,8_paper_1,92,93,1703.07834,1701.0536,3D Face Reconstruction,sent: Article 1 and Article 2 use Florence dataset.,"sent: Article 1 uses VRN-Guided approach for 3D Face Reconstruction, while Article 2 uses itwmm for 3D Face Reconstruction."
373,373,10_paper_10,10_paper_15,110,111,1701.06264,1707.01629,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  SVHN, while Article 2 uses ImageNet dataset.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses DPN-131 for Image Classification."
374,374,10_paper_10,10_paper_58,110,112,1701.06264,1406.3332,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses SVHN dataset, while Article 2 uses multiple datasets:  STL-10, and  MNIST.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses CKN for Image Classification."
375,375,10_paper_10,10_paper_48,110,113,1701.06264,1509.08985,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MNIST, and  CIFAR-100.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
376,376,10_paper_10,10_paper_2,110,115,1701.06264,1604.04112,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses SVHN dataset, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification."
377,377,10_paper_10,10_paper_26,110,116,1701.06264,1506.03767,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses SVHN dataset, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses Spectral Representations for Convolutional Neural Networks for Image Classification."
378,378,10_paper_10,10_paper_28,110,118,1701.06264,1412.6806,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses SVHN dataset, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses ACN for Image Classification."
379,379,10_paper_10,10_paper_18,110,119,1701.06264,1301.3557,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
380,380,10_paper_10,10_paper_35,110,1111,1701.06264,1902.05509,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  SVHN, while Article 2 uses multiple datasets:  ImageNet, and  INRIA Holidays.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
381,381,10_paper_10,10_paper_51,110,1113,1701.06264,1904.01169,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  SVHN, while Article 2 uses multiple datasets:  ImageNet, and  CIFAR-100.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
382,382,10_paper_10,10_paper_50,110,1114,1701.06264,1708.04896,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  SVHN, while Article 2 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses TriNet   Random Erasing for Image Classification. sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses SVDNet   Random Erasing for Image Classification. sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses I ORE for Image Classification. sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses Random Erasing for Image Classification."
383,383,10_paper_10,10_paper_54,110,1116,1701.06264,1512.03385,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses SVHN dataset, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
384,384,10_paper_10,10_paper_57,110,1119,1701.06264,1707.02968,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  SVHN, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
385,385,10_paper_10,10_paper_45,110,1121,1701.06264,1207.058,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses SVHN dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification."
386,386,10_paper_10,10_paper_7,110,1122,1701.06264,1605.07146,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
387,387,10_paper_10,10_paper_44,110,1124,1701.06264,1603.05027,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses SVHN dataset, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
388,388,10_paper_10,10_paper_11,110,1127,1701.06264,1409.5185,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MNIST, and  CIFAR-100.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses DSN for Image Classification."
389,389,10_paper_10,10_paper_20,110,1130,1701.06264,1511.07289,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses SVHN dataset, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
390,390,10_paper_10,10_paper_25,110,1133,1701.06264,1503.04596,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses MNIST dataset.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
391,391,10_paper_10,10_paper_0,110,1135,1701.06264,1611.05431,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  SVHN, while Article 2 uses ImageNet dataset.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
392,392,10_paper_10,10_paper_30,110,1141,1701.06264,1206.2944,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses SVHN dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses GP EI for Image Classification."
393,393,10_paper_10,10_paper_61,110,1142,1701.06264,1610.02915,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses SVHN dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
394,394,10_paper_10,10_paper_31,110,1149,1701.06264,1407.3068,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses SVHN dataset, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
395,395,10_paper_10,10_paper_22,110,1159,1701.06264,1605.07648,Image Classification,"sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 uses CIFAR-10 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
396,396,10_paper_15,10_paper_58,111,112,1707.01629,1406.3332,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  STL-10,  MNIST, and  CIFAR-10.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses CKN for Image Classification."
397,397,10_paper_15,10_paper_48,111,113,1707.01629,1509.08985,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN,  MNIST, and  CIFAR-100.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
398,398,10_paper_15,10_paper_2,111,115,1707.01629,1604.04112,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification."
399,399,10_paper_15,10_paper_26,111,116,1707.01629,1506.03767,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses Spectral Representations for Convolutional Neural Networks for Image Classification."
400,400,10_paper_15,10_paper_28,111,118,1707.01629,1412.6806,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses ACN for Image Classification."
401,401,10_paper_15,10_paper_18,111,119,1707.01629,1301.3557,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
402,402,10_paper_15,10_paper_35,111,1111,1707.01629,1902.05509,Image Classification,"sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses INRIA Holidays dataset.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
403,403,10_paper_15,10_paper_51,111,1113,1707.01629,1904.01169,Image Classification,"sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
404,404,10_paper_15,10_paper_50,111,1114,1707.01629,1708.04896,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses TriNet   Random Erasing for Image Classification. sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses SVDNet   Random Erasing for Image Classification. sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses I ORE for Image Classification. sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses Random Erasing for Image Classification."
405,405,10_paper_15,10_paper_54,111,1116,1707.01629,1512.03385,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10,  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
406,406,10_paper_15,10_paper_9,111,1117,1707.01629,1701.06264,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10, and  SVHN.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification."
407,407,10_paper_15,10_paper_57,111,1119,1707.01629,1707.02968,Image Classification,"sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses PASCAL VOC 2012 dataset.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
408,408,10_paper_15,10_paper_45,111,1121,1707.01629,1207.058,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification."
409,409,10_paper_15,10_paper_7,111,1122,1707.01629,1605.07146,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
410,410,10_paper_15,10_paper_44,111,1124,1707.01629,1603.05027,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
411,411,10_paper_15,10_paper_11,111,1127,1707.01629,1409.5185,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN,  MNIST, and  CIFAR-100.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses DSN for Image Classification."
412,412,10_paper_15,10_paper_20,111,1130,1707.01629,1511.07289,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
413,413,10_paper_15,10_paper_25,111,1133,1707.01629,1503.04596,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  MNIST.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
414,414,10_paper_15,10_paper_0,111,1135,1707.01629,1611.05431,Image Classification,sent: Article 1 and Article 2 use ImageNet dataset.,"sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
415,415,10_paper_15,10_paper_30,111,1141,1707.01629,1206.2944,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses GP EI for Image Classification."
416,416,10_paper_15,10_paper_61,111,1142,1707.01629,1610.02915,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
417,417,10_paper_15,10_paper_31,111,1149,1707.01629,1407.3068,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
418,418,10_paper_15,10_paper_22,111,1159,1707.01629,1605.07648,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses SVHN dataset.","sent: Article 1 uses DPN-131 approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
419,419,10_paper_58,10_paper_48,112,113,1406.3332,1509.08985,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use MNIST dataset. sent: Article 1 uses STL-10 dataset, while Article 2 uses multiple datasets:  SVHN, and  CIFAR-100.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
420,420,10_paper_58,10_paper_2,112,115,1406.3332,1604.04112,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  STL-10, and  MNIST, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification."
421,421,10_paper_58,10_paper_26,112,116,1406.3332,1506.03767,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  STL-10, and  MNIST, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses Spectral Representations for Convolutional Neural Networks for Image Classification."
422,422,10_paper_58,10_paper_28,112,118,1406.3332,1412.6806,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  STL-10, and  MNIST, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses ACN for Image Classification."
423,423,10_paper_58,10_paper_18,112,119,1406.3332,1301.3557,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  STL-10, and  MNIST, while Article 2 uses multiple datasets:  SVHN, and  CIFAR-100.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
424,424,10_paper_58,10_paper_35,112,1111,1406.3332,1902.05509,Image Classification,"sent: Article 1 uses multiple datasets:  STL-10,  MNIST, and  CIFAR-10, while Article 2 uses multiple datasets:  ImageNet, and  INRIA Holidays.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
425,425,10_paper_58,10_paper_51,112,1113,1406.3332,1904.01169,Image Classification,"sent: Article 1 uses multiple datasets:  STL-10,  MNIST, and  CIFAR-10, while Article 2 uses multiple datasets:  ImageNet, and  CIFAR-100.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
426,426,10_paper_58,10_paper_50,112,1114,1406.3332,1708.04896,Image Classification,"sent: Article 1 uses multiple datasets:  STL-10,  MNIST, and  CIFAR-10, while Article 2 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses TriNet   Random Erasing for Image Classification. sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses SVDNet   Random Erasing for Image Classification. sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses I ORE for Image Classification. sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses Random Erasing for Image Classification."
427,427,10_paper_58,10_paper_54,112,1116,1406.3332,1512.03385,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  STL-10, and  MNIST, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
428,428,10_paper_58,10_paper_9,112,1117,1406.3332,1701.06264,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  STL-10, and  MNIST, while Article 2 uses SVHN dataset.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification."
429,429,10_paper_58,10_paper_57,112,1119,1406.3332,1707.02968,Image Classification,"sent: Article 1 uses multiple datasets:  STL-10,  MNIST, and  CIFAR-10, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
430,430,10_paper_58,10_paper_45,112,1121,1406.3332,1207.058,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  STL-10, and  MNIST, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification."
431,431,10_paper_58,10_paper_7,112,1122,1406.3332,1605.07146,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  STL-10, and  MNIST, while Article 2 uses multiple datasets:  SVHN, and  CIFAR-100.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
432,432,10_paper_58,10_paper_44,112,1124,1406.3332,1603.05027,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  STL-10, and  MNIST, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
433,433,10_paper_58,10_paper_16,112,1126,1406.3332,1707.01629,Image Classification,"sent: Article 1 uses multiple datasets:  STL-10,  MNIST, and  CIFAR-10, while Article 2 uses ImageNet dataset.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses DPN-131 for Image Classification."
434,434,10_paper_58,10_paper_11,112,1127,1406.3332,1409.5185,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use MNIST dataset. sent: Article 1 uses STL-10 dataset, while Article 2 uses multiple datasets:  SVHN, and  CIFAR-100.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses DSN for Image Classification."
435,435,10_paper_58,10_paper_20,112,1130,1406.3332,1511.07289,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  STL-10, and  MNIST, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
436,436,10_paper_58,10_paper_25,112,1133,1406.3332,1503.04596,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use MNIST dataset. sent: Article 1 uses STL-10 dataset, while Article 2 uses SVHN dataset.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
437,437,10_paper_58,10_paper_0,112,1135,1406.3332,1611.05431,Image Classification,"sent: Article 1 uses multiple datasets:  STL-10,  MNIST, and  CIFAR-10, while Article 2 uses ImageNet dataset.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
438,438,10_paper_58,10_paper_30,112,1141,1406.3332,1206.2944,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  STL-10, and  MNIST, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses GP EI for Image Classification."
439,439,10_paper_58,10_paper_61,112,1142,1406.3332,1610.02915,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  STL-10, and  MNIST, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
440,440,10_paper_58,10_paper_31,112,1149,1406.3332,1407.3068,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  STL-10, and  MNIST, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
441,441,10_paper_58,10_paper_22,112,1159,1406.3332,1605.07648,Image Classification,"sent: Article 1 uses multiple datasets:  STL-10,  MNIST, and  CIFAR-10, while Article 2 uses SVHN dataset.","sent: Article 1 uses CKN approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
442,442,10_paper_48,10_paper_2,113,115,1509.08985,1604.04112,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  MNIST, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification."
443,443,10_paper_48,10_paper_26,113,116,1509.08985,1506.03767,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  MNIST, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses Spectral Representations for Convolutional Neural Networks for Image Classification."
444,444,10_paper_48,10_paper_28,113,118,1509.08985,1412.6806,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  MNIST, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses ACN for Image Classification."
445,445,10_paper_48,10_paper_18,113,119,1509.08985,1301.3557,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses MNIST dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
446,446,10_paper_48,10_paper_35,113,1111,1509.08985,1902.05509,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN,  MNIST, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  INRIA Holidays.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
447,447,10_paper_48,10_paper_51,113,1113,1509.08985,1904.01169,Image Classification,"sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN, and  MNIST, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
448,448,10_paper_48,10_paper_50,113,1114,1509.08985,1708.04896,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN,  MNIST, and  CIFAR-100, while Article 2 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses TriNet   Random Erasing for Image Classification. sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses SVDNet   Random Erasing for Image Classification. sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses I ORE for Image Classification. sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses Random Erasing for Image Classification."
449,449,10_paper_48,10_paper_54,113,1116,1509.08985,1512.03385,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN,  MNIST, and  CIFAR-100, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
450,450,10_paper_48,10_paper_9,113,1117,1509.08985,1701.06264,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 uses multiple datasets:  MNIST, and  CIFAR-100, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification."
451,451,10_paper_48,10_paper_57,113,1119,1509.08985,1707.02968,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN,  MNIST, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
452,452,10_paper_48,10_paper_45,113,1121,1509.08985,1207.058,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN,  MNIST, and  CIFAR-100, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification."
453,453,10_paper_48,10_paper_7,113,1122,1509.08985,1605.07146,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses MNIST dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
454,454,10_paper_48,10_paper_44,113,1124,1509.08985,1603.05027,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  MNIST, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
455,455,10_paper_48,10_paper_59,113,1125,1509.08985,1406.3332,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use MNIST dataset. sent: Article 1 uses multiple datasets:  SVHN, and  CIFAR-100, while Article 2 uses STL-10 dataset.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses CKN for Image Classification."
456,456,10_paper_48,10_paper_16,113,1126,1509.08985,1707.01629,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN,  MNIST, and  CIFAR-100, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses DPN-131 for Image Classification."
457,457,10_paper_48,10_paper_11,113,1127,1509.08985,1409.5185,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset.sent: Article 1 and Article 2 use MNIST dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses DSN for Image Classification."
458,458,10_paper_48,10_paper_20,113,1130,1509.08985,1511.07289,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  MNIST, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
459,459,10_paper_48,10_paper_25,113,1133,1509.08985,1503.04596,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset.sent: Article 1 and Article 2 use MNIST dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
460,460,10_paper_48,10_paper_0,113,1135,1509.08985,1611.05431,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN,  MNIST, and  CIFAR-100, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
461,461,10_paper_48,10_paper_30,113,1141,1509.08985,1206.2944,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN,  MNIST, and  CIFAR-100, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses GP EI for Image Classification."
462,462,10_paper_48,10_paper_61,113,1142,1509.08985,1610.02915,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN,  MNIST, and  CIFAR-100, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
463,463,10_paper_48,10_paper_31,113,1149,1509.08985,1407.3068,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  MNIST, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
464,464,10_paper_48,10_paper_22,113,1159,1509.08985,1605.07648,Image Classification,"sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 uses multiple datasets:  CIFAR-10,  MNIST, and  CIFAR-100, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Tree Max-Avg pooling approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
465,465,10_paper_2,10_paper_26,115,116,1604.04112,1506.03767,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses Spectral Representations for Convolutional Neural Networks for Image Classification."
466,466,10_paper_2,10_paper_47,115,117,1604.04112,1509.08985,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN, and  MNIST.","sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
467,467,10_paper_2,10_paper_28,115,118,1604.04112,1412.6806,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses ACN for Image Classification."
468,468,10_paper_2,10_paper_18,115,119,1604.04112,1301.3557,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses SVHN dataset.","sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
469,469,10_paper_2,10_paper_35,115,1111,1604.04112,1902.05509,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  INRIA Holidays.","sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
470,470,10_paper_2,10_paper_51,115,1113,1604.04112,1904.01169,Image Classification,"sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses ImageNet dataset.","sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
471,471,10_paper_2,10_paper_50,115,1114,1604.04112,1708.04896,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses TriNet   Random Erasing for Image Classification. sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses SVDNet   Random Erasing for Image Classification. sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses I ORE for Image Classification. sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses Random Erasing for Image Classification."
472,472,10_paper_2,10_paper_54,115,1116,1604.04112,1512.03385,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
473,473,10_paper_2,10_paper_9,115,1117,1604.04112,1701.06264,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses SVHN dataset.","sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification."
474,474,10_paper_2,10_paper_57,115,1119,1604.04112,1707.02968,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
475,475,10_paper_2,10_paper_45,115,1121,1604.04112,1207.058,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification."
476,476,10_paper_2,10_paper_7,115,1122,1604.04112,1605.07146,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses SVHN dataset.","sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
477,477,10_paper_2,10_paper_44,115,1124,1604.04112,1603.05027,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
478,478,10_paper_2,10_paper_59,115,1125,1604.04112,1406.3332,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  STL-10, and  MNIST.","sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses CKN for Image Classification."
479,479,10_paper_2,10_paper_16,115,1126,1604.04112,1707.01629,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses ImageNet dataset.","sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses DPN-131 for Image Classification."
480,480,10_paper_2,10_paper_11,115,1127,1604.04112,1409.5185,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN, and  MNIST.","sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses DSN for Image Classification."
481,481,10_paper_2,10_paper_20,115,1130,1604.04112,1511.07289,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
482,482,10_paper_2,10_paper_25,115,1133,1604.04112,1503.04596,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  SVHN, and  MNIST.","sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
483,483,10_paper_2,10_paper_0,115,1135,1604.04112,1611.05431,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses ImageNet dataset.","sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
484,484,10_paper_2,10_paper_30,115,1141,1604.04112,1206.2944,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses GP EI for Image Classification."
485,485,10_paper_2,10_paper_61,115,1142,1604.04112,1610.02915,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
486,486,10_paper_2,10_paper_31,115,1149,1604.04112,1407.3068,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
487,487,10_paper_2,10_paper_22,115,1159,1604.04112,1605.07648,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses SVHN dataset.","sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
488,488,10_paper_26,10_paper_47,116,117,1506.03767,1509.08985,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN, and  MNIST.","sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
489,489,10_paper_26,10_paper_28,116,118,1506.03767,1412.6806,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses ACN for Image Classification."
490,490,10_paper_26,10_paper_18,116,119,1506.03767,1301.3557,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses SVHN dataset.","sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
491,491,10_paper_26,10_paper_35,116,1111,1506.03767,1902.05509,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  INRIA Holidays.","sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
492,492,10_paper_26,10_paper_51,116,1113,1506.03767,1904.01169,Image Classification,"sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
493,493,10_paper_26,10_paper_50,116,1114,1506.03767,1708.04896,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses TriNet   Random Erasing for Image Classification. sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses SVDNet   Random Erasing for Image Classification. sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses I ORE for Image Classification. sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses Random Erasing for Image Classification."
494,494,10_paper_26,10_paper_54,116,1116,1506.03767,1512.03385,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
495,495,10_paper_26,10_paper_9,116,1117,1506.03767,1701.06264,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses SVHN dataset.","sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification."
496,496,10_paper_26,10_paper_57,116,1119,1506.03767,1707.02968,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
497,497,10_paper_26,10_paper_45,116,1121,1506.03767,1207.058,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification."
498,498,10_paper_26,10_paper_7,116,1122,1506.03767,1605.07146,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses SVHN dataset.","sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
499,499,10_paper_26,10_paper_3,116,1123,1506.03767,1604.04112,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification."
500,500,10_paper_26,10_paper_44,116,1124,1506.03767,1603.05027,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
501,501,10_paper_26,10_paper_59,116,1125,1506.03767,1406.3332,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  STL-10, and  MNIST.","sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses CKN for Image Classification."
502,502,10_paper_26,10_paper_16,116,1126,1506.03767,1707.01629,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses DPN-131 for Image Classification."
503,503,10_paper_26,10_paper_11,116,1127,1506.03767,1409.5185,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN, and  MNIST.","sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses DSN for Image Classification."
504,504,10_paper_26,10_paper_20,116,1130,1506.03767,1511.07289,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
505,505,10_paper_26,10_paper_25,116,1133,1506.03767,1503.04596,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  SVHN, and  MNIST.","sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
506,506,10_paper_26,10_paper_0,116,1135,1506.03767,1611.05431,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
507,507,10_paper_26,10_paper_30,116,1141,1506.03767,1206.2944,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses GP EI for Image Classification."
508,508,10_paper_26,10_paper_61,116,1142,1506.03767,1610.02915,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
509,509,10_paper_26,10_paper_31,116,1149,1506.03767,1407.3068,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
510,510,10_paper_26,10_paper_22,116,1159,1506.03767,1605.07648,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses SVHN dataset.","sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
511,511,10_paper_28,10_paper_18,118,119,1412.6806,1301.3557,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses SVHN dataset.","sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
512,512,10_paper_28,10_paper_35,118,1111,1412.6806,1902.05509,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  INRIA Holidays.","sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
513,513,10_paper_28,10_paper_27,118,1112,1412.6806,1506.03767,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses Spectral Representations for Convolutional Neural Networks for Image Classification."
514,514,10_paper_28,10_paper_51,118,1113,1412.6806,1904.01169,Image Classification,"sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses ImageNet dataset.","sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
515,515,10_paper_28,10_paper_50,118,1114,1412.6806,1708.04896,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses TriNet   Random Erasing for Image Classification. sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses SVDNet   Random Erasing for Image Classification. sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses I ORE for Image Classification. sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses Random Erasing for Image Classification."
516,516,10_paper_28,10_paper_54,118,1116,1412.6806,1512.03385,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
517,517,10_paper_28,10_paper_9,118,1117,1412.6806,1701.06264,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses SVHN dataset.","sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification."
518,518,10_paper_28,10_paper_57,118,1119,1412.6806,1707.02968,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
519,519,10_paper_28,10_paper_45,118,1121,1412.6806,1207.058,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification."
520,520,10_paper_28,10_paper_7,118,1122,1412.6806,1605.07146,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses SVHN dataset.","sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
521,521,10_paper_28,10_paper_3,118,1123,1412.6806,1604.04112,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification."
522,522,10_paper_28,10_paper_44,118,1124,1412.6806,1603.05027,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
523,523,10_paper_28,10_paper_59,118,1125,1412.6806,1406.3332,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  STL-10, and  MNIST.","sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses CKN for Image Classification."
524,524,10_paper_28,10_paper_16,118,1126,1412.6806,1707.01629,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses ImageNet dataset.","sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses DPN-131 for Image Classification."
525,525,10_paper_28,10_paper_11,118,1127,1412.6806,1409.5185,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN, and  MNIST.","sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses DSN for Image Classification."
526,526,10_paper_28,10_paper_20,118,1130,1412.6806,1511.07289,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
527,527,10_paper_28,10_paper_25,118,1133,1412.6806,1503.04596,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  SVHN, and  MNIST.","sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
528,528,10_paper_28,10_paper_0,118,1135,1412.6806,1611.05431,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses ImageNet dataset.","sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
529,529,10_paper_28,10_paper_30,118,1141,1412.6806,1206.2944,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses GP EI for Image Classification."
530,530,10_paper_28,10_paper_61,118,1142,1412.6806,1610.02915,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
531,531,10_paper_28,10_paper_46,118,1143,1412.6806,1509.08985,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN, and  MNIST.","sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
532,532,10_paper_28,10_paper_31,118,1149,1412.6806,1407.3068,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
533,533,10_paper_28,10_paper_22,118,1159,1412.6806,1605.07648,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses SVHN dataset.","sent: Article 1 uses ACN approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
534,534,10_paper_18,10_paper_29,119,1110,1301.3557,1412.6806,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses SVHN dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses ACN for Image Classification."
535,535,10_paper_18,10_paper_35,119,1111,1301.3557,1902.05509,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  INRIA Holidays.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
536,536,10_paper_18,10_paper_27,119,1112,1301.3557,1506.03767,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses SVHN dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses Spectral Representations for Convolutional Neural Networks for Image Classification."
537,537,10_paper_18,10_paper_51,119,1113,1301.3557,1904.01169,Image Classification,"sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses multiple datasets:  CIFAR-10, and  SVHN, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
538,538,10_paper_18,10_paper_50,119,1114,1301.3557,1708.04896,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100, while Article 2 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses TriNet   Random Erasing for Image Classification. sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses SVDNet   Random Erasing for Image Classification. sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses I ORE for Image Classification. sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses Random Erasing for Image Classification."
539,539,10_paper_18,10_paper_54,119,1116,1301.3557,1512.03385,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  CIFAR-100, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
540,540,10_paper_18,10_paper_9,119,1117,1301.3557,1701.06264,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification."
541,541,10_paper_18,10_paper_57,119,1119,1301.3557,1707.02968,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
542,542,10_paper_18,10_paper_45,119,1121,1301.3557,1207.058,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  CIFAR-100, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification."
543,543,10_paper_18,10_paper_7,119,1122,1301.3557,1605.07146,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
544,544,10_paper_18,10_paper_3,119,1123,1301.3557,1604.04112,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses SVHN dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification."
545,545,10_paper_18,10_paper_44,119,1124,1301.3557,1603.05027,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses SVHN dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
546,546,10_paper_18,10_paper_59,119,1125,1301.3557,1406.3332,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  CIFAR-100, while Article 2 uses multiple datasets:  STL-10, and  MNIST.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses CKN for Image Classification."
547,547,10_paper_18,10_paper_16,119,1126,1301.3557,1707.01629,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses DPN-131 for Image Classification."
548,548,10_paper_18,10_paper_11,119,1127,1301.3557,1409.5185,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses MNIST dataset.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses DSN for Image Classification."
549,549,10_paper_18,10_paper_20,119,1130,1301.3557,1511.07289,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses SVHN dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
550,550,10_paper_18,10_paper_25,119,1133,1301.3557,1503.04596,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses MNIST dataset.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
551,551,10_paper_18,10_paper_0,119,1135,1301.3557,1611.05431,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
552,552,10_paper_18,10_paper_30,119,1141,1301.3557,1206.2944,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  CIFAR-100, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses GP EI for Image Classification."
553,553,10_paper_18,10_paper_61,119,1142,1301.3557,1610.02915,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  CIFAR-100, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
554,554,10_paper_18,10_paper_46,119,1143,1301.3557,1509.08985,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses MNIST dataset.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
555,555,10_paper_18,10_paper_31,119,1149,1301.3557,1407.3068,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses SVHN dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
556,556,10_paper_18,10_paper_22,119,1159,1301.3557,1605.07648,Image Classification,"sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
557,557,10_paper_35,10_paper_27,1111,1112,1902.05509,1506.03767,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  INRIA Holidays, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses Spectral Representations for Convolutional Neural Networks for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses Spectral Representations for Convolutional Neural Networks for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses Spectral Representations for Convolutional Neural Networks for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses Spectral Representations for Convolutional Neural Networks for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses Spectral Representations for Convolutional Neural Networks for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses Spectral Representations for Convolutional Neural Networks for Image Classification."
558,558,10_paper_35,10_paper_51,1111,1113,1902.05509,1904.01169,Image Classification,"sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 uses INRIA Holidays dataset, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
559,559,10_paper_35,10_paper_50,1111,1114,1902.05509,1708.04896,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  INRIA Holidays, while Article 2 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses TriNet   Random Erasing for Image Classification. sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses SVDNet   Random Erasing for Image Classification. sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses I ORE for Image Classification. sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses Random Erasing for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses TriNet   Random Erasing for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses SVDNet   Random Erasing for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses I ORE for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses Random Erasing for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses TriNet   Random Erasing for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses SVDNet   Random Erasing for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses I ORE for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses Random Erasing for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses TriNet   Random Erasing for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses SVDNet   Random Erasing for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses I ORE for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses Random Erasing for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses TriNet   Random Erasing for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses SVDNet   Random Erasing for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses I ORE for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses Random Erasing for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses TriNet   Random Erasing for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses SVDNet   Random Erasing for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses I ORE for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses Random Erasing for Image Classification."
560,560,10_paper_35,10_paper_54,1111,1116,1902.05509,1512.03385,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  INRIA Holidays, while Article 2 uses multiple datasets:  CIFAR-10,  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
561,561,10_paper_35,10_paper_9,1111,1117,1902.05509,1701.06264,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  INRIA Holidays, while Article 2 uses multiple datasets:  CIFAR-10, and  SVHN.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification."
562,562,10_paper_35,10_paper_57,1111,1119,1902.05509,1707.02968,Image Classification,"sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 uses INRIA Holidays dataset, while Article 2 uses PASCAL VOC 2012 dataset.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
563,563,10_paper_35,10_paper_45,1111,1121,1902.05509,1207.058,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  INRIA Holidays, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification."
564,564,10_paper_35,10_paper_7,1111,1122,1902.05509,1605.07146,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  INRIA Holidays, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
565,565,10_paper_35,10_paper_3,1111,1123,1902.05509,1604.04112,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  INRIA Holidays, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification."
566,566,10_paper_35,10_paper_44,1111,1124,1902.05509,1603.05027,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  INRIA Holidays, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
567,567,10_paper_35,10_paper_59,1111,1125,1902.05509,1406.3332,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  INRIA Holidays, while Article 2 uses multiple datasets:  STL-10,  MNIST, and  CIFAR-10.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses CKN for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses CKN for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses CKN for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses CKN for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses CKN for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses CKN for Image Classification."
568,568,10_paper_35,10_paper_16,1111,1126,1902.05509,1707.01629,Image Classification,"sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 uses INRIA Holidays dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses DPN-131 for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses DPN-131 for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses DPN-131 for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses DPN-131 for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses DPN-131 for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses DPN-131 for Image Classification."
569,569,10_paper_35,10_paper_11,1111,1127,1902.05509,1409.5185,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  INRIA Holidays, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN,  MNIST, and  CIFAR-100.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses DSN for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses DSN for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses DSN for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses DSN for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses DSN for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses DSN for Image Classification."
570,570,10_paper_35,10_paper_19,1111,1129,1902.05509,1301.3557,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  INRIA Holidays, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
571,571,10_paper_35,10_paper_20,1111,1130,1902.05509,1511.07289,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  INRIA Holidays, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
572,572,10_paper_35,10_paper_25,1111,1133,1902.05509,1503.04596,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  INRIA Holidays, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  MNIST.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses FLSCNN for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses FLSCNN for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses FLSCNN for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses FLSCNN for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses FLSCNN for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
573,573,10_paper_35,10_paper_0,1111,1135,1902.05509,1611.05431,Image Classification,"sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 uses INRIA Holidays dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
574,574,10_paper_35,10_paper_30,1111,1141,1902.05509,1206.2944,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  INRIA Holidays, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses GP EI for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses GP EI for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses GP EI for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses GP EI for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses GP EI for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses GP EI for Image Classification."
575,575,10_paper_35,10_paper_61,1111,1142,1902.05509,1610.02915,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  INRIA Holidays, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
576,576,10_paper_35,10_paper_46,1111,1143,1902.05509,1509.08985,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  INRIA Holidays, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN,  MNIST, and  CIFAR-100.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
577,577,10_paper_35,10_paper_31,1111,1149,1902.05509,1407.3068,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  INRIA Holidays, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
578,578,10_paper_35,10_paper_22,1111,1159,1902.05509,1605.07648,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  INRIA Holidays, while Article 2 uses SVHN dataset.","sent: Article 1 uses MultiGrain R50   500 approach for Image Classification, while Article 2 uses FractalNet for Image Classification. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Classification, while Article 2 uses FractalNet for Image Classification. sent: Article 1 uses MultiGrain R50   800 approach for Image Classification, while Article 2 uses FractalNet for Image Classification. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Classification, while Article 2 uses FractalNet for Image Classification. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Classification, while Article 2 uses FractalNet for Image Classification. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
579,579,10_paper_51,10_paper_50,1113,1114,1904.01169,1708.04896,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  CIFAR-100, while Article 2 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses TriNet   Random Erasing for Image Classification. sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses SVDNet   Random Erasing for Image Classification. sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses I ORE for Image Classification. sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses Random Erasing for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses TriNet   Random Erasing for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses SVDNet   Random Erasing for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses I ORE for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses Random Erasing for Image Classification."
580,580,10_paper_51,10_paper_54,1113,1116,1904.01169,1512.03385,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  CIFAR-100, while Article 2 uses multiple datasets:  CIFAR-10,  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
581,581,10_paper_51,10_paper_9,1113,1117,1904.01169,1701.06264,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  CIFAR-100, while Article 2 uses multiple datasets:  CIFAR-10, and  SVHN.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification."
582,582,10_paper_51,10_paper_37,1113,1118,1904.01169,1902.05509,Image Classification,"sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses INRIA Holidays dataset.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
583,583,10_paper_51,10_paper_57,1113,1119,1904.01169,1707.02968,Image Classification,"sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses PASCAL VOC 2012 dataset.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
584,584,10_paper_51,10_paper_45,1113,1121,1904.01169,1207.058,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  CIFAR-100, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification."
585,585,10_paper_51,10_paper_7,1113,1122,1904.01169,1605.07146,Image Classification,"sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10, and  SVHN.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
586,586,10_paper_51,10_paper_3,1113,1123,1904.01169,1604.04112,Image Classification,"sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses ImageNet dataset, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification."
587,587,10_paper_51,10_paper_44,1113,1124,1904.01169,1603.05027,Image Classification,"sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses ImageNet dataset, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
588,588,10_paper_51,10_paper_59,1113,1125,1904.01169,1406.3332,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  CIFAR-100, while Article 2 uses multiple datasets:  STL-10,  MNIST, and  CIFAR-10.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses CKN for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses CKN for Image Classification."
589,589,10_paper_51,10_paper_16,1113,1126,1904.01169,1707.01629,Image Classification,"sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses DPN-131 for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses DPN-131 for Image Classification."
590,590,10_paper_51,10_paper_11,1113,1127,1904.01169,1409.5185,Image Classification,"sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  MNIST.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses DSN for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses DSN for Image Classification."
591,591,10_paper_51,10_paper_19,1113,1129,1904.01169,1301.3557,Image Classification,"sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10, and  SVHN.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
592,592,10_paper_51,10_paper_20,1113,1130,1904.01169,1511.07289,Image Classification,"sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses ImageNet dataset, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
593,593,10_paper_51,10_paper_25,1113,1133,1904.01169,1503.04596,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  CIFAR-100, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  MNIST.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses FLSCNN for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
594,594,10_paper_51,10_paper_0,1113,1135,1904.01169,1611.05431,Image Classification,"sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
595,595,10_paper_51,10_paper_30,1113,1141,1904.01169,1206.2944,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  CIFAR-100, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses GP EI for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses GP EI for Image Classification."
596,596,10_paper_51,10_paper_61,1113,1142,1904.01169,1610.02915,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  CIFAR-100, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
597,597,10_paper_51,10_paper_46,1113,1143,1904.01169,1509.08985,Image Classification,"sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  MNIST.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
598,598,10_paper_51,10_paper_31,1113,1149,1904.01169,1407.3068,Image Classification,"sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses ImageNet dataset, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
599,599,10_paper_51,10_paper_22,1113,1159,1904.01169,1605.07648,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  CIFAR-100, while Article 2 uses SVHN dataset.","sent: Article 1 uses Res2Net-DLA-60 approach for Image Classification, while Article 2 uses FractalNet for Image Classification. sent: Article 1 uses Res2NeXt-29 approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
600,600,10_paper_50,10_paper_53,1114,1115,1708.04896,1904.01169,Image Classification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses multiple datasets:  ImageNet, and  CIFAR-100.","sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
601,601,10_paper_50,10_paper_54,1114,1116,1708.04896,1512.03385,Image Classification,"sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 uses multiple datasets:  DukeMTMC-reID, and  Fashion-MNIST, while Article 2 uses multiple datasets:  CIFAR-10, and  COCO.","sent: Article 1 uses I ORE approach for Object Detection, while Article 2 uses ResNet-101 for Object Detection. sent: Article 1 uses I ORE approach for Object Detection, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Object Detection. sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
602,602,10_paper_50,10_paper_9,1114,1117,1708.04896,1701.06264,Image Classification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses multiple datasets:  CIFAR-10, and  SVHN.","sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification."
603,603,10_paper_50,10_paper_37,1114,1118,1708.04896,1902.05509,Image Classification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses multiple datasets:  ImageNet, and  INRIA Holidays.","sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
604,604,10_paper_50,10_paper_57,1114,1119,1708.04896,1707.02968,Image Classification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
605,605,10_paper_50,10_paper_45,1114,1121,1708.04896,1207.058,Image Classification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification."
606,606,10_paper_50,10_paper_7,1114,1122,1708.04896,1605.07146,Image Classification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100.","sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
607,607,10_paper_50,10_paper_3,1114,1123,1708.04896,1604.04112,Image Classification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification."
608,608,10_paper_50,10_paper_44,1114,1124,1708.04896,1603.05027,Image Classification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
609,609,10_paper_50,10_paper_59,1114,1125,1708.04896,1406.3332,Image Classification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses multiple datasets:  STL-10,  MNIST, and  CIFAR-10.","sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses CKN for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses CKN for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses CKN for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses CKN for Image Classification."
610,610,10_paper_50,10_paper_16,1114,1126,1708.04896,1707.01629,Image Classification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses ImageNet dataset.","sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses DPN-131 for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses DPN-131 for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses DPN-131 for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses DPN-131 for Image Classification."
611,611,10_paper_50,10_paper_11,1114,1127,1708.04896,1409.5185,Image Classification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN,  MNIST, and  CIFAR-100.","sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses DSN for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses DSN for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses DSN for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses DSN for Image Classification."
612,612,10_paper_50,10_paper_19,1114,1129,1708.04896,1301.3557,Image Classification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100.","sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
613,613,10_paper_50,10_paper_20,1114,1130,1708.04896,1511.07289,Image Classification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
614,614,10_paper_50,10_paper_25,1114,1133,1708.04896,1503.04596,Image Classification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  MNIST.","sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses FLSCNN for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses FLSCNN for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses FLSCNN for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
615,615,10_paper_50,10_paper_0,1114,1135,1708.04896,1611.05431,Image Classification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses ImageNet dataset.","sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
616,616,10_paper_50,10_paper_30,1114,1141,1708.04896,1206.2944,Image Classification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses GP EI for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses GP EI for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses GP EI for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses GP EI for Image Classification."
617,617,10_paper_50,10_paper_61,1114,1142,1708.04896,1610.02915,Image Classification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
618,618,10_paper_50,10_paper_46,1114,1143,1708.04896,1509.08985,Image Classification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN,  MNIST, and  CIFAR-100.","sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
619,619,10_paper_50,10_paper_31,1114,1149,1708.04896,1407.3068,Image Classification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
620,620,10_paper_50,10_paper_22,1114,1159,1708.04896,1605.07648,Image Classification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses SVHN dataset.","sent: Article 1 uses TriNet   Random Erasing approach for Image Classification, while Article 2 uses FractalNet for Image Classification. sent: Article 1 uses SVDNet   Random Erasing approach for Image Classification, while Article 2 uses FractalNet for Image Classification. sent: Article 1 uses I ORE approach for Image Classification, while Article 2 uses FractalNet for Image Classification. sent: Article 1 uses Random Erasing approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
621,621,10_paper_54,10_paper_9,1116,1117,1512.03385,1701.06264,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses SVHN dataset.","sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification."
622,622,10_paper_54,10_paper_37,1116,1118,1512.03385,1902.05509,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  COCO, and  PASCAL VOC 2007, while Article 2 uses multiple datasets:  ImageNet, and  INRIA Holidays.","sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
623,623,10_paper_54,10_paper_57,1116,1119,1512.03385,1707.02968,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  COCO, and  PASCAL VOC 2007, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
624,624,10_paper_54,10_paper_45,1116,1121,1512.03385,1207.058,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification."
625,625,10_paper_54,10_paper_7,1116,1122,1512.03385,1605.07146,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses multiple datasets:  SVHN, and  CIFAR-100.","sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
626,626,10_paper_54,10_paper_3,1116,1123,1512.03385,1604.04112,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification."
627,627,10_paper_54,10_paper_44,1116,1124,1512.03385,1603.05027,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
628,628,10_paper_54,10_paper_59,1116,1125,1512.03385,1406.3332,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses multiple datasets:  STL-10, and  MNIST.","sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses CKN for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses CKN for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses CKN for Image Classification."
629,629,10_paper_54,10_paper_16,1116,1126,1512.03385,1707.01629,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  COCO, and  PASCAL VOC 2007, while Article 2 uses ImageNet dataset.","sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses DPN-131 for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses DPN-131 for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses DPN-131 for Image Classification."
630,630,10_paper_54,10_paper_11,1116,1127,1512.03385,1409.5185,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses multiple datasets:  SVHN,  MNIST, and  CIFAR-100.","sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses DSN for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses DSN for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses DSN for Image Classification."
631,631,10_paper_54,10_paper_19,1116,1129,1512.03385,1301.3557,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses multiple datasets:  SVHN, and  CIFAR-100.","sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
632,632,10_paper_54,10_paper_20,1116,1130,1512.03385,1511.07289,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
633,633,10_paper_54,10_paper_25,1116,1133,1512.03385,1503.04596,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses multiple datasets:  SVHN, and  MNIST.","sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses FLSCNN for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses FLSCNN for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
634,634,10_paper_54,10_paper_0,1116,1135,1512.03385,1611.05431,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  COCO, and  PASCAL VOC 2007, while Article 2 uses ImageNet dataset.","sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
635,635,10_paper_54,10_paper_30,1116,1141,1512.03385,1206.2944,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses GP EI for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses GP EI for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses GP EI for Image Classification."
636,636,10_paper_54,10_paper_61,1116,1142,1512.03385,1610.02915,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
637,637,10_paper_54,10_paper_46,1116,1143,1512.03385,1509.08985,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses multiple datasets:  SVHN,  MNIST, and  CIFAR-100.","sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
638,638,10_paper_54,10_paper_31,1116,1149,1512.03385,1407.3068,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
639,639,10_paper_54,10_paper_52,1116,1157,1512.03385,1904.01169,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  COCO, and  PASCAL VOC 2007, while Article 2 uses multiple datasets:  ImageNet, and  CIFAR-100.","sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
640,640,10_paper_54,10_paper_22,1116,1159,1512.03385,1605.07648,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  COCO, and  PASCAL VOC 2007, while Article 2 uses SVHN dataset.","sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses FractalNet for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses FractalNet for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
641,641,10_paper_57,10_paper_33,1119,1120,1707.02968,1902.05509,Image Classification,"sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 uses PASCAL VOC 2012 dataset, while Article 2 uses INRIA Holidays dataset.","sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
642,642,10_paper_57,10_paper_45,1119,1121,1707.02968,1207.058,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses Improving neural networks by preventing co-adaptation of feature detectors for Image Classification."
643,643,10_paper_57,10_paper_7,1119,1122,1707.02968,1605.07146,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100.","sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
644,644,10_paper_57,10_paper_3,1119,1123,1707.02968,1604.04112,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification."
645,645,10_paper_57,10_paper_44,1119,1124,1707.02968,1603.05027,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
646,646,10_paper_57,10_paper_59,1119,1125,1707.02968,1406.3332,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses multiple datasets:  STL-10,  MNIST, and  CIFAR-10.","sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses CKN for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses CKN for Image Classification."
647,647,10_paper_57,10_paper_16,1119,1126,1707.02968,1707.01629,Image Classification,"sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 uses PASCAL VOC 2012 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses DPN-131 for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses DPN-131 for Image Classification."
648,648,10_paper_57,10_paper_11,1119,1127,1707.02968,1409.5185,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN,  MNIST, and  CIFAR-100.","sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses DSN for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses DSN for Image Classification."
649,649,10_paper_57,10_paper_19,1119,1129,1707.02968,1301.3557,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100.","sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
650,650,10_paper_57,10_paper_20,1119,1130,1707.02968,1511.07289,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
651,651,10_paper_57,10_paper_25,1119,1133,1707.02968,1503.04596,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  MNIST.","sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses FLSCNN for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
652,652,10_paper_57,10_paper_0,1119,1135,1707.02968,1611.05431,Image Classification,"sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 uses PASCAL VOC 2012 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
653,653,10_paper_57,10_paper_30,1119,1141,1707.02968,1206.2944,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses GP EI for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses GP EI for Image Classification."
654,654,10_paper_57,10_paper_61,1119,1142,1707.02968,1610.02915,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
655,655,10_paper_57,10_paper_46,1119,1143,1707.02968,1509.08985,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN,  MNIST, and  CIFAR-100.","sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
656,656,10_paper_57,10_paper_55,1119,1147,1707.02968,1512.03385,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses multiple datasets:  CIFAR-10,  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
657,657,10_paper_57,10_paper_31,1119,1149,1707.02968,1407.3068,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
658,658,10_paper_57,10_paper_52,1119,1157,1707.02968,1904.01169,Image Classification,"sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 uses PASCAL VOC 2012 dataset, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
659,659,10_paper_57,10_paper_22,1119,1159,1707.02968,1605.07648,Image Classification,"sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses SVHN dataset.","sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses FractalNet for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
660,660,10_paper_45,10_paper_7,1121,1122,1207.058,1605.07146,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN, and  CIFAR-100.","sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
661,661,10_paper_45,10_paper_3,1121,1123,1207.058,1604.04112,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification."
662,662,10_paper_45,10_paper_44,1121,1124,1207.058,1603.05027,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
663,663,10_paper_45,10_paper_59,1121,1125,1207.058,1406.3332,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  STL-10, and  MNIST.","sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses CKN for Image Classification."
664,664,10_paper_45,10_paper_16,1121,1126,1207.058,1707.01629,Image Classification,"sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses DPN-131 for Image Classification."
665,665,10_paper_45,10_paper_11,1121,1127,1207.058,1409.5185,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN,  MNIST, and  CIFAR-100.","sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses DSN for Image Classification."
666,666,10_paper_45,10_paper_19,1121,1129,1207.058,1301.3557,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN, and  CIFAR-100.","sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
667,667,10_paper_45,10_paper_20,1121,1130,1207.058,1511.07289,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
668,668,10_paper_45,10_paper_25,1121,1133,1207.058,1503.04596,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN, and  MNIST.","sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
669,669,10_paper_45,10_paper_0,1121,1135,1207.058,1611.05431,Image Classification,"sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
670,670,10_paper_45,10_paper_39,1121,1137,1207.058,1902.05509,Image Classification,"sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses multiple datasets:  ImageNet, and  INRIA Holidays.","sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
671,671,10_paper_45,10_paper_30,1121,1141,1207.058,1206.2944,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses GP EI for Image Classification."
672,672,10_paper_45,10_paper_61,1121,1142,1207.058,1610.02915,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
673,673,10_paper_45,10_paper_46,1121,1143,1207.058,1509.08985,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN,  MNIST, and  CIFAR-100.","sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
674,674,10_paper_45,10_paper_55,1121,1147,1207.058,1512.03385,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
675,675,10_paper_45,10_paper_31,1121,1149,1207.058,1407.3068,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
676,676,10_paper_45,10_paper_56,1121,1151,1207.058,1707.02968,Image Classification,"sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
677,677,10_paper_45,10_paper_52,1121,1157,1207.058,1904.01169,Image Classification,"sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses multiple datasets:  ImageNet, and  CIFAR-100.","sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
678,678,10_paper_45,10_paper_22,1121,1159,1207.058,1605.07648,Image Classification,"sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses SVHN dataset.","sent: Article 1 uses Improving neural networks by preventing co-adaptation of feature detectors approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
679,679,10_paper_7,10_paper_3,1122,1123,1605.07146,1604.04112,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses SVHN dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification."
680,680,10_paper_7,10_paper_44,1122,1124,1605.07146,1603.05027,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses SVHN dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
681,681,10_paper_7,10_paper_59,1122,1125,1605.07146,1406.3332,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  CIFAR-100, while Article 2 uses multiple datasets:  STL-10, and  MNIST.","sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses CKN for Image Classification."
682,682,10_paper_7,10_paper_16,1122,1126,1605.07146,1707.01629,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses DPN-131 for Image Classification."
683,683,10_paper_7,10_paper_11,1122,1127,1605.07146,1409.5185,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses MNIST dataset.","sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses DSN for Image Classification."
684,684,10_paper_7,10_paper_19,1122,1129,1605.07146,1301.3557,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
685,685,10_paper_7,10_paper_20,1122,1130,1605.07146,1511.07289,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses SVHN dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
686,686,10_paper_7,10_paper_25,1122,1133,1605.07146,1503.04596,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses MNIST dataset.","sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
687,687,10_paper_7,10_paper_0,1122,1135,1605.07146,1611.05431,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
688,688,10_paper_7,10_paper_39,1122,1137,1605.07146,1902.05509,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  INRIA Holidays.","sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
689,689,10_paper_7,10_paper_30,1122,1141,1605.07146,1206.2944,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  CIFAR-100, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses GP EI for Image Classification."
690,690,10_paper_7,10_paper_61,1122,1142,1605.07146,1610.02915,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  CIFAR-100, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
691,691,10_paper_7,10_paper_46,1122,1143,1605.07146,1509.08985,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses MNIST dataset.","sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
692,692,10_paper_7,10_paper_55,1122,1147,1605.07146,1512.03385,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  CIFAR-100, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
693,693,10_paper_7,10_paper_31,1122,1149,1605.07146,1407.3068,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses SVHN dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
694,694,10_paper_7,10_paper_56,1122,1151,1605.07146,1707.02968,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
695,695,10_paper_7,10_paper_52,1122,1157,1605.07146,1904.01169,Image Classification,"sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses multiple datasets:  CIFAR-10, and  SVHN, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
696,696,10_paper_7,10_paper_22,1122,1159,1605.07146,1605.07648,Image Classification,"sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Wide ResNet approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
697,697,10_paper_44,10_paper_59,1124,1125,1603.05027,1406.3332,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  STL-10, and  MNIST.","sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses CKN for Image Classification."
698,698,10_paper_44,10_paper_16,1124,1126,1603.05027,1707.01629,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses ImageNet dataset.","sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses DPN-131 for Image Classification."
699,699,10_paper_44,10_paper_11,1124,1127,1603.05027,1409.5185,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN, and  MNIST.","sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses DSN for Image Classification."
700,700,10_paper_44,10_paper_6,1124,1128,1603.05027,1605.07146,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses SVHN dataset.","sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
701,701,10_paper_44,10_paper_19,1124,1129,1603.05027,1301.3557,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses SVHN dataset.","sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
702,702,10_paper_44,10_paper_20,1124,1130,1603.05027,1511.07289,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
703,703,10_paper_44,10_paper_25,1124,1133,1603.05027,1503.04596,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  SVHN, and  MNIST.","sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
704,704,10_paper_44,10_paper_0,1124,1135,1603.05027,1611.05431,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses ImageNet dataset.","sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
705,705,10_paper_44,10_paper_39,1124,1137,1603.05027,1902.05509,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  INRIA Holidays.","sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
706,706,10_paper_44,10_paper_30,1124,1141,1603.05027,1206.2944,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses GP EI for Image Classification."
707,707,10_paper_44,10_paper_61,1124,1142,1603.05027,1610.02915,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
708,708,10_paper_44,10_paper_46,1124,1143,1603.05027,1509.08985,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN, and  MNIST.","sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
709,709,10_paper_44,10_paper_55,1124,1147,1603.05027,1512.03385,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
710,710,10_paper_44,10_paper_31,1124,1149,1603.05027,1407.3068,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
711,711,10_paper_44,10_paper_56,1124,1151,1603.05027,1707.02968,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
712,712,10_paper_44,10_paper_52,1124,1157,1603.05027,1904.01169,Image Classification,"sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses ImageNet dataset.","sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
713,713,10_paper_44,10_paper_22,1124,1159,1603.05027,1605.07648,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses SVHN dataset.","sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
714,714,10_paper_11,10_paper_6,1127,1128,1409.5185,1605.07146,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses MNIST dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
715,715,10_paper_11,10_paper_19,1127,1129,1409.5185,1301.3557,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses MNIST dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
716,716,10_paper_11,10_paper_20,1127,1130,1409.5185,1511.07289,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  MNIST, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
717,717,10_paper_11,10_paper_41,1127,1131,1409.5185,1603.05027,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  MNIST, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
718,718,10_paper_11,10_paper_25,1127,1133,1409.5185,1503.04596,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset.sent: Article 1 and Article 2 use MNIST dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
719,719,10_paper_11,10_paper_0,1127,1135,1409.5185,1611.05431,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN,  MNIST, and  CIFAR-100, while Article 2 uses ImageNet dataset.","sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
720,720,10_paper_11,10_paper_39,1127,1137,1409.5185,1902.05509,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN,  MNIST, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  INRIA Holidays.","sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
721,721,10_paper_11,10_paper_60,1127,1138,1409.5185,1406.3332,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use MNIST dataset. sent: Article 1 uses multiple datasets:  SVHN, and  CIFAR-100, while Article 2 uses STL-10 dataset.","sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses CKN for Image Classification."
722,722,10_paper_11,10_paper_30,1127,1141,1409.5185,1206.2944,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN,  MNIST, and  CIFAR-100, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses GP EI for Image Classification."
723,723,10_paper_11,10_paper_61,1127,1142,1409.5185,1610.02915,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN,  MNIST, and  CIFAR-100, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
724,724,10_paper_11,10_paper_46,1127,1143,1409.5185,1509.08985,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset.sent: Article 1 and Article 2 use MNIST dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
725,725,10_paper_11,10_paper_55,1127,1147,1409.5185,1512.03385,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN,  MNIST, and  CIFAR-100, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
726,726,10_paper_11,10_paper_31,1127,1149,1409.5185,1407.3068,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  MNIST, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
727,727,10_paper_11,10_paper_56,1127,1151,1409.5185,1707.02968,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN,  MNIST, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
728,728,10_paper_11,10_paper_52,1127,1157,1409.5185,1904.01169,Image Classification,"sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN, and  MNIST, while Article 2 uses ImageNet dataset.","sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
729,729,10_paper_11,10_paper_22,1127,1159,1409.5185,1605.07648,Image Classification,"sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 uses multiple datasets:  CIFAR-10,  MNIST, and  CIFAR-100, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DSN approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
730,730,10_paper_20,10_paper_41,1130,1131,1511.07289,1603.05027,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
731,731,10_paper_20,10_paper_12,1130,1132,1511.07289,1409.5185,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN, and  MNIST.","sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses DSN for Image Classification."
732,732,10_paper_20,10_paper_25,1130,1133,1511.07289,1503.04596,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  SVHN, and  MNIST.","sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
733,733,10_paper_20,10_paper_0,1130,1135,1511.07289,1611.05431,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
734,734,10_paper_20,10_paper_39,1130,1137,1511.07289,1902.05509,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  INRIA Holidays.","sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
735,735,10_paper_20,10_paper_60,1130,1138,1511.07289,1406.3332,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  STL-10, and  MNIST.","sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses CKN for Image Classification."
736,736,10_paper_20,10_paper_8,1130,1139,1511.07289,1605.07146,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses SVHN dataset.","sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
737,737,10_paper_20,10_paper_30,1130,1141,1511.07289,1206.2944,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses GP EI for Image Classification."
738,738,10_paper_20,10_paper_61,1130,1142,1511.07289,1610.02915,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
739,739,10_paper_20,10_paper_46,1130,1143,1511.07289,1509.08985,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN, and  MNIST.","sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
740,740,10_paper_20,10_paper_55,1130,1147,1511.07289,1512.03385,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
741,741,10_paper_20,10_paper_31,1130,1149,1511.07289,1407.3068,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
742,742,10_paper_20,10_paper_56,1130,1151,1511.07289,1707.02968,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
743,743,10_paper_20,10_paper_17,1130,1152,1511.07289,1301.3557,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses SVHN dataset.","sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
744,744,10_paper_20,10_paper_52,1130,1157,1511.07289,1904.01169,Image Classification,"sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
745,745,10_paper_20,10_paper_22,1130,1159,1511.07289,1605.07648,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses SVHN dataset.","sent: Article 1 uses Exponential Linear Units approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
746,746,10_paper_25,10_paper_14,1133,1134,1503.04596,1409.5185,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset.sent: Article 1 and Article 2 use MNIST dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses DSN for Image Classification."
747,747,10_paper_25,10_paper_0,1133,1135,1503.04596,1611.05431,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN, and  MNIST, while Article 2 uses ImageNet dataset.","sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
748,748,10_paper_25,10_paper_39,1133,1137,1503.04596,1902.05509,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN, and  MNIST, while Article 2 uses multiple datasets:  ImageNet, and  INRIA Holidays.","sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
749,749,10_paper_25,10_paper_60,1133,1138,1503.04596,1406.3332,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use MNIST dataset. sent: Article 1 uses SVHN dataset, while Article 2 uses STL-10 dataset.","sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses CKN for Image Classification."
750,750,10_paper_25,10_paper_8,1133,1139,1503.04596,1605.07146,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 uses MNIST dataset, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
751,751,10_paper_25,10_paper_30,1133,1141,1503.04596,1206.2944,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  MNIST, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses GP EI for Image Classification."
752,752,10_paper_25,10_paper_61,1133,1142,1503.04596,1610.02915,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  MNIST, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
753,753,10_paper_25,10_paper_46,1133,1143,1503.04596,1509.08985,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset.sent: Article 1 and Article 2 use MNIST dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
754,754,10_paper_25,10_paper_55,1133,1147,1503.04596,1512.03385,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  MNIST, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
755,755,10_paper_25,10_paper_31,1133,1149,1503.04596,1407.3068,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  MNIST, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
756,756,10_paper_25,10_paper_56,1133,1151,1503.04596,1707.02968,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN, and  MNIST, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
757,757,10_paper_25,10_paper_17,1133,1152,1503.04596,1301.3557,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 uses MNIST dataset, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
758,758,10_paper_25,10_paper_21,1133,1153,1503.04596,1511.07289,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  MNIST, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
759,759,10_paper_25,10_paper_52,1133,1157,1503.04596,1904.01169,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN, and  MNIST, while Article 2 uses multiple datasets:  ImageNet, and  CIFAR-100.","sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
760,760,10_paper_25,10_paper_22,1133,1159,1503.04596,1605.07648,Image Classification,"sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 uses multiple datasets:  CIFAR-10, and  MNIST, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
761,761,10_paper_25,10_paper_42,1133,1161,1503.04596,1603.05027,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  MNIST, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
762,762,10_paper_0,10_paper_24,1135,1136,1611.05431,1503.04596,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  MNIST.","sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
763,763,10_paper_0,10_paper_39,1135,1137,1611.05431,1902.05509,Image Classification,"sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses INRIA Holidays dataset.","sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
764,764,10_paper_0,10_paper_60,1135,1138,1611.05431,1406.3332,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  STL-10,  MNIST, and  CIFAR-10.","sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses CKN for Image Classification."
765,765,10_paper_0,10_paper_8,1135,1139,1611.05431,1605.07146,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100.","sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
766,766,10_paper_0,10_paper_13,1135,1140,1611.05431,1409.5185,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN,  MNIST, and  CIFAR-100.","sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses DSN for Image Classification."
767,767,10_paper_0,10_paper_30,1135,1141,1611.05431,1206.2944,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses GP EI for Image Classification."
768,768,10_paper_0,10_paper_61,1135,1142,1611.05431,1610.02915,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
769,769,10_paper_0,10_paper_46,1135,1143,1611.05431,1509.08985,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN,  MNIST, and  CIFAR-100.","sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
770,770,10_paper_0,10_paper_55,1135,1147,1611.05431,1512.03385,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10,  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
771,771,10_paper_0,10_paper_31,1135,1149,1611.05431,1407.3068,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
772,772,10_paper_0,10_paper_56,1135,1151,1611.05431,1707.02968,Image Classification,"sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses PASCAL VOC 2012 dataset.","sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
773,773,10_paper_0,10_paper_17,1135,1152,1611.05431,1301.3557,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100.","sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
774,774,10_paper_0,10_paper_21,1135,1153,1611.05431,1511.07289,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
775,775,10_paper_0,10_paper_52,1135,1157,1611.05431,1904.01169,Image Classification,"sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
776,776,10_paper_0,10_paper_22,1135,1159,1611.05431,1605.07648,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses SVHN dataset.","sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
777,777,10_paper_0,10_paper_42,1135,1161,1611.05431,1603.05027,Image Classification,"sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
778,778,10_paper_30,10_paper_61,1141,1142,1206.2944,1610.02915,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
779,779,10_paper_30,10_paper_46,1141,1143,1206.2944,1509.08985,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN,  MNIST, and  CIFAR-100.","sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
780,780,10_paper_30,10_paper_5,1141,1144,1206.2944,1605.07146,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN, and  CIFAR-100.","sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
781,781,10_paper_30,10_paper_23,1141,1146,1206.2944,1503.04596,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN, and  MNIST.","sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
782,782,10_paper_30,10_paper_55,1141,1147,1206.2944,1512.03385,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
783,783,10_paper_30,10_paper_40,1141,1148,1206.2944,1902.05509,Image Classification,"sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses multiple datasets:  ImageNet, and  INRIA Holidays.","sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
784,784,10_paper_30,10_paper_31,1141,1149,1206.2944,1407.3068,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
785,785,10_paper_30,10_paper_56,1141,1151,1206.2944,1707.02968,Image Classification,"sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
786,786,10_paper_30,10_paper_17,1141,1152,1206.2944,1301.3557,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN, and  CIFAR-100.","sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
787,787,10_paper_30,10_paper_21,1141,1153,1206.2944,1511.07289,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
788,788,10_paper_30,10_paper_1,1141,1156,1206.2944,1611.05431,Image Classification,"sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses ImageNet dataset.","sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
789,789,10_paper_30,10_paper_52,1141,1157,1206.2944,1904.01169,Image Classification,"sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses multiple datasets:  ImageNet, and  CIFAR-100.","sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
790,790,10_paper_30,10_paper_22,1141,1159,1206.2944,1605.07648,Image Classification,"sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses SVHN dataset.","sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
791,791,10_paper_30,10_paper_42,1141,1161,1206.2944,1603.05027,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
792,792,10_paper_61,10_paper_46,1142,1143,1610.02915,1509.08985,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN,  MNIST, and  CIFAR-100.","sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses Tree Max-Avg pooling for Image Classification."
793,793,10_paper_61,10_paper_5,1142,1144,1610.02915,1605.07146,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN, and  CIFAR-100.","sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses Wide ResNet for Image Classification."
794,794,10_paper_61,10_paper_23,1142,1146,1610.02915,1503.04596,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN, and  MNIST.","sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses FLSCNN for Image Classification."
795,795,10_paper_61,10_paper_55,1142,1147,1610.02915,1512.03385,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification."
796,796,10_paper_61,10_paper_40,1142,1148,1610.02915,1902.05509,Image Classification,"sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses multiple datasets:  ImageNet, and  INRIA Holidays.","sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
797,797,10_paper_61,10_paper_31,1142,1149,1610.02915,1407.3068,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses Deep Networks with Internal Selective Attention through Feedback Connections for Image Classification."
798,798,10_paper_61,10_paper_56,1142,1151,1610.02915,1707.02968,Image Classification,"sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
799,799,10_paper_61,10_paper_17,1142,1152,1610.02915,1301.3557,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN, and  CIFAR-100.","sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
800,800,10_paper_61,10_paper_21,1142,1153,1610.02915,1511.07289,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
801,801,10_paper_61,10_paper_1,1142,1156,1610.02915,1611.05431,Image Classification,"sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
802,802,10_paper_61,10_paper_52,1142,1157,1610.02915,1904.01169,Image Classification,"sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses multiple datasets:  ImageNet, and  CIFAR-100.","sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
803,803,10_paper_61,10_paper_22,1142,1159,1610.02915,1605.07648,Image Classification,"sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses SVHN dataset.","sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
804,804,10_paper_61,10_paper_42,1142,1161,1610.02915,1603.05027,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CIFAR-100 dataset.","sent: Article 1 uses Deep pyramidal residual network approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
805,805,10_paper_31,10_paper_62,1149,1150,1407.3068,1610.02915,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Deep Networks with Internal Selective Attention through Feedback Connections approach for Image Classification, while Article 2 uses Deep pyramidal residual network for Image Classification."
806,806,10_paper_31,10_paper_56,1149,1151,1407.3068,1707.02968,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.","sent: Article 1 uses Deep Networks with Internal Selective Attention through Feedback Connections approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses Deep Networks with Internal Selective Attention through Feedback Connections approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification."
807,807,10_paper_31,10_paper_17,1149,1152,1407.3068,1301.3557,Image Classification,"sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses SVHN dataset.","sent: Article 1 uses Deep Networks with Internal Selective Attention through Feedback Connections approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification."
808,808,10_paper_31,10_paper_21,1149,1153,1407.3068,1511.07289,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses Deep Networks with Internal Selective Attention through Feedback Connections approach for Image Classification, while Article 2 uses Exponential Linear Units for Image Classification."
809,809,10_paper_31,10_paper_34,1149,1154,1407.3068,1902.05509,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  INRIA Holidays.","sent: Article 1 uses Deep Networks with Internal Selective Attention through Feedback Connections approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses Deep Networks with Internal Selective Attention through Feedback Connections approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses Deep Networks with Internal Selective Attention through Feedback Connections approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses Deep Networks with Internal Selective Attention through Feedback Connections approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses Deep Networks with Internal Selective Attention through Feedback Connections approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses Deep Networks with Internal Selective Attention through Feedback Connections approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
810,810,10_paper_31,10_paper_1,1149,1156,1407.3068,1611.05431,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Deep Networks with Internal Selective Attention through Feedback Connections approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification."
811,811,10_paper_31,10_paper_52,1149,1157,1407.3068,1904.01169,Image Classification,"sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses ImageNet dataset.","sent: Article 1 uses Deep Networks with Internal Selective Attention through Feedback Connections approach for Image Classification, while Article 2 uses Res2Net-DLA-60 for Image Classification. sent: Article 1 uses Deep Networks with Internal Selective Attention through Feedback Connections approach for Image Classification, while Article 2 uses Res2NeXt-29 for Image Classification."
812,812,10_paper_31,10_paper_22,1149,1159,1407.3068,1605.07648,Image Classification,"sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses SVHN dataset.","sent: Article 1 uses Deep Networks with Internal Selective Attention through Feedback Connections approach for Image Classification, while Article 2 uses FractalNet for Image Classification."
813,813,10_paper_31,10_paper_42,1149,1161,1407.3068,1603.05027,Image Classification,sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.,"sent: Article 1 uses Deep Networks with Internal Selective Attention through Feedback Connections approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
814,814,10_paper_22,10_paper_38,1159,1160,1605.07648,1902.05509,Image Classification,"sent: Article 1 uses SVHN dataset, while Article 2 uses multiple datasets:  ImageNet, and  INRIA Holidays.","sent: Article 1 uses FractalNet approach for Image Classification, while Article 2 uses MultiGrain R50   500 for Image Classification. sent: Article 1 uses FractalNet approach for Image Classification, while Article 2 uses MultiGrain R50-AA-500 for Image Classification. sent: Article 1 uses FractalNet approach for Image Classification, while Article 2 uses MultiGrain R50   800 for Image Classification. sent: Article 1 uses FractalNet approach for Image Classification, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Classification. sent: Article 1 uses FractalNet approach for Image Classification, while Article 2 uses SENet154   MultiGrain p    450 for Image Classification. sent: Article 1 uses FractalNet approach for Image Classification, while Article 2 uses MultiGrain R50-AA-224 for Image Classification."
815,815,10_paper_22,10_paper_42,1159,1161,1605.07648,1603.05027,Image Classification,"sent: Article 1 uses SVHN dataset, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.","sent: Article 1 uses FractalNet approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification."
816,816,11_paper_3,11_paper_10,120,121,1505.00468,1512.02167,Visual Question Answering,"sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 multiple choice dataset.sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset. sent: Article 1 uses multiple datasets:  COCO Visual Question Answering  VQA  abstract images 1 0 open ended,  COCO Visual Question Answering  VQA  real images 2 0 open ended, and  COCO Visual Question Answering  VQA  abstract 1 0 multiple choice, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DLAIT approach for Visual Question Answering, while Article 2 uses iBOWIMG baseline for Visual Question Answering. sent: Article 1 uses LSTM   global features approach for Visual Question Answering, while Article 2 uses iBOWIMG baseline for Visual Question Answering. sent: Article 1 uses HDU-USYD-UNCC approach for Visual Question Answering, while Article 2 uses iBOWIMG baseline for Visual Question Answering. sent: Article 1 uses LSTM blind approach for Visual Question Answering, while Article 2 uses iBOWIMG baseline for Visual Question Answering. sent: Article 1 uses LSTM Q I approach for Visual Question Answering, while Article 2 uses iBOWIMG baseline for Visual Question Answering. sent: Article 1 uses Dualnet ensemble approach for Visual Question Answering, while Article 2 uses iBOWIMG baseline for Visual Question Answering."
817,817,11_paper_3,11_paper_19,120,122,1505.00468,1609.056,Visual Question Answering,"sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  abstract images 1 0 open ended dataset.sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  abstract 1 0 multiple choice dataset. sent: Article 1 uses multiple datasets:  COCO Visual Question Answering  VQA  real images 1 0 multiple choice,  COCO Visual Question Answering  VQA  real images 1 0 open ended, and  COCO Visual Question Answering  VQA  real images 2 0 open ended, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DLAIT approach for Visual Question Answering, while Article 2 uses Graph VQA for Visual Question Answering. sent: Article 1 uses LSTM   global features approach for Visual Question Answering, while Article 2 uses Graph VQA for Visual Question Answering. sent: Article 1 uses HDU-USYD-UNCC approach for Visual Question Answering, while Article 2 uses Graph VQA for Visual Question Answering. sent: Article 1 uses LSTM blind approach for Visual Question Answering, while Article 2 uses Graph VQA for Visual Question Answering. sent: Article 1 uses LSTM Q I approach for Visual Question Answering, while Article 2 uses Graph VQA for Visual Question Answering. sent: Article 1 uses Dualnet ensemble approach for Visual Question Answering, while Article 2 uses Graph VQA for Visual Question Answering."
818,818,11_paper_3,11_paper_16,120,128,1505.00468,1807.09956,Visual Question Answering,"sent: Article 1 uses multiple datasets:  COCO Visual Question Answering  VQA  real images 2 0 open ended,  COCO Visual Question Answering  VQA  abstract 1 0 multiple choice,  COCO Visual Question Answering  VQA  abstract images 1 0 open ended,  COCO Visual Question Answering  VQA  real images 1 0 multiple choice, and  COCO Visual Question Answering  VQA  real images 1 0 open ended, while Article 2 uses VQA v2 dataset.","sent: Article 1 uses DLAIT approach for Visual Question Answering, while Article 2 uses Pythia v0 1 for Visual Question Answering. sent: Article 1 uses LSTM   global features approach for Visual Question Answering, while Article 2 uses Pythia v0 1 for Visual Question Answering. sent: Article 1 uses HDU-USYD-UNCC approach for Visual Question Answering, while Article 2 uses Pythia v0 1 for Visual Question Answering. sent: Article 1 uses LSTM blind approach for Visual Question Answering, while Article 2 uses Pythia v0 1 for Visual Question Answering. sent: Article 1 uses LSTM Q I approach for Visual Question Answering, while Article 2 uses Pythia v0 1 for Visual Question Answering. sent: Article 1 uses Dualnet ensemble approach for Visual Question Answering, while Article 2 uses Pythia v0 1 for Visual Question Answering."
819,819,11_paper_3,11_paper_13,120,129,1505.00468,1511.02274,Visual Question Answering,"sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset. sent: Article 1 uses multiple datasets:  COCO Visual Question Answering  VQA  real images 1 0 multiple choice,  COCO Visual Question Answering  VQA  abstract images 1 0 open ended,  COCO Visual Question Answering  VQA  real images 2 0 open ended, and  COCO Visual Question Answering  VQA  abstract 1 0 multiple choice, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DLAIT approach for Visual Question Answering, while Article 2 uses SAN for Visual Question Answering. sent: Article 1 uses LSTM   global features approach for Visual Question Answering, while Article 2 uses SAN for Visual Question Answering. sent: Article 1 uses HDU-USYD-UNCC approach for Visual Question Answering, while Article 2 uses SAN for Visual Question Answering. sent: Article 1 uses LSTM blind approach for Visual Question Answering, while Article 2 uses SAN for Visual Question Answering. sent: Article 1 uses LSTM Q I approach for Visual Question Answering, while Article 2 uses SAN for Visual Question Answering. sent: Article 1 uses Dualnet ensemble approach for Visual Question Answering, while Article 2 uses SAN for Visual Question Answering."
820,820,11_paper_3,11_paper_12,120,1210,1505.00468,1511.05234,Visual Question Answering,"sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset. sent: Article 1 uses multiple datasets:  COCO Visual Question Answering  VQA  real images 1 0 multiple choice,  COCO Visual Question Answering  VQA  abstract images 1 0 open ended,  COCO Visual Question Answering  VQA  real images 2 0 open ended, and  COCO Visual Question Answering  VQA  abstract 1 0 multiple choice, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DLAIT approach for Visual Question Answering, while Article 2 uses SMem-VQA for Visual Question Answering. sent: Article 1 uses LSTM   global features approach for Visual Question Answering, while Article 2 uses SMem-VQA for Visual Question Answering. sent: Article 1 uses HDU-USYD-UNCC approach for Visual Question Answering, while Article 2 uses SMem-VQA for Visual Question Answering. sent: Article 1 uses LSTM blind approach for Visual Question Answering, while Article 2 uses SMem-VQA for Visual Question Answering. sent: Article 1 uses LSTM Q I approach for Visual Question Answering, while Article 2 uses SMem-VQA for Visual Question Answering. sent: Article 1 uses Dualnet ensemble approach for Visual Question Answering, while Article 2 uses SMem-VQA for Visual Question Answering."
821,821,11_paper_3,11_paper_15,120,1212,1505.00468,1606.01455,Visual Question Answering,"sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 multiple choice dataset.sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset. sent: Article 1 uses multiple datasets:  COCO Visual Question Answering  VQA  abstract images 1 0 open ended,  COCO Visual Question Answering  VQA  real images 2 0 open ended, and  COCO Visual Question Answering  VQA  abstract 1 0 multiple choice, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DLAIT approach for Visual Question Answering, while Article 2 uses MRN   global features for Visual Question Answering. sent: Article 1 uses DLAIT approach for Visual Question Answering, while Article 2 uses MRN for Visual Question Answering. sent: Article 1 uses LSTM   global features approach for Visual Question Answering, while Article 2 uses MRN   global features for Visual Question Answering. sent: Article 1 uses LSTM   global features approach for Visual Question Answering, while Article 2 uses MRN for Visual Question Answering. sent: Article 1 uses HDU-USYD-UNCC approach for Visual Question Answering, while Article 2 uses MRN   global features for Visual Question Answering. sent: Article 1 uses HDU-USYD-UNCC approach for Visual Question Answering, while Article 2 uses MRN for Visual Question Answering. sent: Article 1 uses LSTM blind approach for Visual Question Answering, while Article 2 uses MRN   global features for Visual Question Answering. sent: Article 1 uses LSTM blind approach for Visual Question Answering, while Article 2 uses MRN for Visual Question Answering. sent: Article 1 uses LSTM Q I approach for Visual Question Answering, while Article 2 uses MRN   global features for Visual Question Answering. sent: Article 1 uses LSTM Q I approach for Visual Question Answering, while Article 2 uses MRN for Visual Question Answering. sent: Article 1 uses Dualnet ensemble approach for Visual Question Answering, while Article 2 uses MRN   global features for Visual Question Answering. sent: Article 1 uses Dualnet ensemble approach for Visual Question Answering, while Article 2 uses MRN for Visual Question Answering."
822,822,11_paper_3,11_paper_17,120,1217,1505.00468,1603.02814,Visual Question Answering,"sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset. sent: Article 1 uses multiple datasets:  COCO Visual Question Answering  VQA  real images 1 0 multiple choice,  COCO Visual Question Answering  VQA  abstract images 1 0 open ended,  COCO Visual Question Answering  VQA  real images 2 0 open ended, and  COCO Visual Question Answering  VQA  abstract 1 0 multiple choice, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DLAIT approach for Visual Question Answering, while Article 2 uses CNN-RNN for Visual Question Answering. sent: Article 1 uses LSTM   global features approach for Visual Question Answering, while Article 2 uses CNN-RNN for Visual Question Answering. sent: Article 1 uses HDU-USYD-UNCC approach for Visual Question Answering, while Article 2 uses CNN-RNN for Visual Question Answering. sent: Article 1 uses LSTM blind approach for Visual Question Answering, while Article 2 uses CNN-RNN for Visual Question Answering. sent: Article 1 uses LSTM Q I approach for Visual Question Answering, while Article 2 uses CNN-RNN for Visual Question Answering. sent: Article 1 uses Dualnet ensemble approach for Visual Question Answering, while Article 2 uses CNN-RNN for Visual Question Answering."
823,823,11_paper_10,11_paper_19,121,122,1512.02167,1609.056,Visual Question Answering,"sent: Article 1 uses multiple datasets:  COCO Visual Question Answering  VQA  real images 1 0 multiple choice, and  COCO Visual Question Answering  VQA  real images 1 0 open ended, while Article 2 uses multiple datasets:  COCO Visual Question Answering  VQA  abstract images 1 0 open ended, and  COCO Visual Question Answering  VQA  abstract 1 0 multiple choice.","sent: Article 1 uses iBOWIMG baseline approach for Visual Question Answering, while Article 2 uses Graph VQA for Visual Question Answering."
824,824,11_paper_10,11_paper_7,121,123,1512.02167,1505.00468,Visual Question Answering,"sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 multiple choice dataset.sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  COCO Visual Question Answering  VQA  abstract images 1 0 open ended,  COCO Visual Question Answering  VQA  real images 2 0 open ended, and  COCO Visual Question Answering  VQA  abstract 1 0 multiple choice.","sent: Article 1 uses iBOWIMG baseline approach for Visual Question Answering, while Article 2 uses DLAIT for Visual Question Answering. sent: Article 1 uses iBOWIMG baseline approach for Visual Question Answering, while Article 2 uses LSTM   global features for Visual Question Answering. sent: Article 1 uses iBOWIMG baseline approach for Visual Question Answering, while Article 2 uses HDU-USYD-UNCC for Visual Question Answering. sent: Article 1 uses iBOWIMG baseline approach for Visual Question Answering, while Article 2 uses LSTM blind for Visual Question Answering. sent: Article 1 uses iBOWIMG baseline approach for Visual Question Answering, while Article 2 uses LSTM Q I for Visual Question Answering. sent: Article 1 uses iBOWIMG baseline approach for Visual Question Answering, while Article 2 uses Dualnet ensemble for Visual Question Answering."
825,825,11_paper_10,11_paper_16,121,128,1512.02167,1807.09956,Visual Question Answering,"sent: Article 1 uses multiple datasets:  COCO Visual Question Answering  VQA  real images 1 0 multiple choice, and  COCO Visual Question Answering  VQA  real images 1 0 open ended, while Article 2 uses VQA v2 dataset.","sent: Article 1 uses iBOWIMG baseline approach for Visual Question Answering, while Article 2 uses Pythia v0 1 for Visual Question Answering."
826,826,11_paper_10,11_paper_13,121,129,1512.02167,1511.02274,Visual Question Answering,"sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset. sent: Article 1 uses COCO Visual Question Answering  VQA  real images 1 0 multiple choice dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses iBOWIMG baseline approach for Visual Question Answering, while Article 2 uses SAN for Visual Question Answering."
827,827,11_paper_10,11_paper_12,121,1210,1512.02167,1511.05234,Visual Question Answering,"sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset. sent: Article 1 uses COCO Visual Question Answering  VQA  real images 1 0 multiple choice dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses iBOWIMG baseline approach for Visual Question Answering, while Article 2 uses SMem-VQA for Visual Question Answering."
828,828,11_paper_10,11_paper_15,121,1212,1512.02167,1606.01455,Visual Question Answering,sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 multiple choice dataset.sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset.,"sent: Article 1 uses iBOWIMG baseline approach for Visual Question Answering, while Article 2 uses MRN   global features for Visual Question Answering. sent: Article 1 uses iBOWIMG baseline approach for Visual Question Answering, while Article 2 uses MRN for Visual Question Answering."
829,829,11_paper_10,11_paper_17,121,1217,1512.02167,1603.02814,Visual Question Answering,"sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset. sent: Article 1 uses COCO Visual Question Answering  VQA  real images 1 0 multiple choice dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses iBOWIMG baseline approach for Visual Question Answering, while Article 2 uses CNN-RNN for Visual Question Answering."
830,830,11_paper_19,11_paper_7,122,123,1609.056,1505.00468,Visual Question Answering,"sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  abstract images 1 0 open ended dataset.sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  abstract 1 0 multiple choice dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  COCO Visual Question Answering  VQA  real images 1 0 multiple choice,  COCO Visual Question Answering  VQA  real images 1 0 open ended, and  COCO Visual Question Answering  VQA  real images 2 0 open ended.","sent: Article 1 uses Graph VQA approach for Visual Question Answering, while Article 2 uses DLAIT for Visual Question Answering. sent: Article 1 uses Graph VQA approach for Visual Question Answering, while Article 2 uses LSTM   global features for Visual Question Answering. sent: Article 1 uses Graph VQA approach for Visual Question Answering, while Article 2 uses HDU-USYD-UNCC for Visual Question Answering. sent: Article 1 uses Graph VQA approach for Visual Question Answering, while Article 2 uses LSTM blind for Visual Question Answering. sent: Article 1 uses Graph VQA approach for Visual Question Answering, while Article 2 uses LSTM Q I for Visual Question Answering. sent: Article 1 uses Graph VQA approach for Visual Question Answering, while Article 2 uses Dualnet ensemble for Visual Question Answering."
831,831,11_paper_19,11_paper_11,122,126,1609.056,1512.02167,Visual Question Answering,"sent: Article 1 uses multiple datasets:  COCO Visual Question Answering  VQA  abstract images 1 0 open ended, and  COCO Visual Question Answering  VQA  abstract 1 0 multiple choice, while Article 2 uses multiple datasets:  COCO Visual Question Answering  VQA  real images 1 0 multiple choice, and  COCO Visual Question Answering  VQA  real images 1 0 open ended.","sent: Article 1 uses Graph VQA approach for Visual Question Answering, while Article 2 uses iBOWIMG baseline for Visual Question Answering."
832,832,11_paper_19,11_paper_16,122,128,1609.056,1807.09956,Visual Question Answering,"sent: Article 1 uses multiple datasets:  COCO Visual Question Answering  VQA  abstract images 1 0 open ended, and  COCO Visual Question Answering  VQA  abstract 1 0 multiple choice, while Article 2 uses VQA v2 dataset.","sent: Article 1 uses Graph VQA approach for Visual Question Answering, while Article 2 uses Pythia v0 1 for Visual Question Answering."
833,833,11_paper_19,11_paper_13,122,129,1609.056,1511.02274,Visual Question Answering,"sent: Article 1 uses multiple datasets:  COCO Visual Question Answering  VQA  abstract images 1 0 open ended, and  COCO Visual Question Answering  VQA  abstract 1 0 multiple choice, while Article 2 uses COCO Visual Question Answering  VQA  real images 1 0 open ended dataset.","sent: Article 1 uses Graph VQA approach for Visual Question Answering, while Article 2 uses SAN for Visual Question Answering."
834,834,11_paper_19,11_paper_12,122,1210,1609.056,1511.05234,Visual Question Answering,"sent: Article 1 uses multiple datasets:  COCO Visual Question Answering  VQA  abstract images 1 0 open ended, and  COCO Visual Question Answering  VQA  abstract 1 0 multiple choice, while Article 2 uses COCO Visual Question Answering  VQA  real images 1 0 open ended dataset.","sent: Article 1 uses Graph VQA approach for Visual Question Answering, while Article 2 uses SMem-VQA for Visual Question Answering."
835,835,11_paper_19,11_paper_15,122,1212,1609.056,1606.01455,Visual Question Answering,"sent: Article 1 uses multiple datasets:  COCO Visual Question Answering  VQA  abstract images 1 0 open ended, and  COCO Visual Question Answering  VQA  abstract 1 0 multiple choice, while Article 2 uses multiple datasets:  COCO Visual Question Answering  VQA  real images 1 0 multiple choice, and  COCO Visual Question Answering  VQA  real images 1 0 open ended.","sent: Article 1 uses Graph VQA approach for Visual Question Answering, while Article 2 uses MRN   global features for Visual Question Answering. sent: Article 1 uses Graph VQA approach for Visual Question Answering, while Article 2 uses MRN for Visual Question Answering."
836,836,11_paper_19,11_paper_17,122,1217,1609.056,1603.02814,Visual Question Answering,"sent: Article 1 uses multiple datasets:  COCO Visual Question Answering  VQA  abstract images 1 0 open ended, and  COCO Visual Question Answering  VQA  abstract 1 0 multiple choice, while Article 2 uses COCO Visual Question Answering  VQA  real images 1 0 open ended dataset.","sent: Article 1 uses Graph VQA approach for Visual Question Answering, while Article 2 uses CNN-RNN for Visual Question Answering."
837,837,11_paper_16,11_paper_13,128,129,1807.09956,1511.02274,Visual Question Answering,"sent: Article 1 uses VQA v2 dataset, while Article 2 uses COCO Visual Question Answering  VQA  real images 1 0 open ended dataset.","sent: Article 1 uses Pythia v0 1 approach for Visual Question Answering, while Article 2 uses SAN for Visual Question Answering."
838,838,11_paper_16,11_paper_12,128,1210,1807.09956,1511.05234,Visual Question Answering,"sent: Article 1 uses VQA v2 dataset, while Article 2 uses COCO Visual Question Answering  VQA  real images 1 0 open ended dataset.","sent: Article 1 uses Pythia v0 1 approach for Visual Question Answering, while Article 2 uses SMem-VQA for Visual Question Answering."
839,839,11_paper_16,11_paper_1,128,1211,1807.09956,1505.00468,Visual Question Answering,"sent: Article 1 uses VQA v2 dataset, while Article 2 uses multiple datasets:  COCO Visual Question Answering  VQA  real images 2 0 open ended,  COCO Visual Question Answering  VQA  abstract 1 0 multiple choice,  COCO Visual Question Answering  VQA  abstract images 1 0 open ended,  COCO Visual Question Answering  VQA  real images 1 0 multiple choice, and  COCO Visual Question Answering  VQA  real images 1 0 open ended.","sent: Article 1 uses Pythia v0 1 approach for Visual Question Answering, while Article 2 uses DLAIT for Visual Question Answering. sent: Article 1 uses Pythia v0 1 approach for Visual Question Answering, while Article 2 uses LSTM   global features for Visual Question Answering. sent: Article 1 uses Pythia v0 1 approach for Visual Question Answering, while Article 2 uses HDU-USYD-UNCC for Visual Question Answering. sent: Article 1 uses Pythia v0 1 approach for Visual Question Answering, while Article 2 uses LSTM blind for Visual Question Answering. sent: Article 1 uses Pythia v0 1 approach for Visual Question Answering, while Article 2 uses LSTM Q I for Visual Question Answering. sent: Article 1 uses Pythia v0 1 approach for Visual Question Answering, while Article 2 uses Dualnet ensemble for Visual Question Answering."
840,840,11_paper_16,11_paper_15,128,1212,1807.09956,1606.01455,Visual Question Answering,"sent: Article 1 uses VQA v2 dataset, while Article 2 uses multiple datasets:  COCO Visual Question Answering  VQA  real images 1 0 multiple choice, and  COCO Visual Question Answering  VQA  real images 1 0 open ended.","sent: Article 1 uses Pythia v0 1 approach for Visual Question Answering, while Article 2 uses MRN   global features for Visual Question Answering. sent: Article 1 uses Pythia v0 1 approach for Visual Question Answering, while Article 2 uses MRN for Visual Question Answering."
841,841,11_paper_16,11_paper_17,128,1217,1807.09956,1603.02814,Visual Question Answering,"sent: Article 1 uses VQA v2 dataset, while Article 2 uses COCO Visual Question Answering  VQA  real images 1 0 open ended dataset.","sent: Article 1 uses Pythia v0 1 approach for Visual Question Answering, while Article 2 uses CNN-RNN for Visual Question Answering."
842,842,11_paper_13,11_paper_12,129,1210,1511.02274,1511.05234,Visual Question Answering,sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset.,"sent: Article 1 uses SAN approach for Visual Question Answering, while Article 2 uses SMem-VQA for Visual Question Answering."
843,843,11_paper_13,11_paper_1,129,1211,1511.02274,1505.00468,Visual Question Answering,"sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  COCO Visual Question Answering  VQA  real images 1 0 multiple choice,  COCO Visual Question Answering  VQA  abstract images 1 0 open ended,  COCO Visual Question Answering  VQA  real images 2 0 open ended, and  COCO Visual Question Answering  VQA  abstract 1 0 multiple choice.","sent: Article 1 uses SAN approach for Visual Question Answering, while Article 2 uses DLAIT for Visual Question Answering. sent: Article 1 uses SAN approach for Visual Question Answering, while Article 2 uses LSTM   global features for Visual Question Answering. sent: Article 1 uses SAN approach for Visual Question Answering, while Article 2 uses HDU-USYD-UNCC for Visual Question Answering. sent: Article 1 uses SAN approach for Visual Question Answering, while Article 2 uses LSTM blind for Visual Question Answering. sent: Article 1 uses SAN approach for Visual Question Answering, while Article 2 uses LSTM Q I for Visual Question Answering. sent: Article 1 uses SAN approach for Visual Question Answering, while Article 2 uses Dualnet ensemble for Visual Question Answering."
844,844,11_paper_13,11_paper_15,129,1212,1511.02274,1606.01455,Visual Question Answering,"sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses COCO Visual Question Answering  VQA  real images 1 0 multiple choice dataset.","sent: Article 1 uses SAN approach for Visual Question Answering, while Article 2 uses MRN   global features for Visual Question Answering. sent: Article 1 uses SAN approach for Visual Question Answering, while Article 2 uses MRN for Visual Question Answering."
845,845,11_paper_13,11_paper_17,129,1217,1511.02274,1603.02814,Visual Question Answering,sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset.,"sent: Article 1 uses SAN approach for Visual Question Answering, while Article 2 uses CNN-RNN for Visual Question Answering."
846,846,11_paper_12,11_paper_1,1210,1211,1511.05234,1505.00468,Visual Question Answering,"sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  COCO Visual Question Answering  VQA  real images 1 0 multiple choice,  COCO Visual Question Answering  VQA  abstract images 1 0 open ended,  COCO Visual Question Answering  VQA  real images 2 0 open ended, and  COCO Visual Question Answering  VQA  abstract 1 0 multiple choice.","sent: Article 1 uses SMem-VQA approach for Visual Question Answering, while Article 2 uses DLAIT for Visual Question Answering. sent: Article 1 uses SMem-VQA approach for Visual Question Answering, while Article 2 uses LSTM   global features for Visual Question Answering. sent: Article 1 uses SMem-VQA approach for Visual Question Answering, while Article 2 uses HDU-USYD-UNCC for Visual Question Answering. sent: Article 1 uses SMem-VQA approach for Visual Question Answering, while Article 2 uses LSTM blind for Visual Question Answering. sent: Article 1 uses SMem-VQA approach for Visual Question Answering, while Article 2 uses LSTM Q I for Visual Question Answering. sent: Article 1 uses SMem-VQA approach for Visual Question Answering, while Article 2 uses Dualnet ensemble for Visual Question Answering."
847,847,11_paper_12,11_paper_15,1210,1212,1511.05234,1606.01455,Visual Question Answering,"sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses COCO Visual Question Answering  VQA  real images 1 0 multiple choice dataset.","sent: Article 1 uses SMem-VQA approach for Visual Question Answering, while Article 2 uses MRN   global features for Visual Question Answering. sent: Article 1 uses SMem-VQA approach for Visual Question Answering, while Article 2 uses MRN for Visual Question Answering."
848,848,11_paper_12,11_paper_17,1210,1217,1511.05234,1603.02814,Visual Question Answering,sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset.,"sent: Article 1 uses SMem-VQA approach for Visual Question Answering, while Article 2 uses CNN-RNN for Visual Question Answering."
849,849,11_paper_15,11_paper_5,1212,1213,1606.01455,1505.00468,Visual Question Answering,"sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 multiple choice dataset.sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  COCO Visual Question Answering  VQA  abstract images 1 0 open ended,  COCO Visual Question Answering  VQA  real images 2 0 open ended, and  COCO Visual Question Answering  VQA  abstract 1 0 multiple choice.","sent: Article 1 uses MRN   global features approach for Visual Question Answering, while Article 2 uses DLAIT for Visual Question Answering. sent: Article 1 uses MRN   global features approach for Visual Question Answering, while Article 2 uses LSTM   global features for Visual Question Answering. sent: Article 1 uses MRN   global features approach for Visual Question Answering, while Article 2 uses HDU-USYD-UNCC for Visual Question Answering. sent: Article 1 uses MRN   global features approach for Visual Question Answering, while Article 2 uses LSTM blind for Visual Question Answering. sent: Article 1 uses MRN   global features approach for Visual Question Answering, while Article 2 uses LSTM Q I for Visual Question Answering. sent: Article 1 uses MRN   global features approach for Visual Question Answering, while Article 2 uses Dualnet ensemble for Visual Question Answering. sent: Article 1 uses MRN approach for Visual Question Answering, while Article 2 uses DLAIT for Visual Question Answering. sent: Article 1 uses MRN approach for Visual Question Answering, while Article 2 uses LSTM   global features for Visual Question Answering. sent: Article 1 uses MRN approach for Visual Question Answering, while Article 2 uses HDU-USYD-UNCC for Visual Question Answering. sent: Article 1 uses MRN approach for Visual Question Answering, while Article 2 uses LSTM blind for Visual Question Answering. sent: Article 1 uses MRN approach for Visual Question Answering, while Article 2 uses LSTM Q I for Visual Question Answering. sent: Article 1 uses MRN approach for Visual Question Answering, while Article 2 uses Dualnet ensemble for Visual Question Answering."
850,850,11_paper_15,11_paper_17,1212,1217,1606.01455,1603.02814,Visual Question Answering,"sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset. sent: Article 1 uses COCO Visual Question Answering  VQA  real images 1 0 multiple choice dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses MRN   global features approach for Visual Question Answering, while Article 2 uses CNN-RNN for Visual Question Answering. sent: Article 1 uses MRN approach for Visual Question Answering, while Article 2 uses CNN-RNN for Visual Question Answering."
851,851,11_paper_17,11_paper_8,1217,1218,1603.02814,1505.00468,Visual Question Answering,"sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  COCO Visual Question Answering  VQA  real images 1 0 multiple choice,  COCO Visual Question Answering  VQA  abstract images 1 0 open ended,  COCO Visual Question Answering  VQA  real images 2 0 open ended, and  COCO Visual Question Answering  VQA  abstract 1 0 multiple choice.","sent: Article 1 uses CNN-RNN approach for Visual Question Answering, while Article 2 uses DLAIT for Visual Question Answering. sent: Article 1 uses CNN-RNN approach for Visual Question Answering, while Article 2 uses LSTM   global features for Visual Question Answering. sent: Article 1 uses CNN-RNN approach for Visual Question Answering, while Article 2 uses HDU-USYD-UNCC for Visual Question Answering. sent: Article 1 uses CNN-RNN approach for Visual Question Answering, while Article 2 uses LSTM blind for Visual Question Answering. sent: Article 1 uses CNN-RNN approach for Visual Question Answering, while Article 2 uses LSTM Q I for Visual Question Answering. sent: Article 1 uses CNN-RNN approach for Visual Question Answering, while Article 2 uses Dualnet ensemble for Visual Question Answering."
852,852,11_paper_17,11_paper_14,1217,1219,1603.02814,1606.01455,Visual Question Answering,"sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses COCO Visual Question Answering  VQA  real images 1 0 multiple choice dataset.","sent: Article 1 uses CNN-RNN approach for Visual Question Answering, while Article 2 uses MRN   global features for Visual Question Answering. sent: Article 1 uses CNN-RNN approach for Visual Question Answering, while Article 2 uses MRN for Visual Question Answering."
853,853,12_paper_5,12_paper_0,130,133,1809.11096,1612.04357,Conditional Image Generation,"sent: Article 1 uses ImageNet 128x128 dataset, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses BigGAN-deep approach for Conditional Image Generation, while Article 2 uses SGAN for Conditional Image Generation. sent: Article 1 uses BigGAN approach for Conditional Image Generation, while Article 2 uses SGAN for Conditional Image Generation."
854,854,12_paper_5,12_paper_1,130,135,1809.11096,1611.01722,Conditional Image Generation,"sent: Article 1 uses ImageNet 128x128 dataset, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses BigGAN-deep approach for Conditional Image Generation, while Article 2 uses SteinGAN for Conditional Image Generation. sent: Article 1 uses BigGAN approach for Conditional Image Generation, while Article 2 uses SteinGAN for Conditional Image Generation."
855,855,12_paper_0,12_paper_4,133,134,1612.04357,1809.11096,Conditional Image Generation,"sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses ImageNet 128x128 dataset.","sent: Article 1 uses SGAN approach for Conditional Image Generation, while Article 2 uses BigGAN-deep for Conditional Image Generation. sent: Article 1 uses SGAN approach for Conditional Image Generation, while Article 2 uses BigGAN for Conditional Image Generation."
856,856,12_paper_0,12_paper_1,133,135,1612.04357,1611.01722,Conditional Image Generation,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 uses SGAN approach for Conditional Image Generation, while Article 2 uses SteinGAN for Conditional Image Generation."
857,857,13_paper_4,13_paper_0,140,141,1711.07399,1803.02188,Pose Estimation,"sent: Article 1 uses multiple datasets:   ITOP front-view,  MSRA Hands,  ICVL Hands,  NYU Hands,  HANDS 2017, and  ITOP top-view, while Article 2 uses DensePose-COCO dataset.","sent: Article 1 uses V2V-PoseNet approach for Pose Estimation, while Article 2 uses DensePose   keypoints for Pose Estimation."
858,858,13_paper_4,13_paper_1,140,142,1711.07399,1603.06937,Pose Estimation,"sent: Article 1 uses multiple datasets:   ITOP front-view,  MSRA Hands,  ICVL Hands,  NYU Hands,  HANDS 2017, and  ITOP top-view, while Article 2 uses multiple datasets:  MPII Human Pose,  FLIC Wrists, and  FLIC Elbows.","sent: Article 1 uses V2V-PoseNet approach for Pose Estimation, while Article 2 uses Stacked Hourglass Networks for Pose Estimation."
859,859,13_paper_4,13_paper_10,140,144,1711.07399,1708.01101,Pose Estimation,"sent: Article 1 uses multiple datasets:   ITOP front-view,  MSRA Hands,  ICVL Hands,  NYU Hands,  HANDS 2017, and  ITOP top-view, while Article 2 uses multiple datasets:  MPII Human Pose, and  Leeds Sports Poses.","sent: Article 1 uses V2V-PoseNet approach for Pose Estimation, while Article 2 uses Pyramid Residual Modules  PRMs  for Pose Estimation."
860,860,13_paper_4,13_paper_8,140,145,1711.07399,1511.06645,Pose Estimation,"sent: Article 1 uses multiple datasets:   ITOP front-view,  MSRA Hands,  ICVL Hands,  NYU Hands,  HANDS 2017, and  ITOP top-view, while Article 2 uses multiple datasets:  MPII Human Pose, and  WAF.","sent: Article 1 uses V2V-PoseNet approach for Pose Estimation, while Article 2 uses DeepCut for Pose Estimation."
861,861,13_paper_4,13_paper_9,140,148,1711.07399,1808.02194,Pose Estimation,"sent: Article 1 uses multiple datasets:   ITOP front-view,  MSRA Hands,  ICVL Hands,  NYU Hands,  HANDS 2017, and  ITOP top-view, while Article 2 uses MPII Human Pose dataset.","sent: Article 1 uses V2V-PoseNet approach for Pose Estimation, while Article 2 uses DU-Net for Pose Estimation."
862,862,13_paper_0,13_paper_1,141,142,1803.02188,1603.06937,Pose Estimation,"sent: Article 1 uses DensePose-COCO dataset, while Article 2 uses multiple datasets:  MPII Human Pose,  FLIC Wrists, and  FLIC Elbows.","sent: Article 1 uses DensePose   keypoints approach for Pose Estimation, while Article 2 uses Stacked Hourglass Networks for Pose Estimation."
863,863,13_paper_0,13_paper_10,141,144,1803.02188,1708.01101,Pose Estimation,"sent: Article 1 uses DensePose-COCO dataset, while Article 2 uses multiple datasets:  MPII Human Pose, and  Leeds Sports Poses.","sent: Article 1 uses DensePose   keypoints approach for Pose Estimation, while Article 2 uses Pyramid Residual Modules  PRMs  for Pose Estimation."
864,864,13_paper_0,13_paper_8,141,145,1803.02188,1511.06645,Pose Estimation,"sent: Article 1 uses DensePose-COCO dataset, while Article 2 uses multiple datasets:  MPII Human Pose, and  WAF.","sent: Article 1 uses DensePose   keypoints approach for Pose Estimation, while Article 2 uses DeepCut for Pose Estimation."
865,865,13_paper_0,13_paper_5,141,147,1803.02188,1711.07399,Pose Estimation,"sent: Article 1 uses DensePose-COCO dataset, while Article 2 uses multiple datasets:   ITOP front-view,  MSRA Hands,  ICVL Hands,  NYU Hands,  HANDS 2017, and  ITOP top-view.","sent: Article 1 uses DensePose   keypoints approach for Pose Estimation, while Article 2 uses V2V-PoseNet for Pose Estimation."
866,866,13_paper_0,13_paper_9,141,148,1803.02188,1808.02194,Pose Estimation,"sent: Article 1 uses DensePose-COCO dataset, while Article 2 uses MPII Human Pose dataset.","sent: Article 1 uses DensePose   keypoints approach for Pose Estimation, while Article 2 uses DU-Net for Pose Estimation."
867,867,13_paper_1,13_paper_10,142,144,1603.06937,1708.01101,Pose Estimation,"sent: Article 1 and Article 2 use MPII Human Pose dataset. sent: Article 1 uses multiple datasets:  FLIC Wrists, and  FLIC Elbows, while Article 2 uses Leeds Sports Poses dataset.","sent: Article 1 uses Stacked Hourglass Networks approach for Pose Estimation, while Article 2 uses Pyramid Residual Modules  PRMs  for Pose Estimation."
868,868,13_paper_1,13_paper_8,142,145,1603.06937,1511.06645,Pose Estimation,"sent: Article 1 and Article 2 use MPII Human Pose dataset. sent: Article 1 uses multiple datasets:  FLIC Wrists, and  FLIC Elbows, while Article 2 uses WAF dataset.","sent: Article 1 uses Stacked Hourglass Networks approach for Pose Estimation, while Article 2 uses DeepCut for Pose Estimation."
869,869,13_paper_1,13_paper_5,142,147,1603.06937,1711.07399,Pose Estimation,"sent: Article 1 uses multiple datasets:  MPII Human Pose,  FLIC Wrists, and  FLIC Elbows, while Article 2 uses multiple datasets:   ITOP front-view,  MSRA Hands,  ICVL Hands,  NYU Hands,  HANDS 2017, and  ITOP top-view.","sent: Article 1 uses Stacked Hourglass Networks approach for Pose Estimation, while Article 2 uses V2V-PoseNet for Pose Estimation."
870,870,13_paper_1,13_paper_9,142,148,1603.06937,1808.02194,Pose Estimation,"sent: Article 1 and Article 2 use MPII Human Pose dataset. sent: Article 1 uses multiple datasets:  FLIC Wrists, and  FLIC Elbows, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Stacked Hourglass Networks approach for Pose Estimation, while Article 2 uses DU-Net for Pose Estimation."
871,871,13_paper_10,13_paper_8,144,145,1708.01101,1511.06645,Pose Estimation,"sent: Article 1 and Article 2 use MPII Human Pose dataset. sent: Article 1 uses Leeds Sports Poses dataset, while Article 2 uses WAF dataset.","sent: Article 1 uses Pyramid Residual Modules  PRMs  approach for Pose Estimation, while Article 2 uses DeepCut for Pose Estimation."
872,872,13_paper_10,13_paper_5,144,147,1708.01101,1711.07399,Pose Estimation,"sent: Article 1 uses multiple datasets:  MPII Human Pose, and  Leeds Sports Poses, while Article 2 uses multiple datasets:   ITOP front-view,  MSRA Hands,  ICVL Hands,  NYU Hands,  HANDS 2017, and  ITOP top-view.","sent: Article 1 uses Pyramid Residual Modules  PRMs  approach for Pose Estimation, while Article 2 uses V2V-PoseNet for Pose Estimation."
873,873,13_paper_10,13_paper_9,144,148,1708.01101,1808.02194,Pose Estimation,"sent: Article 1 and Article 2 use MPII Human Pose dataset. sent: Article 1 uses Leeds Sports Poses dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Pyramid Residual Modules  PRMs  approach for Pose Estimation, while Article 2 uses DU-Net for Pose Estimation."
874,874,13_paper_10,13_paper_3,144,149,1708.01101,1603.06937,Pose Estimation,"sent: Article 1 and Article 2 use MPII Human Pose dataset. sent: Article 1 uses Leeds Sports Poses dataset, while Article 2 uses multiple datasets:  FLIC Wrists, and  FLIC Elbows.","sent: Article 1 uses Pyramid Residual Modules  PRMs  approach for Pose Estimation, while Article 2 uses Stacked Hourglass Networks for Pose Estimation."
875,875,13_paper_8,13_paper_11,145,146,1511.06645,1708.01101,Pose Estimation,"sent: Article 1 and Article 2 use MPII Human Pose dataset. sent: Article 1 uses WAF dataset, while Article 2 uses Leeds Sports Poses dataset.","sent: Article 1 uses DeepCut approach for Pose Estimation, while Article 2 uses Pyramid Residual Modules  PRMs  for Pose Estimation."
876,876,13_paper_8,13_paper_5,145,147,1511.06645,1711.07399,Pose Estimation,"sent: Article 1 uses multiple datasets:  MPII Human Pose, and  WAF, while Article 2 uses multiple datasets:   ITOP front-view,  MSRA Hands,  ICVL Hands,  NYU Hands,  HANDS 2017, and  ITOP top-view.","sent: Article 1 uses DeepCut approach for Pose Estimation, while Article 2 uses V2V-PoseNet for Pose Estimation."
877,877,13_paper_8,13_paper_9,145,148,1511.06645,1808.02194,Pose Estimation,"sent: Article 1 and Article 2 use MPII Human Pose dataset. sent: Article 1 uses WAF dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DeepCut approach for Pose Estimation, while Article 2 uses DU-Net for Pose Estimation."
878,878,13_paper_8,13_paper_3,145,149,1511.06645,1603.06937,Pose Estimation,"sent: Article 1 and Article 2 use MPII Human Pose dataset. sent: Article 1 uses WAF dataset, while Article 2 uses multiple datasets:  FLIC Wrists, and  FLIC Elbows.","sent: Article 1 uses DeepCut approach for Pose Estimation, while Article 2 uses Stacked Hourglass Networks for Pose Estimation."
879,879,13_paper_9,13_paper_3,148,149,1808.02194,1603.06937,Pose Estimation,"sent: Article 1 and Article 2 use MPII Human Pose dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  FLIC Wrists, and  FLIC Elbows.","sent: Article 1 uses DU-Net approach for Pose Estimation, while Article 2 uses Stacked Hourglass Networks for Pose Estimation."
880,880,14_paper_501,14_paper_79,150,153,1509.06461,1511.06581,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Fishing Derby dataset.sent: Article 1 and Article 2 use Atari 2600 Atlantis dataset.sent: Article 1 and Article 2 use Atari 2600 Tennis dataset.sent: Article 1 and Article 2 use Atari 2600 Video Pinball dataset.sent: Article 1 and Article 2 use Atari 2600 Up and Down dataset.sent: Article 1 and Article 2 use Atari 2600 Gopher dataset.sent: Article 1 and Article 2 use Atari 2600 Double Dunk dataset.sent: Article 1 and Article 2 use Atari 2600 Assault dataset.sent: Article 1 and Article 2 use Atari 2600 Road Runner dataset.sent: Article 1 and Article 2 use Atari 2600 Berzerk dataset.sent: Article 1 and Article 2 use Atari 2600 Private Eye dataset.sent: Article 1 and Article 2 use Atari 2600 Gravitar dataset.sent: Article 1 and Article 2 use Atari 2600 Breakout dataset.sent: Article 1 and Article 2 use Atari 2600 Krull dataset.sent: Article 1 and Article 2 use Atari 2600 HERO dataset.sent: Article 1 and Article 2 use Atari 2600 Ice Hockey dataset.sent: Article 1 and Article 2 use Atari 2600 Asterix dataset.sent: Article 1 and Article 2 use Atari 2600 Time Pilot dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Seaquest dataset.sent: Article 1 and Article 2 use Atari 2600 Name This Game dataset.sent: Article 1 and Article 2 use Atari 2600 Chopper Command dataset.sent: Article 1 and Article 2 use Atari 2600 Kangaroo dataset.sent: Article 1 and Article 2 use Atari 2600 Zaxxon dataset.sent: Article 1 and Article 2 use Atari 2600 Bowling dataset.sent: Article 1 and Article 2 use Atari 2600 Enduro dataset.sent: Article 1 and Article 2 use Atari 2600 Wizard of Wor dataset.sent: Article 1 and Article 2 use Atari 2600 Ms  Pacman dataset.sent: Article 1 and Article 2 use Atari 2600 Pong dataset.sent: Article 1 and Article 2 use Atari 2600 Star Gunner dataset.sent: Article 1 and Article 2 use Atari 2600 Space Invaders dataset.sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Bank Heist dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Alien dataset.sent: Article 1 and Article 2 use Atari 2600 Amidar dataset.sent: Article 1 and Article 2 use Atari 2600 Centipede dataset.sent: Article 1 and Article 2 use Atari 2600 Beam Rider dataset.sent: Article 1 and Article 2 use Atari 2600 Demon Attack dataset.sent: Article 1 and Article 2 use Atari 2600 Crazy Climber dataset.sent: Article 1 and Article 2 use Atari 2600 James Bond dataset.sent: Article 1 and Article 2 use Atari 2600 Kung-Fu Master dataset.sent: Article 1 and Article 2 use Atari 2600 Asteroids dataset.sent: Article 1 and Article 2 use Atari 2600 Battle Zone dataset.sent: Article 1 and Article 2 use Atari 2600 Tutankham dataset.sent: Article 1 and Article 2 use Atari 2600 Robotank dataset.sent: Article 1 and Article 2 use Atari 2600 Boxing dataset.sent: Article 1 and Article 2 use Atari 2600 River Raid dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Atari 2600 Phoenix, and  Atari 2600 Defender.","sent: Article 1 and Article 2 use Prior Duel hsapproach for Atari Games. sent: Article 1 uses DDQN  tuned  hs approach for Atari Games, while Article 2 uses Duel hs for Atari Games. sent: Article 1 uses DDQN  tuned  hs approach for Atari Games, while Article 2 uses DDQN  tuned  noop for Atari Games. sent: Article 1 uses DDQN  tuned  hs approach for Atari Games, while Article 2 uses Prior Duel noop for Atari Games. sent: Article 1 uses DDQN  tuned  hs approach for Atari Games, while Article 2 uses Duel noop for Atari Games. sent: Article 1 uses DQN hs approach for Atari Games, while Article 2 uses Duel hs for Atari Games. sent: Article 1 uses DQN hs approach for Atari Games, while Article 2 uses DDQN  tuned  noop for Atari Games. sent: Article 1 uses DQN hs approach for Atari Games, while Article 2 uses Prior Duel noop for Atari Games. sent: Article 1 uses DQN hs approach for Atari Games, while Article 2 uses Duel noop for Atari Games. sent: Article 1 uses DQN noop approach for Atari Games, while Article 2 uses Duel hs for Atari Games. sent: Article 1 uses DQN noop approach for Atari Games, while Article 2 uses DDQN  tuned  noop for Atari Games. sent: Article 1 uses DQN noop approach for Atari Games, while Article 2 uses Prior Duel noop for Atari Games. sent: Article 1 uses DQN noop approach for Atari Games, while Article 2 uses Duel noop for Atari Games."
881,881,14_paper_501,14_paper_59,150,155,1509.06461,1706.0809,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Berzerk,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Asterix,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Bank Heist,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Boxing, and  Atari 2600 River Raid, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DDQN  tuned  hs approach for Atari Games, while Article 2 uses Sarsa- for Atari Games. sent: Article 1 uses DDQN  tuned  hs approach for Atari Games, while Article 2 uses Sarsa--EB for Atari Games. sent: Article 1 uses Prior Duel hs approach for Atari Games, while Article 2 uses Sarsa- for Atari Games. sent: Article 1 uses Prior Duel hs approach for Atari Games, while Article 2 uses Sarsa--EB for Atari Games. sent: Article 1 uses DQN hs approach for Atari Games, while Article 2 uses Sarsa- for Atari Games. sent: Article 1 uses DQN hs approach for Atari Games, while Article 2 uses Sarsa--EB for Atari Games. sent: Article 1 uses DQN noop approach for Atari Games, while Article 2 uses Sarsa- for Atari Games. sent: Article 1 uses DQN noop approach for Atari Games, while Article 2 uses Sarsa--EB for Atari Games."
882,882,14_paper_501,14_paper_45,150,158,1509.06461,1207.4708,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Fishing Derby dataset.sent: Article 1 and Article 2 use Atari 2600 Atlantis dataset.sent: Article 1 and Article 2 use Atari 2600 Tennis dataset.sent: Article 1 and Article 2 use Atari 2600 Video Pinball dataset.sent: Article 1 and Article 2 use Atari 2600 Up and Down dataset.sent: Article 1 and Article 2 use Atari 2600 Gopher dataset.sent: Article 1 and Article 2 use Atari 2600 Double Dunk dataset.sent: Article 1 and Article 2 use Atari 2600 Assault dataset.sent: Article 1 and Article 2 use Atari 2600 Road Runner dataset.sent: Article 1 and Article 2 use Atari 2600 Private Eye dataset.sent: Article 1 and Article 2 use Atari 2600 Gravitar dataset.sent: Article 1 and Article 2 use Atari 2600 Breakout dataset.sent: Article 1 and Article 2 use Atari 2600 Krull dataset.sent: Article 1 and Article 2 use Atari 2600 HERO dataset.sent: Article 1 and Article 2 use Atari 2600 Ice Hockey dataset.sent: Article 1 and Article 2 use Atari 2600 Asterix dataset.sent: Article 1 and Article 2 use Atari 2600 Time Pilot dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Seaquest dataset.sent: Article 1 and Article 2 use Atari 2600 Name This Game dataset.sent: Article 1 and Article 2 use Atari 2600 Chopper Command dataset.sent: Article 1 and Article 2 use Atari 2600 Kangaroo dataset.sent: Article 1 and Article 2 use Atari 2600 Zaxxon dataset.sent: Article 1 and Article 2 use Atari 2600 Bowling dataset.sent: Article 1 and Article 2 use Atari 2600 Enduro dataset.sent: Article 1 and Article 2 use Atari 2600 Wizard of Wor dataset.sent: Article 1 and Article 2 use Atari 2600 Ms  Pacman dataset.sent: Article 1 and Article 2 use Atari 2600 Pong dataset.sent: Article 1 and Article 2 use Atari 2600 Star Gunner dataset.sent: Article 1 and Article 2 use Atari 2600 Space Invaders dataset.sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Bank Heist dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Alien dataset.sent: Article 1 and Article 2 use Atari 2600 Amidar dataset.sent: Article 1 and Article 2 use Atari 2600 Centipede dataset.sent: Article 1 and Article 2 use Atari 2600 Beam Rider dataset.sent: Article 1 and Article 2 use Atari 2600 Demon Attack dataset.sent: Article 1 and Article 2 use Atari 2600 Crazy Climber dataset.sent: Article 1 and Article 2 use Atari 2600 James Bond dataset.sent: Article 1 and Article 2 use Atari 2600 Kung-Fu Master dataset.sent: Article 1 and Article 2 use Atari 2600 Asteroids dataset.sent: Article 1 and Article 2 use Atari 2600 Battle Zone dataset.sent: Article 1 and Article 2 use Atari 2600 Tutankham dataset.sent: Article 1 and Article 2 use Atari 2600 Robotank dataset.sent: Article 1 and Article 2 use Atari 2600 Boxing dataset.sent: Article 1 and Article 2 use Atari 2600 River Raid dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 uses Atari 2600 Berzerk dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DDQN  tuned  hs approach for Atari Games, while Article 2 uses Best linear for Atari Games. sent: Article 1 uses Prior Duel hs approach for Atari Games, while Article 2 uses Best linear for Atari Games. sent: Article 1 uses DQN hs approach for Atari Games, while Article 2 uses Best linear for Atari Games. sent: Article 1 uses DQN noop approach for Atari Games, while Article 2 uses Best linear for Atari Games."
883,883,14_paper_501,14_paper_288,150,1511,1509.06461,1507.04296,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Fishing Derby dataset.sent: Article 1 and Article 2 use Atari 2600 Atlantis dataset.sent: Article 1 and Article 2 use Atari 2600 Tennis dataset.sent: Article 1 and Article 2 use Atari 2600 Video Pinball dataset.sent: Article 1 and Article 2 use Atari 2600 Up and Down dataset.sent: Article 1 and Article 2 use Atari 2600 Gopher dataset.sent: Article 1 and Article 2 use Atari 2600 Double Dunk dataset.sent: Article 1 and Article 2 use Atari 2600 Assault dataset.sent: Article 1 and Article 2 use Atari 2600 Road Runner dataset.sent: Article 1 and Article 2 use Atari 2600 Private Eye dataset.sent: Article 1 and Article 2 use Atari 2600 Gravitar dataset.sent: Article 1 and Article 2 use Atari 2600 Breakout dataset.sent: Article 1 and Article 2 use Atari 2600 Krull dataset.sent: Article 1 and Article 2 use Atari 2600 HERO dataset.sent: Article 1 and Article 2 use Atari 2600 Ice Hockey dataset.sent: Article 1 and Article 2 use Atari 2600 Asterix dataset.sent: Article 1 and Article 2 use Atari 2600 Time Pilot dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Seaquest dataset.sent: Article 1 and Article 2 use Atari 2600 Name This Game dataset.sent: Article 1 and Article 2 use Atari 2600 Chopper Command dataset.sent: Article 1 and Article 2 use Atari 2600 Kangaroo dataset.sent: Article 1 and Article 2 use Atari 2600 Zaxxon dataset.sent: Article 1 and Article 2 use Atari 2600 Bowling dataset.sent: Article 1 and Article 2 use Atari 2600 Enduro dataset.sent: Article 1 and Article 2 use Atari 2600 Wizard of Wor dataset.sent: Article 1 and Article 2 use Atari 2600 Ms  Pacman dataset.sent: Article 1 and Article 2 use Atari 2600 Pong dataset.sent: Article 1 and Article 2 use Atari 2600 Star Gunner dataset.sent: Article 1 and Article 2 use Atari 2600 Space Invaders dataset.sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Bank Heist dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Alien dataset.sent: Article 1 and Article 2 use Atari 2600 Amidar dataset.sent: Article 1 and Article 2 use Atari 2600 Centipede dataset.sent: Article 1 and Article 2 use Atari 2600 Beam Rider dataset.sent: Article 1 and Article 2 use Atari 2600 Demon Attack dataset.sent: Article 1 and Article 2 use Atari 2600 Crazy Climber dataset.sent: Article 1 and Article 2 use Atari 2600 James Bond dataset.sent: Article 1 and Article 2 use Atari 2600 Kung-Fu Master dataset.sent: Article 1 and Article 2 use Atari 2600 Asteroids dataset.sent: Article 1 and Article 2 use Atari 2600 Battle Zone dataset.sent: Article 1 and Article 2 use Atari 2600 Tutankham dataset.sent: Article 1 and Article 2 use Atari 2600 Robotank dataset.sent: Article 1 and Article 2 use Atari 2600 Boxing dataset.sent: Article 1 and Article 2 use Atari 2600 River Raid dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 uses Atari 2600 Berzerk dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DDQN  tuned  hs approach for Atari Games, while Article 2 uses Gorila for Atari Games. sent: Article 1 uses Prior Duel hs approach for Atari Games, while Article 2 uses Gorila for Atari Games. sent: Article 1 uses DQN hs approach for Atari Games, while Article 2 uses Gorila for Atari Games. sent: Article 1 uses DQN noop approach for Atari Games, while Article 2 uses Gorila for Atari Games."
884,884,14_paper_501,14_paper_0,150,1562,1509.06461,1507.00814,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Berzerk,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Asterix,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Bank Heist,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Boxing, and  Atari 2600 River Raid, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DDQN  tuned  hs approach for Atari Games, while Article 2 uses MP-EB for Atari Games. sent: Article 1 uses Prior Duel hs approach for Atari Games, while Article 2 uses MP-EB for Atari Games. sent: Article 1 uses DQN hs approach for Atari Games, while Article 2 uses MP-EB for Atari Games. sent: Article 1 uses DQN noop approach for Atari Games, while Article 2 uses MP-EB for Atari Games."
885,885,14_paper_501,14_paper_5,150,15186,1509.06461,1704.04651,Atari Games,"sent: Article 1 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Berzerk,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Asterix,  Atari 2600 Freeway,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Venture,  Atari 2600 Q Bert,  Atari 2600 Bank Heist,  Atari 2600 Montezuma s Revenge,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Boxing,  Atari 2600 River Raid, and  Atari 2600 Frostbite, while Article 2 uses Atari-57 dataset.","sent: Article 1 uses DDQN  tuned  hs approach for Atari Games, while Article 2 uses Reactor for Atari Games. sent: Article 1 uses Prior Duel hs approach for Atari Games, while Article 2 uses Reactor for Atari Games. sent: Article 1 uses DQN hs approach for Atari Games, while Article 2 uses Reactor for Atari Games. sent: Article 1 uses DQN noop approach for Atari Games, while Article 2 uses Reactor for Atari Games."
886,886,14_paper_79,14_paper_59,153,155,1511.06581,1706.0809,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Berzerk,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Defender,  Atari 2600 Asterix,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Bank Heist,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Phoenix,  Atari 2600 Boxing, and  Atari 2600 River Raid, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Prior Duel hs approach for Atari Games, while Article 2 uses Sarsa- for Atari Games. sent: Article 1 uses Prior Duel hs approach for Atari Games, while Article 2 uses Sarsa--EB for Atari Games. sent: Article 1 uses Prior Duel noop approach for Atari Games, while Article 2 uses Sarsa- for Atari Games. sent: Article 1 uses Prior Duel noop approach for Atari Games, while Article 2 uses Sarsa--EB for Atari Games. sent: Article 1 uses Duel noop approach for Atari Games, while Article 2 uses Sarsa- for Atari Games. sent: Article 1 uses Duel noop approach for Atari Games, while Article 2 uses Sarsa--EB for Atari Games. sent: Article 1 uses Duel hs approach for Atari Games, while Article 2 uses Sarsa- for Atari Games. sent: Article 1 uses Duel hs approach for Atari Games, while Article 2 uses Sarsa--EB for Atari Games. sent: Article 1 uses DDQN  tuned  noop approach for Atari Games, while Article 2 uses Sarsa- for Atari Games. sent: Article 1 uses DDQN  tuned  noop approach for Atari Games, while Article 2 uses Sarsa--EB for Atari Games."
887,887,14_paper_79,14_paper_478,153,156,1511.06581,1509.06461,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Fishing Derby dataset.sent: Article 1 and Article 2 use Atari 2600 Atlantis dataset.sent: Article 1 and Article 2 use Atari 2600 Tennis dataset.sent: Article 1 and Article 2 use Atari 2600 Video Pinball dataset.sent: Article 1 and Article 2 use Atari 2600 Up and Down dataset.sent: Article 1 and Article 2 use Atari 2600 Gopher dataset.sent: Article 1 and Article 2 use Atari 2600 Double Dunk dataset.sent: Article 1 and Article 2 use Atari 2600 Assault dataset.sent: Article 1 and Article 2 use Atari 2600 Road Runner dataset.sent: Article 1 and Article 2 use Atari 2600 Berzerk dataset.sent: Article 1 and Article 2 use Atari 2600 Private Eye dataset.sent: Article 1 and Article 2 use Atari 2600 Gravitar dataset.sent: Article 1 and Article 2 use Atari 2600 Breakout dataset.sent: Article 1 and Article 2 use Atari 2600 Krull dataset.sent: Article 1 and Article 2 use Atari 2600 HERO dataset.sent: Article 1 and Article 2 use Atari 2600 Ice Hockey dataset.sent: Article 1 and Article 2 use Atari 2600 Asterix dataset.sent: Article 1 and Article 2 use Atari 2600 Time Pilot dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Seaquest dataset.sent: Article 1 and Article 2 use Atari 2600 Name This Game dataset.sent: Article 1 and Article 2 use Atari 2600 Chopper Command dataset.sent: Article 1 and Article 2 use Atari 2600 Kangaroo dataset.sent: Article 1 and Article 2 use Atari 2600 Zaxxon dataset.sent: Article 1 and Article 2 use Atari 2600 Bowling dataset.sent: Article 1 and Article 2 use Atari 2600 Enduro dataset.sent: Article 1 and Article 2 use Atari 2600 Wizard of Wor dataset.sent: Article 1 and Article 2 use Atari 2600 Ms  Pacman dataset.sent: Article 1 and Article 2 use Atari 2600 Pong dataset.sent: Article 1 and Article 2 use Atari 2600 Star Gunner dataset.sent: Article 1 and Article 2 use Atari 2600 Space Invaders dataset.sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Bank Heist dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Alien dataset.sent: Article 1 and Article 2 use Atari 2600 Amidar dataset.sent: Article 1 and Article 2 use Atari 2600 Centipede dataset.sent: Article 1 and Article 2 use Atari 2600 Beam Rider dataset.sent: Article 1 and Article 2 use Atari 2600 Demon Attack dataset.sent: Article 1 and Article 2 use Atari 2600 Crazy Climber dataset.sent: Article 1 and Article 2 use Atari 2600 James Bond dataset.sent: Article 1 and Article 2 use Atari 2600 Kung-Fu Master dataset.sent: Article 1 and Article 2 use Atari 2600 Asteroids dataset.sent: Article 1 and Article 2 use Atari 2600 Battle Zone dataset.sent: Article 1 and Article 2 use Atari 2600 Tutankham dataset.sent: Article 1 and Article 2 use Atari 2600 Robotank dataset.sent: Article 1 and Article 2 use Atari 2600 Boxing dataset.sent: Article 1 and Article 2 use Atari 2600 River Raid dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 uses multiple datasets:  Atari 2600 Phoenix, and  Atari 2600 Defender, while Article 2 does not use specific datasets for experiments.","sent: Article 1 and Article 2 use Prior Duel hsapproach for Atari Games. sent: Article 1 uses Duel hs approach for Atari Games, while Article 2 uses DDQN  tuned  hs for Atari Games. sent: Article 1 uses Duel hs approach for Atari Games, while Article 2 uses DQN hs for Atari Games. sent: Article 1 uses Duel hs approach for Atari Games, while Article 2 uses DQN noop for Atari Games. sent: Article 1 uses DDQN  tuned  noop approach for Atari Games, while Article 2 uses DDQN  tuned  hs for Atari Games. sent: Article 1 uses DDQN  tuned  noop approach for Atari Games, while Article 2 uses DQN hs for Atari Games. sent: Article 1 uses DDQN  tuned  noop approach for Atari Games, while Article 2 uses DQN noop for Atari Games. sent: Article 1 uses Prior Duel noop approach for Atari Games, while Article 2 uses DDQN  tuned  hs for Atari Games. sent: Article 1 uses Prior Duel noop approach for Atari Games, while Article 2 uses DQN hs for Atari Games. sent: Article 1 uses Prior Duel noop approach for Atari Games, while Article 2 uses DQN noop for Atari Games. sent: Article 1 uses Duel noop approach for Atari Games, while Article 2 uses DDQN  tuned  hs for Atari Games. sent: Article 1 uses Duel noop approach for Atari Games, while Article 2 uses DQN hs for Atari Games. sent: Article 1 uses Duel noop approach for Atari Games, while Article 2 uses DQN noop for Atari Games."
888,888,14_paper_79,14_paper_45,153,158,1511.06581,1207.4708,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Fishing Derby dataset.sent: Article 1 and Article 2 use Atari 2600 Atlantis dataset.sent: Article 1 and Article 2 use Atari 2600 Tennis dataset.sent: Article 1 and Article 2 use Atari 2600 Video Pinball dataset.sent: Article 1 and Article 2 use Atari 2600 Up and Down dataset.sent: Article 1 and Article 2 use Atari 2600 Gopher dataset.sent: Article 1 and Article 2 use Atari 2600 Double Dunk dataset.sent: Article 1 and Article 2 use Atari 2600 Assault dataset.sent: Article 1 and Article 2 use Atari 2600 Road Runner dataset.sent: Article 1 and Article 2 use Atari 2600 Private Eye dataset.sent: Article 1 and Article 2 use Atari 2600 Gravitar dataset.sent: Article 1 and Article 2 use Atari 2600 Breakout dataset.sent: Article 1 and Article 2 use Atari 2600 Krull dataset.sent: Article 1 and Article 2 use Atari 2600 HERO dataset.sent: Article 1 and Article 2 use Atari 2600 Ice Hockey dataset.sent: Article 1 and Article 2 use Atari 2600 Asterix dataset.sent: Article 1 and Article 2 use Atari 2600 Time Pilot dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Seaquest dataset.sent: Article 1 and Article 2 use Atari 2600 Name This Game dataset.sent: Article 1 and Article 2 use Atari 2600 Chopper Command dataset.sent: Article 1 and Article 2 use Atari 2600 Kangaroo dataset.sent: Article 1 and Article 2 use Atari 2600 Zaxxon dataset.sent: Article 1 and Article 2 use Atari 2600 Bowling dataset.sent: Article 1 and Article 2 use Atari 2600 Enduro dataset.sent: Article 1 and Article 2 use Atari 2600 Wizard of Wor dataset.sent: Article 1 and Article 2 use Atari 2600 Ms  Pacman dataset.sent: Article 1 and Article 2 use Atari 2600 Pong dataset.sent: Article 1 and Article 2 use Atari 2600 Star Gunner dataset.sent: Article 1 and Article 2 use Atari 2600 Space Invaders dataset.sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Bank Heist dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Alien dataset.sent: Article 1 and Article 2 use Atari 2600 Amidar dataset.sent: Article 1 and Article 2 use Atari 2600 Centipede dataset.sent: Article 1 and Article 2 use Atari 2600 Beam Rider dataset.sent: Article 1 and Article 2 use Atari 2600 Demon Attack dataset.sent: Article 1 and Article 2 use Atari 2600 Crazy Climber dataset.sent: Article 1 and Article 2 use Atari 2600 James Bond dataset.sent: Article 1 and Article 2 use Atari 2600 Kung-Fu Master dataset.sent: Article 1 and Article 2 use Atari 2600 Asteroids dataset.sent: Article 1 and Article 2 use Atari 2600 Battle Zone dataset.sent: Article 1 and Article 2 use Atari 2600 Tutankham dataset.sent: Article 1 and Article 2 use Atari 2600 Robotank dataset.sent: Article 1 and Article 2 use Atari 2600 Boxing dataset.sent: Article 1 and Article 2 use Atari 2600 River Raid dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 uses multiple datasets:  Atari 2600 Berzerk,  Atari 2600 Defender, and  Atari 2600 Phoenix, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Prior Duel hs approach for Atari Games, while Article 2 uses Best linear for Atari Games. sent: Article 1 uses Prior Duel noop approach for Atari Games, while Article 2 uses Best linear for Atari Games. sent: Article 1 uses Duel noop approach for Atari Games, while Article 2 uses Best linear for Atari Games. sent: Article 1 uses Duel hs approach for Atari Games, while Article 2 uses Best linear for Atari Games. sent: Article 1 uses DDQN  tuned  noop approach for Atari Games, while Article 2 uses Best linear for Atari Games."
889,889,14_paper_79,14_paper_288,153,1511,1511.06581,1507.04296,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Fishing Derby dataset.sent: Article 1 and Article 2 use Atari 2600 Atlantis dataset.sent: Article 1 and Article 2 use Atari 2600 Tennis dataset.sent: Article 1 and Article 2 use Atari 2600 Video Pinball dataset.sent: Article 1 and Article 2 use Atari 2600 Up and Down dataset.sent: Article 1 and Article 2 use Atari 2600 Gopher dataset.sent: Article 1 and Article 2 use Atari 2600 Double Dunk dataset.sent: Article 1 and Article 2 use Atari 2600 Assault dataset.sent: Article 1 and Article 2 use Atari 2600 Road Runner dataset.sent: Article 1 and Article 2 use Atari 2600 Private Eye dataset.sent: Article 1 and Article 2 use Atari 2600 Gravitar dataset.sent: Article 1 and Article 2 use Atari 2600 Breakout dataset.sent: Article 1 and Article 2 use Atari 2600 Krull dataset.sent: Article 1 and Article 2 use Atari 2600 HERO dataset.sent: Article 1 and Article 2 use Atari 2600 Ice Hockey dataset.sent: Article 1 and Article 2 use Atari 2600 Asterix dataset.sent: Article 1 and Article 2 use Atari 2600 Time Pilot dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Seaquest dataset.sent: Article 1 and Article 2 use Atari 2600 Name This Game dataset.sent: Article 1 and Article 2 use Atari 2600 Chopper Command dataset.sent: Article 1 and Article 2 use Atari 2600 Kangaroo dataset.sent: Article 1 and Article 2 use Atari 2600 Zaxxon dataset.sent: Article 1 and Article 2 use Atari 2600 Bowling dataset.sent: Article 1 and Article 2 use Atari 2600 Enduro dataset.sent: Article 1 and Article 2 use Atari 2600 Wizard of Wor dataset.sent: Article 1 and Article 2 use Atari 2600 Ms  Pacman dataset.sent: Article 1 and Article 2 use Atari 2600 Pong dataset.sent: Article 1 and Article 2 use Atari 2600 Star Gunner dataset.sent: Article 1 and Article 2 use Atari 2600 Space Invaders dataset.sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Bank Heist dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Alien dataset.sent: Article 1 and Article 2 use Atari 2600 Amidar dataset.sent: Article 1 and Article 2 use Atari 2600 Centipede dataset.sent: Article 1 and Article 2 use Atari 2600 Beam Rider dataset.sent: Article 1 and Article 2 use Atari 2600 Demon Attack dataset.sent: Article 1 and Article 2 use Atari 2600 Crazy Climber dataset.sent: Article 1 and Article 2 use Atari 2600 James Bond dataset.sent: Article 1 and Article 2 use Atari 2600 Kung-Fu Master dataset.sent: Article 1 and Article 2 use Atari 2600 Asteroids dataset.sent: Article 1 and Article 2 use Atari 2600 Battle Zone dataset.sent: Article 1 and Article 2 use Atari 2600 Tutankham dataset.sent: Article 1 and Article 2 use Atari 2600 Robotank dataset.sent: Article 1 and Article 2 use Atari 2600 Boxing dataset.sent: Article 1 and Article 2 use Atari 2600 River Raid dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 uses multiple datasets:  Atari 2600 Berzerk,  Atari 2600 Defender, and  Atari 2600 Phoenix, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Prior Duel hs approach for Atari Games, while Article 2 uses Gorila for Atari Games. sent: Article 1 uses Prior Duel noop approach for Atari Games, while Article 2 uses Gorila for Atari Games. sent: Article 1 uses Duel noop approach for Atari Games, while Article 2 uses Gorila for Atari Games. sent: Article 1 uses Duel hs approach for Atari Games, while Article 2 uses Gorila for Atari Games. sent: Article 1 uses DDQN  tuned  noop approach for Atari Games, while Article 2 uses Gorila for Atari Games."
890,890,14_paper_79,14_paper_0,153,1562,1511.06581,1507.00814,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Berzerk,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Defender,  Atari 2600 Asterix,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Bank Heist,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Phoenix,  Atari 2600 Boxing, and  Atari 2600 River Raid, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Prior Duel hs approach for Atari Games, while Article 2 uses MP-EB for Atari Games. sent: Article 1 uses Prior Duel noop approach for Atari Games, while Article 2 uses MP-EB for Atari Games. sent: Article 1 uses Duel noop approach for Atari Games, while Article 2 uses MP-EB for Atari Games. sent: Article 1 uses Duel hs approach for Atari Games, while Article 2 uses MP-EB for Atari Games. sent: Article 1 uses DDQN  tuned  noop approach for Atari Games, while Article 2 uses MP-EB for Atari Games."
891,891,14_paper_79,14_paper_5,153,15186,1511.06581,1704.04651,Atari Games,"sent: Article 1 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Berzerk,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Defender,  Atari 2600 Asterix,  Atari 2600 Freeway,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Venture,  Atari 2600 Q Bert,  Atari 2600 Bank Heist,  Atari 2600 Montezuma s Revenge,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Phoenix,  Atari 2600 Boxing,  Atari 2600 River Raid, and  Atari 2600 Frostbite, while Article 2 uses Atari-57 dataset.","sent: Article 1 uses Prior Duel hs approach for Atari Games, while Article 2 uses Reactor for Atari Games. sent: Article 1 uses Prior Duel noop approach for Atari Games, while Article 2 uses Reactor for Atari Games. sent: Article 1 uses Duel noop approach for Atari Games, while Article 2 uses Reactor for Atari Games. sent: Article 1 uses Duel hs approach for Atari Games, while Article 2 uses Reactor for Atari Games. sent: Article 1 uses DDQN  tuned  noop approach for Atari Games, while Article 2 uses Reactor for Atari Games."
892,892,14_paper_59,14_paper_478,155,156,1706.0809,1509.06461,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Berzerk,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Asterix,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Bank Heist,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Boxing, and  Atari 2600 River Raid.","sent: Article 1 uses Sarsa- approach for Atari Games, while Article 2 uses DDQN  tuned  hs for Atari Games. sent: Article 1 uses Sarsa- approach for Atari Games, while Article 2 uses Prior Duel hs for Atari Games. sent: Article 1 uses Sarsa- approach for Atari Games, while Article 2 uses DQN hs for Atari Games. sent: Article 1 uses Sarsa- approach for Atari Games, while Article 2 uses DQN noop for Atari Games. sent: Article 1 uses Sarsa--EB approach for Atari Games, while Article 2 uses DDQN  tuned  hs for Atari Games. sent: Article 1 uses Sarsa--EB approach for Atari Games, while Article 2 uses Prior Duel hs for Atari Games. sent: Article 1 uses Sarsa--EB approach for Atari Games, while Article 2 uses DQN hs for Atari Games. sent: Article 1 uses Sarsa--EB approach for Atari Games, while Article 2 uses DQN noop for Atari Games."
893,893,14_paper_59,14_paper_45,155,158,1706.0809,1207.4708,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Asterix,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Bank Heist,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Boxing, and  Atari 2600 River Raid.","sent: Article 1 uses Sarsa- approach for Atari Games, while Article 2 uses Best linear for Atari Games. sent: Article 1 uses Sarsa--EB approach for Atari Games, while Article 2 uses Best linear for Atari Games."
894,894,14_paper_59,14_paper_120,155,159,1706.0809,1511.06581,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Berzerk,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Defender,  Atari 2600 Asterix,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Bank Heist,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Phoenix,  Atari 2600 Boxing, and  Atari 2600 River Raid.","sent: Article 1 uses Sarsa- approach for Atari Games, while Article 2 uses Prior Duel hs for Atari Games. sent: Article 1 uses Sarsa- approach for Atari Games, while Article 2 uses Prior Duel noop for Atari Games. sent: Article 1 uses Sarsa- approach for Atari Games, while Article 2 uses Duel noop for Atari Games. sent: Article 1 uses Sarsa- approach for Atari Games, while Article 2 uses Duel hs for Atari Games. sent: Article 1 uses Sarsa- approach for Atari Games, while Article 2 uses DDQN  tuned  noop for Atari Games. sent: Article 1 uses Sarsa--EB approach for Atari Games, while Article 2 uses Prior Duel hs for Atari Games. sent: Article 1 uses Sarsa--EB approach for Atari Games, while Article 2 uses Prior Duel noop for Atari Games. sent: Article 1 uses Sarsa--EB approach for Atari Games, while Article 2 uses Duel noop for Atari Games. sent: Article 1 uses Sarsa--EB approach for Atari Games, while Article 2 uses Duel hs for Atari Games. sent: Article 1 uses Sarsa--EB approach for Atari Games, while Article 2 uses DDQN  tuned  noop for Atari Games."
895,895,14_paper_59,14_paper_288,155,1511,1706.0809,1507.04296,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Asterix,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Bank Heist,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Boxing, and  Atari 2600 River Raid.","sent: Article 1 uses Sarsa- approach for Atari Games, while Article 2 uses Gorila for Atari Games. sent: Article 1 uses Sarsa--EB approach for Atari Games, while Article 2 uses Gorila for Atari Games."
896,896,14_paper_59,14_paper_0,155,1562,1706.0809,1507.00814,Atari Games,sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset.,"sent: Article 1 uses Sarsa- approach for Atari Games, while Article 2 uses MP-EB for Atari Games. sent: Article 1 uses Sarsa--EB approach for Atari Games, while Article 2 uses MP-EB for Atari Games."
897,897,14_paper_59,14_paper_5,155,15186,1706.0809,1704.04651,Atari Games,"sent: Article 1 uses multiple datasets:  Atari 2600 Venture,  Atari 2600 Q Bert,  Atari 2600 Freeway,  Atari 2600 Montezuma s Revenge, and  Atari 2600 Frostbite, while Article 2 uses Atari-57 dataset.","sent: Article 1 uses Sarsa- approach for Atari Games, while Article 2 uses Reactor for Atari Games. sent: Article 1 uses Sarsa--EB approach for Atari Games, while Article 2 uses Reactor for Atari Games."
898,898,14_paper_45,14_paper_120,158,159,1207.4708,1511.06581,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Fishing Derby dataset.sent: Article 1 and Article 2 use Atari 2600 Atlantis dataset.sent: Article 1 and Article 2 use Atari 2600 Tennis dataset.sent: Article 1 and Article 2 use Atari 2600 Video Pinball dataset.sent: Article 1 and Article 2 use Atari 2600 Up and Down dataset.sent: Article 1 and Article 2 use Atari 2600 Gopher dataset.sent: Article 1 and Article 2 use Atari 2600 Double Dunk dataset.sent: Article 1 and Article 2 use Atari 2600 Assault dataset.sent: Article 1 and Article 2 use Atari 2600 Road Runner dataset.sent: Article 1 and Article 2 use Atari 2600 Private Eye dataset.sent: Article 1 and Article 2 use Atari 2600 Gravitar dataset.sent: Article 1 and Article 2 use Atari 2600 Breakout dataset.sent: Article 1 and Article 2 use Atari 2600 Krull dataset.sent: Article 1 and Article 2 use Atari 2600 HERO dataset.sent: Article 1 and Article 2 use Atari 2600 Ice Hockey dataset.sent: Article 1 and Article 2 use Atari 2600 Asterix dataset.sent: Article 1 and Article 2 use Atari 2600 Time Pilot dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Seaquest dataset.sent: Article 1 and Article 2 use Atari 2600 Name This Game dataset.sent: Article 1 and Article 2 use Atari 2600 Chopper Command dataset.sent: Article 1 and Article 2 use Atari 2600 Kangaroo dataset.sent: Article 1 and Article 2 use Atari 2600 Zaxxon dataset.sent: Article 1 and Article 2 use Atari 2600 Bowling dataset.sent: Article 1 and Article 2 use Atari 2600 Enduro dataset.sent: Article 1 and Article 2 use Atari 2600 Wizard of Wor dataset.sent: Article 1 and Article 2 use Atari 2600 Ms  Pacman dataset.sent: Article 1 and Article 2 use Atari 2600 Pong dataset.sent: Article 1 and Article 2 use Atari 2600 Star Gunner dataset.sent: Article 1 and Article 2 use Atari 2600 Space Invaders dataset.sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Bank Heist dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Alien dataset.sent: Article 1 and Article 2 use Atari 2600 Amidar dataset.sent: Article 1 and Article 2 use Atari 2600 Centipede dataset.sent: Article 1 and Article 2 use Atari 2600 Beam Rider dataset.sent: Article 1 and Article 2 use Atari 2600 Demon Attack dataset.sent: Article 1 and Article 2 use Atari 2600 Crazy Climber dataset.sent: Article 1 and Article 2 use Atari 2600 James Bond dataset.sent: Article 1 and Article 2 use Atari 2600 Kung-Fu Master dataset.sent: Article 1 and Article 2 use Atari 2600 Asteroids dataset.sent: Article 1 and Article 2 use Atari 2600 Battle Zone dataset.sent: Article 1 and Article 2 use Atari 2600 Tutankham dataset.sent: Article 1 and Article 2 use Atari 2600 Robotank dataset.sent: Article 1 and Article 2 use Atari 2600 Boxing dataset.sent: Article 1 and Article 2 use Atari 2600 River Raid dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Atari 2600 Berzerk,  Atari 2600 Defender, and  Atari 2600 Phoenix.","sent: Article 1 uses Best linear approach for Atari Games, while Article 2 uses Prior Duel hs for Atari Games. sent: Article 1 uses Best linear approach for Atari Games, while Article 2 uses Prior Duel noop for Atari Games. sent: Article 1 uses Best linear approach for Atari Games, while Article 2 uses Duel noop for Atari Games. sent: Article 1 uses Best linear approach for Atari Games, while Article 2 uses Duel hs for Atari Games. sent: Article 1 uses Best linear approach for Atari Games, while Article 2 uses DDQN  tuned  noop for Atari Games."
899,899,14_paper_45,14_paper_288,158,1511,1207.4708,1507.04296,Atari Games,sent: Article 1 and Article 2 use Atari 2600 Fishing Derby dataset.sent: Article 1 and Article 2 use Atari 2600 Atlantis dataset.sent: Article 1 and Article 2 use Atari 2600 Tennis dataset.sent: Article 1 and Article 2 use Atari 2600 Video Pinball dataset.sent: Article 1 and Article 2 use Atari 2600 Up and Down dataset.sent: Article 1 and Article 2 use Atari 2600 Gopher dataset.sent: Article 1 and Article 2 use Atari 2600 Double Dunk dataset.sent: Article 1 and Article 2 use Atari 2600 Assault dataset.sent: Article 1 and Article 2 use Atari 2600 Road Runner dataset.sent: Article 1 and Article 2 use Atari 2600 Private Eye dataset.sent: Article 1 and Article 2 use Atari 2600 Gravitar dataset.sent: Article 1 and Article 2 use Atari 2600 Breakout dataset.sent: Article 1 and Article 2 use Atari 2600 Krull dataset.sent: Article 1 and Article 2 use Atari 2600 HERO dataset.sent: Article 1 and Article 2 use Atari 2600 Ice Hockey dataset.sent: Article 1 and Article 2 use Atari 2600 Asterix dataset.sent: Article 1 and Article 2 use Atari 2600 Time Pilot dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Seaquest dataset.sent: Article 1 and Article 2 use Atari 2600 Name This Game dataset.sent: Article 1 and Article 2 use Atari 2600 Chopper Command dataset.sent: Article 1 and Article 2 use Atari 2600 Kangaroo dataset.sent: Article 1 and Article 2 use Atari 2600 Zaxxon dataset.sent: Article 1 and Article 2 use Atari 2600 Bowling dataset.sent: Article 1 and Article 2 use Atari 2600 Enduro dataset.sent: Article 1 and Article 2 use Atari 2600 Wizard of Wor dataset.sent: Article 1 and Article 2 use Atari 2600 Ms  Pacman dataset.sent: Article 1 and Article 2 use Atari 2600 Pong dataset.sent: Article 1 and Article 2 use Atari 2600 Star Gunner dataset.sent: Article 1 and Article 2 use Atari 2600 Space Invaders dataset.sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Bank Heist dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Alien dataset.sent: Article 1 and Article 2 use Atari 2600 Amidar dataset.sent: Article 1 and Article 2 use Atari 2600 Centipede dataset.sent: Article 1 and Article 2 use Atari 2600 Beam Rider dataset.sent: Article 1 and Article 2 use Atari 2600 Demon Attack dataset.sent: Article 1 and Article 2 use Atari 2600 Crazy Climber dataset.sent: Article 1 and Article 2 use Atari 2600 James Bond dataset.sent: Article 1 and Article 2 use Atari 2600 Kung-Fu Master dataset.sent: Article 1 and Article 2 use Atari 2600 Asteroids dataset.sent: Article 1 and Article 2 use Atari 2600 Battle Zone dataset.sent: Article 1 and Article 2 use Atari 2600 Tutankham dataset.sent: Article 1 and Article 2 use Atari 2600 Robotank dataset.sent: Article 1 and Article 2 use Atari 2600 Boxing dataset.sent: Article 1 and Article 2 use Atari 2600 River Raid dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset.,"sent: Article 1 uses Best linear approach for Atari Games, while Article 2 uses Gorila for Atari Games."
900,900,14_paper_45,14_paper_339,158,1515,1207.4708,1509.06461,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Fishing Derby dataset.sent: Article 1 and Article 2 use Atari 2600 Atlantis dataset.sent: Article 1 and Article 2 use Atari 2600 Tennis dataset.sent: Article 1 and Article 2 use Atari 2600 Video Pinball dataset.sent: Article 1 and Article 2 use Atari 2600 Up and Down dataset.sent: Article 1 and Article 2 use Atari 2600 Gopher dataset.sent: Article 1 and Article 2 use Atari 2600 Double Dunk dataset.sent: Article 1 and Article 2 use Atari 2600 Assault dataset.sent: Article 1 and Article 2 use Atari 2600 Road Runner dataset.sent: Article 1 and Article 2 use Atari 2600 Private Eye dataset.sent: Article 1 and Article 2 use Atari 2600 Gravitar dataset.sent: Article 1 and Article 2 use Atari 2600 Breakout dataset.sent: Article 1 and Article 2 use Atari 2600 Krull dataset.sent: Article 1 and Article 2 use Atari 2600 HERO dataset.sent: Article 1 and Article 2 use Atari 2600 Ice Hockey dataset.sent: Article 1 and Article 2 use Atari 2600 Asterix dataset.sent: Article 1 and Article 2 use Atari 2600 Time Pilot dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Seaquest dataset.sent: Article 1 and Article 2 use Atari 2600 Name This Game dataset.sent: Article 1 and Article 2 use Atari 2600 Chopper Command dataset.sent: Article 1 and Article 2 use Atari 2600 Kangaroo dataset.sent: Article 1 and Article 2 use Atari 2600 Zaxxon dataset.sent: Article 1 and Article 2 use Atari 2600 Bowling dataset.sent: Article 1 and Article 2 use Atari 2600 Enduro dataset.sent: Article 1 and Article 2 use Atari 2600 Wizard of Wor dataset.sent: Article 1 and Article 2 use Atari 2600 Ms  Pacman dataset.sent: Article 1 and Article 2 use Atari 2600 Pong dataset.sent: Article 1 and Article 2 use Atari 2600 Star Gunner dataset.sent: Article 1 and Article 2 use Atari 2600 Space Invaders dataset.sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Bank Heist dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Alien dataset.sent: Article 1 and Article 2 use Atari 2600 Amidar dataset.sent: Article 1 and Article 2 use Atari 2600 Centipede dataset.sent: Article 1 and Article 2 use Atari 2600 Beam Rider dataset.sent: Article 1 and Article 2 use Atari 2600 Demon Attack dataset.sent: Article 1 and Article 2 use Atari 2600 Crazy Climber dataset.sent: Article 1 and Article 2 use Atari 2600 James Bond dataset.sent: Article 1 and Article 2 use Atari 2600 Kung-Fu Master dataset.sent: Article 1 and Article 2 use Atari 2600 Asteroids dataset.sent: Article 1 and Article 2 use Atari 2600 Battle Zone dataset.sent: Article 1 and Article 2 use Atari 2600 Tutankham dataset.sent: Article 1 and Article 2 use Atari 2600 Robotank dataset.sent: Article 1 and Article 2 use Atari 2600 Boxing dataset.sent: Article 1 and Article 2 use Atari 2600 River Raid dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Atari 2600 Berzerk dataset.","sent: Article 1 uses Best linear approach for Atari Games, while Article 2 uses DDQN  tuned  hs for Atari Games. sent: Article 1 uses Best linear approach for Atari Games, while Article 2 uses Prior Duel hs for Atari Games. sent: Article 1 uses Best linear approach for Atari Games, while Article 2 uses DQN hs for Atari Games. sent: Article 1 uses Best linear approach for Atari Games, while Article 2 uses DQN noop for Atari Games."
901,901,14_paper_45,14_paper_64,158,1543,1207.4708,1706.0809,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Asterix,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Bank Heist,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Boxing, and  Atari 2600 River Raid, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Best linear approach for Atari Games, while Article 2 uses Sarsa- for Atari Games. sent: Article 1 uses Best linear approach for Atari Games, while Article 2 uses Sarsa--EB for Atari Games."
902,902,14_paper_45,14_paper_0,158,1562,1207.4708,1507.00814,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Asterix,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Bank Heist,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Boxing, and  Atari 2600 River Raid, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Best linear approach for Atari Games, while Article 2 uses MP-EB for Atari Games."
903,903,14_paper_45,14_paper_5,158,15186,1207.4708,1704.04651,Atari Games,"sent: Article 1 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Asterix,  Atari 2600 Freeway,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Venture,  Atari 2600 Q Bert,  Atari 2600 Bank Heist,  Atari 2600 Montezuma s Revenge,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Boxing,  Atari 2600 River Raid, and  Atari 2600 Frostbite, while Article 2 uses Atari-57 dataset.","sent: Article 1 uses Best linear approach for Atari Games, while Article 2 uses Reactor for Atari Games."
904,904,14_paper_288,14_paper_44,1511,1513,1507.04296,1207.4708,Atari Games,sent: Article 1 and Article 2 use Atari 2600 Fishing Derby dataset.sent: Article 1 and Article 2 use Atari 2600 Atlantis dataset.sent: Article 1 and Article 2 use Atari 2600 Tennis dataset.sent: Article 1 and Article 2 use Atari 2600 Video Pinball dataset.sent: Article 1 and Article 2 use Atari 2600 Up and Down dataset.sent: Article 1 and Article 2 use Atari 2600 Gopher dataset.sent: Article 1 and Article 2 use Atari 2600 Double Dunk dataset.sent: Article 1 and Article 2 use Atari 2600 Assault dataset.sent: Article 1 and Article 2 use Atari 2600 Road Runner dataset.sent: Article 1 and Article 2 use Atari 2600 Private Eye dataset.sent: Article 1 and Article 2 use Atari 2600 Gravitar dataset.sent: Article 1 and Article 2 use Atari 2600 Breakout dataset.sent: Article 1 and Article 2 use Atari 2600 Krull dataset.sent: Article 1 and Article 2 use Atari 2600 HERO dataset.sent: Article 1 and Article 2 use Atari 2600 Ice Hockey dataset.sent: Article 1 and Article 2 use Atari 2600 Asterix dataset.sent: Article 1 and Article 2 use Atari 2600 Time Pilot dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Seaquest dataset.sent: Article 1 and Article 2 use Atari 2600 Name This Game dataset.sent: Article 1 and Article 2 use Atari 2600 Chopper Command dataset.sent: Article 1 and Article 2 use Atari 2600 Kangaroo dataset.sent: Article 1 and Article 2 use Atari 2600 Zaxxon dataset.sent: Article 1 and Article 2 use Atari 2600 Bowling dataset.sent: Article 1 and Article 2 use Atari 2600 Enduro dataset.sent: Article 1 and Article 2 use Atari 2600 Wizard of Wor dataset.sent: Article 1 and Article 2 use Atari 2600 Ms  Pacman dataset.sent: Article 1 and Article 2 use Atari 2600 Pong dataset.sent: Article 1 and Article 2 use Atari 2600 Star Gunner dataset.sent: Article 1 and Article 2 use Atari 2600 Space Invaders dataset.sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Bank Heist dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Alien dataset.sent: Article 1 and Article 2 use Atari 2600 Amidar dataset.sent: Article 1 and Article 2 use Atari 2600 Centipede dataset.sent: Article 1 and Article 2 use Atari 2600 Beam Rider dataset.sent: Article 1 and Article 2 use Atari 2600 Demon Attack dataset.sent: Article 1 and Article 2 use Atari 2600 Crazy Climber dataset.sent: Article 1 and Article 2 use Atari 2600 James Bond dataset.sent: Article 1 and Article 2 use Atari 2600 Kung-Fu Master dataset.sent: Article 1 and Article 2 use Atari 2600 Asteroids dataset.sent: Article 1 and Article 2 use Atari 2600 Battle Zone dataset.sent: Article 1 and Article 2 use Atari 2600 Tutankham dataset.sent: Article 1 and Article 2 use Atari 2600 Robotank dataset.sent: Article 1 and Article 2 use Atari 2600 Boxing dataset.sent: Article 1 and Article 2 use Atari 2600 River Raid dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset.,"sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses Best linear for Atari Games."
905,905,14_paper_288,14_paper_73,1511,1514,1507.04296,1511.06581,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Fishing Derby dataset.sent: Article 1 and Article 2 use Atari 2600 Atlantis dataset.sent: Article 1 and Article 2 use Atari 2600 Tennis dataset.sent: Article 1 and Article 2 use Atari 2600 Video Pinball dataset.sent: Article 1 and Article 2 use Atari 2600 Up and Down dataset.sent: Article 1 and Article 2 use Atari 2600 Gopher dataset.sent: Article 1 and Article 2 use Atari 2600 Double Dunk dataset.sent: Article 1 and Article 2 use Atari 2600 Assault dataset.sent: Article 1 and Article 2 use Atari 2600 Road Runner dataset.sent: Article 1 and Article 2 use Atari 2600 Private Eye dataset.sent: Article 1 and Article 2 use Atari 2600 Gravitar dataset.sent: Article 1 and Article 2 use Atari 2600 Breakout dataset.sent: Article 1 and Article 2 use Atari 2600 Krull dataset.sent: Article 1 and Article 2 use Atari 2600 HERO dataset.sent: Article 1 and Article 2 use Atari 2600 Ice Hockey dataset.sent: Article 1 and Article 2 use Atari 2600 Asterix dataset.sent: Article 1 and Article 2 use Atari 2600 Time Pilot dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Seaquest dataset.sent: Article 1 and Article 2 use Atari 2600 Name This Game dataset.sent: Article 1 and Article 2 use Atari 2600 Chopper Command dataset.sent: Article 1 and Article 2 use Atari 2600 Kangaroo dataset.sent: Article 1 and Article 2 use Atari 2600 Zaxxon dataset.sent: Article 1 and Article 2 use Atari 2600 Bowling dataset.sent: Article 1 and Article 2 use Atari 2600 Enduro dataset.sent: Article 1 and Article 2 use Atari 2600 Wizard of Wor dataset.sent: Article 1 and Article 2 use Atari 2600 Ms  Pacman dataset.sent: Article 1 and Article 2 use Atari 2600 Pong dataset.sent: Article 1 and Article 2 use Atari 2600 Star Gunner dataset.sent: Article 1 and Article 2 use Atari 2600 Space Invaders dataset.sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Bank Heist dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Alien dataset.sent: Article 1 and Article 2 use Atari 2600 Amidar dataset.sent: Article 1 and Article 2 use Atari 2600 Centipede dataset.sent: Article 1 and Article 2 use Atari 2600 Beam Rider dataset.sent: Article 1 and Article 2 use Atari 2600 Demon Attack dataset.sent: Article 1 and Article 2 use Atari 2600 Crazy Climber dataset.sent: Article 1 and Article 2 use Atari 2600 James Bond dataset.sent: Article 1 and Article 2 use Atari 2600 Kung-Fu Master dataset.sent: Article 1 and Article 2 use Atari 2600 Asteroids dataset.sent: Article 1 and Article 2 use Atari 2600 Battle Zone dataset.sent: Article 1 and Article 2 use Atari 2600 Tutankham dataset.sent: Article 1 and Article 2 use Atari 2600 Robotank dataset.sent: Article 1 and Article 2 use Atari 2600 Boxing dataset.sent: Article 1 and Article 2 use Atari 2600 River Raid dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Atari 2600 Berzerk,  Atari 2600 Defender, and  Atari 2600 Phoenix.","sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses Prior Duel hs for Atari Games. sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses Prior Duel noop for Atari Games. sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses Duel noop for Atari Games. sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses Duel hs for Atari Games. sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses DDQN  tuned  noop for Atari Games."
906,906,14_paper_288,14_paper_339,1511,1515,1507.04296,1509.06461,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Fishing Derby dataset.sent: Article 1 and Article 2 use Atari 2600 Atlantis dataset.sent: Article 1 and Article 2 use Atari 2600 Tennis dataset.sent: Article 1 and Article 2 use Atari 2600 Video Pinball dataset.sent: Article 1 and Article 2 use Atari 2600 Up and Down dataset.sent: Article 1 and Article 2 use Atari 2600 Gopher dataset.sent: Article 1 and Article 2 use Atari 2600 Double Dunk dataset.sent: Article 1 and Article 2 use Atari 2600 Assault dataset.sent: Article 1 and Article 2 use Atari 2600 Road Runner dataset.sent: Article 1 and Article 2 use Atari 2600 Private Eye dataset.sent: Article 1 and Article 2 use Atari 2600 Gravitar dataset.sent: Article 1 and Article 2 use Atari 2600 Breakout dataset.sent: Article 1 and Article 2 use Atari 2600 Krull dataset.sent: Article 1 and Article 2 use Atari 2600 HERO dataset.sent: Article 1 and Article 2 use Atari 2600 Ice Hockey dataset.sent: Article 1 and Article 2 use Atari 2600 Asterix dataset.sent: Article 1 and Article 2 use Atari 2600 Time Pilot dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Seaquest dataset.sent: Article 1 and Article 2 use Atari 2600 Name This Game dataset.sent: Article 1 and Article 2 use Atari 2600 Chopper Command dataset.sent: Article 1 and Article 2 use Atari 2600 Kangaroo dataset.sent: Article 1 and Article 2 use Atari 2600 Zaxxon dataset.sent: Article 1 and Article 2 use Atari 2600 Bowling dataset.sent: Article 1 and Article 2 use Atari 2600 Enduro dataset.sent: Article 1 and Article 2 use Atari 2600 Wizard of Wor dataset.sent: Article 1 and Article 2 use Atari 2600 Ms  Pacman dataset.sent: Article 1 and Article 2 use Atari 2600 Pong dataset.sent: Article 1 and Article 2 use Atari 2600 Star Gunner dataset.sent: Article 1 and Article 2 use Atari 2600 Space Invaders dataset.sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Bank Heist dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Alien dataset.sent: Article 1 and Article 2 use Atari 2600 Amidar dataset.sent: Article 1 and Article 2 use Atari 2600 Centipede dataset.sent: Article 1 and Article 2 use Atari 2600 Beam Rider dataset.sent: Article 1 and Article 2 use Atari 2600 Demon Attack dataset.sent: Article 1 and Article 2 use Atari 2600 Crazy Climber dataset.sent: Article 1 and Article 2 use Atari 2600 James Bond dataset.sent: Article 1 and Article 2 use Atari 2600 Kung-Fu Master dataset.sent: Article 1 and Article 2 use Atari 2600 Asteroids dataset.sent: Article 1 and Article 2 use Atari 2600 Battle Zone dataset.sent: Article 1 and Article 2 use Atari 2600 Tutankham dataset.sent: Article 1 and Article 2 use Atari 2600 Robotank dataset.sent: Article 1 and Article 2 use Atari 2600 Boxing dataset.sent: Article 1 and Article 2 use Atari 2600 River Raid dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Atari 2600 Berzerk dataset.","sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses DDQN  tuned  hs for Atari Games. sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses Prior Duel hs for Atari Games. sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses DQN hs for Atari Games. sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses DQN noop for Atari Games."
907,907,14_paper_288,14_paper_64,1511,1543,1507.04296,1706.0809,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Asterix,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Bank Heist,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Boxing, and  Atari 2600 River Raid, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses Sarsa- for Atari Games. sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses Sarsa--EB for Atari Games."
908,908,14_paper_288,14_paper_0,1511,1562,1507.04296,1507.00814,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Asterix,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Bank Heist,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Boxing, and  Atari 2600 River Raid, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses MP-EB for Atari Games."
909,909,14_paper_288,14_paper_5,1511,15186,1507.04296,1704.04651,Atari Games,"sent: Article 1 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Asterix,  Atari 2600 Freeway,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Venture,  Atari 2600 Q Bert,  Atari 2600 Bank Heist,  Atari 2600 Montezuma s Revenge,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Boxing,  Atari 2600 River Raid, and  Atari 2600 Frostbite, while Article 2 uses Atari-57 dataset.","sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses Reactor for Atari Games."
910,910,14_paper_0,14_paper_467,1562,1563,1507.00814,1509.06461,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Berzerk,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Asterix,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Bank Heist,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Boxing, and  Atari 2600 River Raid.","sent: Article 1 uses MP-EB approach for Atari Games, while Article 2 uses DDQN  tuned  hs for Atari Games. sent: Article 1 uses MP-EB approach for Atari Games, while Article 2 uses Prior Duel hs for Atari Games. sent: Article 1 uses MP-EB approach for Atari Games, while Article 2 uses DQN hs for Atari Games. sent: Article 1 uses MP-EB approach for Atari Games, while Article 2 uses DQN noop for Atari Games."
911,911,14_paper_0,14_paper_212,1562,1564,1507.00814,1511.06581,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Berzerk,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Defender,  Atari 2600 Asterix,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Bank Heist,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Phoenix,  Atari 2600 Boxing, and  Atari 2600 River Raid.","sent: Article 1 uses MP-EB approach for Atari Games, while Article 2 uses Prior Duel hs for Atari Games. sent: Article 1 uses MP-EB approach for Atari Games, while Article 2 uses Prior Duel noop for Atari Games. sent: Article 1 uses MP-EB approach for Atari Games, while Article 2 uses Duel noop for Atari Games. sent: Article 1 uses MP-EB approach for Atari Games, while Article 2 uses Duel hs for Atari Games. sent: Article 1 uses MP-EB approach for Atari Games, while Article 2 uses DDQN  tuned  noop for Atari Games."
912,912,14_paper_0,14_paper_48,1562,1569,1507.00814,1207.4708,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Asterix,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Bank Heist,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Boxing, and  Atari 2600 River Raid.","sent: Article 1 uses MP-EB approach for Atari Games, while Article 2 uses Best linear for Atari Games."
913,913,14_paper_0,14_paper_309,1562,1576,1507.00814,1507.04296,Atari Games,"sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Asterix,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Bank Heist,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Boxing, and  Atari 2600 River Raid.","sent: Article 1 uses MP-EB approach for Atari Games, while Article 2 uses Gorila for Atari Games."
914,914,14_paper_0,14_paper_65,1562,15100,1507.00814,1706.0809,Atari Games,sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset.,"sent: Article 1 uses MP-EB approach for Atari Games, while Article 2 uses Sarsa- for Atari Games. sent: Article 1 uses MP-EB approach for Atari Games, while Article 2 uses Sarsa--EB for Atari Games."
915,915,14_paper_0,14_paper_5,1562,15186,1507.00814,1704.04651,Atari Games,"sent: Article 1 uses multiple datasets:  Atari 2600 Venture,  Atari 2600 Q Bert,  Atari 2600 Freeway,  Atari 2600 Montezuma s Revenge, and  Atari 2600 Frostbite, while Article 2 uses Atari-57 dataset.","sent: Article 1 uses MP-EB approach for Atari Games, while Article 2 uses Reactor for Atari Games."
916,916,14_paper_5,14_paper_273,15186,15187,1704.04651,1511.06581,Atari Games,"sent: Article 1 uses Atari-57 dataset, while Article 2 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Berzerk,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Defender,  Atari 2600 Asterix,  Atari 2600 Freeway,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Venture,  Atari 2600 Q Bert,  Atari 2600 Bank Heist,  Atari 2600 Montezuma s Revenge,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Phoenix,  Atari 2600 Boxing,  Atari 2600 River Raid, and  Atari 2600 Frostbite.","sent: Article 1 uses Reactor approach for Atari Games, while Article 2 uses Prior Duel hs for Atari Games. sent: Article 1 uses Reactor approach for Atari Games, while Article 2 uses Prior Duel noop for Atari Games. sent: Article 1 uses Reactor approach for Atari Games, while Article 2 uses Duel noop for Atari Games. sent: Article 1 uses Reactor approach for Atari Games, while Article 2 uses Duel hs for Atari Games. sent: Article 1 uses Reactor approach for Atari Games, while Article 2 uses DDQN  tuned  noop for Atari Games."
917,917,14_paper_5,14_paper_294,15186,15189,1704.04651,1507.04296,Atari Games,"sent: Article 1 uses Atari-57 dataset, while Article 2 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Asterix,  Atari 2600 Freeway,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Venture,  Atari 2600 Q Bert,  Atari 2600 Bank Heist,  Atari 2600 Montezuma s Revenge,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Boxing,  Atari 2600 River Raid, and  Atari 2600 Frostbite.","sent: Article 1 uses Reactor approach for Atari Games, while Article 2 uses Gorila for Atari Games."
918,918,14_paper_5,14_paper_389,15186,15191,1704.04651,1509.06461,Atari Games,"sent: Article 1 uses Atari-57 dataset, while Article 2 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Berzerk,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Asterix,  Atari 2600 Freeway,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Venture,  Atari 2600 Q Bert,  Atari 2600 Bank Heist,  Atari 2600 Montezuma s Revenge,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Boxing,  Atari 2600 River Raid, and  Atari 2600 Frostbite.","sent: Article 1 uses Reactor approach for Atari Games, while Article 2 uses DDQN  tuned  hs for Atari Games. sent: Article 1 uses Reactor approach for Atari Games, while Article 2 uses Prior Duel hs for Atari Games. sent: Article 1 uses Reactor approach for Atari Games, while Article 2 uses DQN hs for Atari Games. sent: Article 1 uses Reactor approach for Atari Games, while Article 2 uses DQN noop for Atari Games."
919,919,14_paper_5,14_paper_23,15186,15208,1704.04651,1207.4708,Atari Games,"sent: Article 1 uses Atari-57 dataset, while Article 2 uses multiple datasets:  Atari 2600 Fishing Derby,  Atari 2600 Atlantis,  Atari 2600 Tennis,  Atari 2600 Up and Down,  Atari 2600 Video Pinball,  Atari 2600 Gopher,  Atari 2600 Double Dunk,  Atari 2600 Assault,  Atari 2600 Road Runner,  Atari 2600 Private Eye,  Atari 2600 Gravitar,  Atari 2600 Breakout,  Atari 2600 Krull,  Atari 2600 HERO,  Atari 2600 Ice Hockey,  Atari 2600 Asterix,  Atari 2600 Freeway,  Atari 2600 Time Pilot,  Atari 2600 Seaquest,  Atari 2600 Name This Game,  Atari 2600 Chopper Command,  Atari 2600 Kangaroo,  Atari 2600 Zaxxon,  Atari 2600 Bowling,  Atari 2600 Enduro,  Atari 2600 Wizard of Wor,  Atari 2600 Ms  Pacman,  Atari 2600 Pong,  Atari 2600 Star Gunner,  Atari 2600 Space Invaders,  Atari 2600 Venture,  Atari 2600 Q Bert,  Atari 2600 Bank Heist,  Atari 2600 Montezuma s Revenge,  Atari 2600 Alien,  Atari 2600 Amidar,  Atari 2600 Centipede,  Atari 2600 Beam Rider,  Atari 2600 Demon Attack,  Atari 2600 Crazy Climber,  Atari 2600 James Bond,  Atari 2600 Kung-Fu Master,  Atari 2600 Asteroids,  Atari 2600 Battle Zone,  Atari 2600 Tutankham,  Atari 2600 Robotank,  Atari 2600 Boxing,  Atari 2600 River Raid, and  Atari 2600 Frostbite.","sent: Article 1 uses Reactor approach for Atari Games, while Article 2 uses Best linear for Atari Games."
920,920,14_paper_5,14_paper_4,15186,15212,1704.04651,1507.00814,Atari Games,"sent: Article 1 uses Atari-57 dataset, while Article 2 uses multiple datasets:  Atari 2600 Venture,  Atari 2600 Q Bert,  Atari 2600 Freeway,  Atari 2600 Montezuma s Revenge, and  Atari 2600 Frostbite.","sent: Article 1 uses Reactor approach for Atari Games, while Article 2 uses MP-EB for Atari Games."
921,921,14_paper_5,14_paper_62,15186,15234,1704.04651,1706.0809,Atari Games,"sent: Article 1 uses Atari-57 dataset, while Article 2 uses multiple datasets:  Atari 2600 Venture,  Atari 2600 Q Bert,  Atari 2600 Freeway,  Atari 2600 Montezuma s Revenge, and  Atari 2600 Frostbite.","sent: Article 1 uses Reactor approach for Atari Games, while Article 2 uses Sarsa- for Atari Games. sent: Article 1 uses Reactor approach for Atari Games, while Article 2 uses Sarsa--EB for Atari Games."
922,922,15_paper_1,15_paper_2,160,161,1807.06653,1511.05644,Unsupervised MNIST,sent: Article 1 and Article 2 use MNIST dataset.,"sent: Article 1 uses IIC approach for Unsupervised MNIST, while Article 2 uses Adversarial AE for Unsupervised MNIST."
923,923,15_paper_1,15_paper_0,160,162,1807.06653,1606.03657,Unsupervised MNIST,sent: Article 1 and Article 2 use MNIST dataset.,"sent: Article 1 uses IIC approach for Unsupervised MNIST, while Article 2 uses InfoGAN for Unsupervised MNIST."
924,924,15_paper_2,15_paper_0,161,162,1511.05644,1606.03657,Unsupervised MNIST,sent: Article 1 and Article 2 use MNIST dataset.,"sent: Article 1 uses Adversarial AE approach for Unsupervised image classification, while Article 2 uses InfoGAN for Unsupervised image classification. sent: Article 1 uses Adversarial AE approach for Unsupervised MNIST, while Article 2 uses InfoGAN for Unsupervised MNIST."
925,925,16_paper_0,16_paper_1,170,171,1606.03657,1511.05644,Unsupervised image classification,sent: Article 1 and Article 2 use MNIST dataset.,"sent: Article 1 uses InfoGAN approach for Unsupervised image classification, while Article 2 uses Adversarial AE for Unsupervised image classification. sent: Article 1 uses InfoGAN approach for Unsupervised MNIST, while Article 2 uses Adversarial AE for Unsupervised MNIST."
926,926,17_paper_8,17_paper_6,180,181,1509.01626,1811.09386,Text Classification,"sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset. sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification, and  Yelp Binary classification, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers.","sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Text Classification, while Article 2 uses EXAM for Text Classification."
927,927,17_paper_8,17_paper_0,180,183,1509.01626,1607.01759,Text Classification,"sent: Article 1 and Article 2 use Yelp Fine-grained classification dataset.sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset.sent: Article 1 and Article 2 use Yelp Binary classification dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Sogou News,  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers.","sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Text Classification, while Article 2 uses FastText for Text Classification. sent: Article 1 uses Char-level CNN approach for Text Classification, while Article 2 uses fastText for Text Classification."
928,928,17_paper_8,17_paper_3,180,188,1509.01626,1511.0863,Text Classification,"sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification, while Article 2 uses multiple datasets:  SST-2 Binary classification,  SST-5 Fine-grained classification, and  TREC-6.","sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses C-LSTM for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Text Classification, while Article 2 uses C-LSTM for Text Classification."
929,929,17_paper_6,17_paper_0,181,183,1811.09386,1607.01759,Text Classification,"sent: Article 1 and Article 2 use Amazon Review Polarity dataset.sent: Article 1 and Article 2 use Amazon Review Full dataset.sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset.sent: Article 1 and Article 2 use Yahoo  Answers dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Sogou News,  Yelp Fine-grained classification, and  Yelp Binary classification.","sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis. sent: Article 1 uses EXAM approach for Text Classification, while Article 2 uses FastText for Text Classification. sent: Article 1 uses EXAM approach for Text Classification, while Article 2 uses fastText for Text Classification."
930,930,17_paper_6,17_paper_7,181,185,1811.09386,1509.01626,Text Classification,"sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset. sent: Article 1 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers, while Article 2 uses multiple datasets:  Yelp Fine-grained classification, and  Yelp Binary classification.","sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis. sent: Article 1 uses EXAM approach for Text Classification, while Article 2 uses Char-level CNN for Text Classification."
931,931,17_paper_6,17_paper_3,181,188,1811.09386,1511.0863,Text Classification,"sent: Article 1 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers, while Article 2 uses multiple datasets:  SST-2 Binary classification,  SST-5 Fine-grained classification, and  TREC-6.","sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses C-LSTM for Sentiment Analysis. sent: Article 1 uses EXAM approach for Text Classification, while Article 2 uses C-LSTM for Text Classification."
932,932,17_paper_0,17_paper_7,183,185,1607.01759,1509.01626,Text Classification,"sent: Article 1 and Article 2 use Yelp Fine-grained classification dataset.sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset.sent: Article 1 and Article 2 use Yelp Binary classification dataset. sent: Article 1 uses multiple datasets:  Sogou News,  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis. sent: Article 1 uses FastText approach for Text Classification, while Article 2 uses Char-level CNN for Text Classification. sent: Article 1 uses fastText approach for Text Classification, while Article 2 uses Char-level CNN for Text Classification."
933,933,17_paper_0,17_paper_5,183,186,1607.01759,1811.09386,Text Classification,"sent: Article 1 and Article 2 use Amazon Review Polarity dataset.sent: Article 1 and Article 2 use Amazon Review Full dataset.sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset.sent: Article 1 and Article 2 use Yahoo  Answers dataset. sent: Article 1 uses multiple datasets:  Sogou News,  Yelp Fine-grained classification, and  Yelp Binary classification, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis. sent: Article 1 uses FastText approach for Text Classification, while Article 2 uses EXAM for Text Classification. sent: Article 1 uses fastText approach for Text Classification, while Article 2 uses EXAM for Text Classification."
934,934,17_paper_0,17_paper_3,183,188,1607.01759,1511.0863,Text Classification,"sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers, while Article 2 uses multiple datasets:  SST-2 Binary classification,  SST-5 Fine-grained classification, and  TREC-6.","sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses C-LSTM for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses C-LSTM for Sentiment Analysis. sent: Article 1 uses FastText approach for Text Classification, while Article 2 uses C-LSTM for Text Classification. sent: Article 1 uses fastText approach for Text Classification, while Article 2 uses C-LSTM for Text Classification."
935,935,18_paper_3,18_paper_10,190,191,1607.01759,1810.09311,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers, while Article 2 uses Multi-Domain Sentiment Dataset dataset.","sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses Distributional Correspondence Indexing for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses Distributional Correspondence Indexing for Sentiment Analysis."
936,936,18_paper_3,18_paper_24,190,192,1607.01759,1901.11504,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers, while Article 2 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification.","sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses MT-DNN for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses MT-DNN for Sentiment Analysis."
937,937,18_paper_3,18_paper_19,190,196,1607.01759,1511.0083,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers, while Article 2 uses Multi-Domain Sentiment Dataset dataset.","sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses VFAE for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses VFAE for Sentiment Analysis."
938,938,18_paper_3,18_paper_6,190,197,1607.01759,1807.0499,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers, while Article 2 uses multiple datasets:  MR, and  SST-5 Fine-grained classification.","sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses MEAN for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses MEAN for Sentiment Analysis."
939,939,18_paper_3,18_paper_26,190,199,1607.01759,1805.02474,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.","sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis."
940,940,18_paper_3,18_paper_25,190,1914,1607.01759,1412.1058,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers, while Article 2 uses IMDb dataset.","sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses seq2-bown-CNN for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses seq2-bown-CNN for Sentiment Analysis."
941,941,18_paper_3,18_paper_28,190,1922,1607.01759,1802.05365,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  ACL-ARC,  SQuAD2 0,  SST-5 Fine-grained classification, and  SQuAD1 1.","sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses BiLSTM-CRF ELMo for Sentiment Analysis. sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Sentiment Analysis. sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Sentiment Analysis. sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses  Lee et al   2017  ELMo for Sentiment Analysis. sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses BCN ELMo for Sentiment Analysis. sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses ESIM   ELMo Ensemble for Sentiment Analysis. sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses  He et al   2017    ELMo for Sentiment Analysis. sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses ESIM   ELMo for Sentiment Analysis. sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses BiLSTM-Attention   ELMo for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses BiLSTM-CRF ELMo for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses  Lee et al   2017  ELMo for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses BCN ELMo for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses ESIM   ELMo Ensemble for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses  He et al   2017    ELMo for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses ESIM   ELMo for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses BiLSTM-Attention   ELMo for Sentiment Analysis."
942,942,18_paper_3,18_paper_9,190,1927,1607.01759,1812.01207,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers, while Article 2 uses multiple datasets:  SST-2 Binary classification, and  SemEval 2018 Task 1E-c.","sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis."
943,943,18_paper_10,18_paper_24,191,192,1810.09311,1901.11504,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification.","sent: Article 1 uses Distributional Correspondence Indexing approach for Sentiment Analysis, while Article 2 uses MT-DNN for Sentiment Analysis."
944,944,18_paper_10,18_paper_7,191,193,1810.09311,1511.0863,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses multiple datasets:  SST-2 Binary classification,  SST-5 Fine-grained classification, and  TREC-6.","sent: Article 1 uses Distributional Correspondence Indexing approach for Sentiment Analysis, while Article 2 uses C-LSTM for Sentiment Analysis."
945,945,18_paper_10,18_paper_19,191,196,1810.09311,1511.0083,Sentiment Analysis,sent: Article 1 and Article 2 use Multi-Domain Sentiment Dataset dataset.,"sent: Article 1 uses Distributional Correspondence Indexing approach for Sentiment Analysis, while Article 2 uses VFAE for Sentiment Analysis."
946,946,18_paper_10,18_paper_6,191,197,1810.09311,1807.0499,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses multiple datasets:  MR, and  SST-5 Fine-grained classification.","sent: Article 1 uses Distributional Correspondence Indexing approach for Sentiment Analysis, while Article 2 uses MEAN for Sentiment Analysis."
947,947,18_paper_10,18_paper_26,191,199,1810.09311,1805.02474,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.","sent: Article 1 uses Distributional Correspondence Indexing approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis."
948,948,18_paper_10,18_paper_16,191,1910,1810.09311,1811.09386,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers.","sent: Article 1 uses Distributional Correspondence Indexing approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis."
949,949,18_paper_10,18_paper_17,191,1912,1810.09311,1509.01626,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification.","sent: Article 1 uses Distributional Correspondence Indexing approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis."
950,950,18_paper_10,18_paper_25,191,1914,1810.09311,1412.1058,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses IMDb dataset.","sent: Article 1 uses Distributional Correspondence Indexing approach for Sentiment Analysis, while Article 2 uses seq2-bown-CNN for Sentiment Analysis."
951,951,18_paper_10,18_paper_4,191,1915,1810.09311,1607.01759,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers.","sent: Article 1 uses Distributional Correspondence Indexing approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses Distributional Correspondence Indexing approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis."
952,952,18_paper_10,18_paper_28,191,1922,1810.09311,1802.05365,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses Distributional Correspondence Indexing approach for Sentiment Analysis, while Article 2 uses BiLSTM-CRF ELMo for Sentiment Analysis. sent: Article 1 uses Distributional Correspondence Indexing approach for Sentiment Analysis, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Sentiment Analysis. sent: Article 1 uses Distributional Correspondence Indexing approach for Sentiment Analysis, while Article 2 uses  Lee et al   2017  ELMo for Sentiment Analysis. sent: Article 1 uses Distributional Correspondence Indexing approach for Sentiment Analysis, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Sentiment Analysis. sent: Article 1 uses Distributional Correspondence Indexing approach for Sentiment Analysis, while Article 2 uses BCN ELMo for Sentiment Analysis. sent: Article 1 uses Distributional Correspondence Indexing approach for Sentiment Analysis, while Article 2 uses ESIM   ELMo Ensemble for Sentiment Analysis. sent: Article 1 uses Distributional Correspondence Indexing approach for Sentiment Analysis, while Article 2 uses  He et al   2017    ELMo for Sentiment Analysis. sent: Article 1 uses Distributional Correspondence Indexing approach for Sentiment Analysis, while Article 2 uses ESIM   ELMo for Sentiment Analysis. sent: Article 1 uses Distributional Correspondence Indexing approach for Sentiment Analysis, while Article 2 uses BiLSTM-Attention   ELMo for Sentiment Analysis."
953,953,18_paper_10,18_paper_9,191,1927,1810.09311,1812.01207,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses multiple datasets:  SST-2 Binary classification, and  SemEval 2018 Task 1E-c.","sent: Article 1 uses Distributional Correspondence Indexing approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis."
954,954,18_paper_24,18_paper_7,192,193,1901.11504,1511.0863,Sentiment Analysis,"sent: Article 1 and Article 2 use SST-2 Binary classification dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  Quora Question Pairs,  SciTail, and  SNLI, while Article 2 uses multiple datasets:  SST-5 Fine-grained classification, and  TREC-6.","sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses C-LSTM for Sentiment Analysis."
955,955,18_paper_24,18_paper_13,192,194,1901.11504,1810.09311,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses Multi-Domain Sentiment Dataset dataset.","sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses Distributional Correspondence Indexing for Sentiment Analysis."
956,956,18_paper_24,18_paper_19,192,196,1901.11504,1511.0083,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses Multi-Domain Sentiment Dataset dataset.","sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses VFAE for Sentiment Analysis."
957,957,18_paper_24,18_paper_6,192,197,1901.11504,1807.0499,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  MR, and  SST-5 Fine-grained classification.","sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses MEAN for Sentiment Analysis."
958,958,18_paper_24,18_paper_26,192,199,1901.11504,1805.02474,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.","sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis."
959,959,18_paper_24,18_paper_16,192,1910,1901.11504,1811.09386,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers.","sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis."
960,960,18_paper_24,18_paper_17,192,1912,1901.11504,1509.01626,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification.","sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis."
961,961,18_paper_24,18_paper_25,192,1914,1901.11504,1412.1058,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses IMDb dataset.","sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses seq2-bown-CNN for Sentiment Analysis."
962,962,18_paper_24,18_paper_4,192,1915,1901.11504,1607.01759,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers.","sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis."
963,963,18_paper_24,18_paper_9,192,1927,1901.11504,1812.01207,Sentiment Analysis,"sent: Article 1 and Article 2 use SST-2 Binary classification dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  Quora Question Pairs,  SciTail, and  SNLI, while Article 2 uses SemEval 2018 Task 1E-c dataset.","sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis."
964,964,18_paper_7,18_paper_13,193,194,1511.0863,1810.09311,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  SST-2 Binary classification,  SST-5 Fine-grained classification, and  TREC-6, while Article 2 uses Multi-Domain Sentiment Dataset dataset.","sent: Article 1 uses C-LSTM approach for Sentiment Analysis, while Article 2 uses Distributional Correspondence Indexing for Sentiment Analysis."
965,965,18_paper_7,18_paper_19,193,196,1511.0863,1511.0083,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  SST-2 Binary classification,  SST-5 Fine-grained classification, and  TREC-6, while Article 2 uses Multi-Domain Sentiment Dataset dataset.","sent: Article 1 uses C-LSTM approach for Sentiment Analysis, while Article 2 uses VFAE for Sentiment Analysis."
966,966,18_paper_7,18_paper_6,193,197,1511.0863,1807.0499,Sentiment Analysis,"sent: Article 1 and Article 2 use SST-5 Fine-grained classification dataset. sent: Article 1 uses multiple datasets:  SST-2 Binary classification, and  TREC-6, while Article 2 uses MR dataset.","sent: Article 1 uses C-LSTM approach for Sentiment Analysis, while Article 2 uses MEAN for Sentiment Analysis."
967,967,18_paper_7,18_paper_26,193,199,1511.0863,1805.02474,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  SST-2 Binary classification,  SST-5 Fine-grained classification, and  TREC-6, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.","sent: Article 1 uses C-LSTM approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis."
968,968,18_paper_7,18_paper_16,193,1910,1511.0863,1811.09386,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  SST-2 Binary classification,  SST-5 Fine-grained classification, and  TREC-6, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers.","sent: Article 1 uses C-LSTM approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis. sent: Article 1 uses C-LSTM approach for Text Classification, while Article 2 uses EXAM for Text Classification."
969,969,18_paper_7,18_paper_17,193,1912,1511.0863,1509.01626,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  SST-2 Binary classification,  SST-5 Fine-grained classification, and  TREC-6, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification.","sent: Article 1 uses C-LSTM approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis. sent: Article 1 uses C-LSTM approach for Text Classification, while Article 2 uses Char-level CNN for Text Classification."
970,970,18_paper_7,18_paper_25,193,1914,1511.0863,1412.1058,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  SST-2 Binary classification,  SST-5 Fine-grained classification, and  TREC-6, while Article 2 uses IMDb dataset.","sent: Article 1 uses C-LSTM approach for Sentiment Analysis, while Article 2 uses seq2-bown-CNN for Sentiment Analysis."
971,971,18_paper_7,18_paper_4,193,1915,1511.0863,1607.01759,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  SST-2 Binary classification,  SST-5 Fine-grained classification, and  TREC-6, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers.","sent: Article 1 uses C-LSTM approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses C-LSTM approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis. sent: Article 1 uses C-LSTM approach for Text Classification, while Article 2 uses FastText for Text Classification. sent: Article 1 uses C-LSTM approach for Text Classification, while Article 2 uses fastText for Text Classification."
972,972,18_paper_7,18_paper_28,193,1922,1511.0863,1802.05365,Sentiment Analysis,"sent: Article 1 and Article 2 use SST-5 Fine-grained classification dataset. sent: Article 1 uses multiple datasets:  SST-2 Binary classification, and  TREC-6, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  ACL-ARC,  SQuAD2 0, and  SQuAD1 1.","sent: Article 1 uses C-LSTM approach for Sentiment Analysis, while Article 2 uses BiLSTM-CRF ELMo for Sentiment Analysis. sent: Article 1 uses C-LSTM approach for Sentiment Analysis, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Sentiment Analysis. sent: Article 1 uses C-LSTM approach for Sentiment Analysis, while Article 2 uses  Lee et al   2017  ELMo for Sentiment Analysis. sent: Article 1 uses C-LSTM approach for Sentiment Analysis, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Sentiment Analysis. sent: Article 1 uses C-LSTM approach for Sentiment Analysis, while Article 2 uses BCN ELMo for Sentiment Analysis. sent: Article 1 uses C-LSTM approach for Sentiment Analysis, while Article 2 uses ESIM   ELMo Ensemble for Sentiment Analysis. sent: Article 1 uses C-LSTM approach for Sentiment Analysis, while Article 2 uses  He et al   2017    ELMo for Sentiment Analysis. sent: Article 1 uses C-LSTM approach for Sentiment Analysis, while Article 2 uses ESIM   ELMo for Sentiment Analysis. sent: Article 1 uses C-LSTM approach for Sentiment Analysis, while Article 2 uses BiLSTM-Attention   ELMo for Sentiment Analysis."
973,973,18_paper_7,18_paper_9,193,1927,1511.0863,1812.01207,Sentiment Analysis,"sent: Article 1 and Article 2 use SST-2 Binary classification dataset. sent: Article 1 uses multiple datasets:  SST-5 Fine-grained classification, and  TREC-6, while Article 2 uses SemEval 2018 Task 1E-c dataset.","sent: Article 1 uses C-LSTM approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis."
974,974,18_paper_19,18_paper_6,196,197,1511.0083,1807.0499,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses multiple datasets:  MR, and  SST-5 Fine-grained classification.","sent: Article 1 uses VFAE approach for Sentiment Analysis, while Article 2 uses MEAN for Sentiment Analysis."
975,975,18_paper_19,18_paper_12,196,198,1511.0083,1810.09311,Sentiment Analysis,sent: Article 1 and Article 2 use Multi-Domain Sentiment Dataset dataset.,"sent: Article 1 uses VFAE approach for Sentiment Analysis, while Article 2 uses Distributional Correspondence Indexing for Sentiment Analysis."
976,976,18_paper_19,18_paper_26,196,199,1511.0083,1805.02474,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.","sent: Article 1 uses VFAE approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis."
977,977,18_paper_19,18_paper_16,196,1910,1511.0083,1811.09386,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers.","sent: Article 1 uses VFAE approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis."
978,978,18_paper_19,18_paper_17,196,1912,1511.0083,1509.01626,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification.","sent: Article 1 uses VFAE approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis."
979,979,18_paper_19,18_paper_8,196,1913,1511.0083,1511.0863,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses multiple datasets:  SST-2 Binary classification,  SST-5 Fine-grained classification, and  TREC-6.","sent: Article 1 uses VFAE approach for Sentiment Analysis, while Article 2 uses C-LSTM for Sentiment Analysis."
980,980,18_paper_19,18_paper_25,196,1914,1511.0083,1412.1058,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses IMDb dataset.","sent: Article 1 uses VFAE approach for Sentiment Analysis, while Article 2 uses seq2-bown-CNN for Sentiment Analysis."
981,981,18_paper_19,18_paper_4,196,1915,1511.0083,1607.01759,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers.","sent: Article 1 uses VFAE approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses VFAE approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis."
982,982,18_paper_19,18_paper_28,196,1922,1511.0083,1802.05365,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses VFAE approach for Sentiment Analysis, while Article 2 uses BiLSTM-CRF ELMo for Sentiment Analysis. sent: Article 1 uses VFAE approach for Sentiment Analysis, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Sentiment Analysis. sent: Article 1 uses VFAE approach for Sentiment Analysis, while Article 2 uses  Lee et al   2017  ELMo for Sentiment Analysis. sent: Article 1 uses VFAE approach for Sentiment Analysis, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Sentiment Analysis. sent: Article 1 uses VFAE approach for Sentiment Analysis, while Article 2 uses BCN ELMo for Sentiment Analysis. sent: Article 1 uses VFAE approach for Sentiment Analysis, while Article 2 uses ESIM   ELMo Ensemble for Sentiment Analysis. sent: Article 1 uses VFAE approach for Sentiment Analysis, while Article 2 uses  He et al   2017    ELMo for Sentiment Analysis. sent: Article 1 uses VFAE approach for Sentiment Analysis, while Article 2 uses ESIM   ELMo for Sentiment Analysis. sent: Article 1 uses VFAE approach for Sentiment Analysis, while Article 2 uses BiLSTM-Attention   ELMo for Sentiment Analysis."
983,983,18_paper_19,18_paper_9,196,1927,1511.0083,1812.01207,Sentiment Analysis,"sent: Article 1 uses Multi-Domain Sentiment Dataset dataset, while Article 2 uses multiple datasets:  SST-2 Binary classification, and  SemEval 2018 Task 1E-c.","sent: Article 1 uses VFAE approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis."
984,984,18_paper_6,18_paper_12,197,198,1807.0499,1810.09311,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  MR, and  SST-5 Fine-grained classification, while Article 2 uses Multi-Domain Sentiment Dataset dataset.","sent: Article 1 uses MEAN approach for Sentiment Analysis, while Article 2 uses Distributional Correspondence Indexing for Sentiment Analysis."
985,985,18_paper_6,18_paper_26,197,199,1807.0499,1805.02474,Sentiment Analysis,"sent: Article 1 and Article 2 use MR dataset. sent: Article 1 uses SST-5 Fine-grained classification dataset, while Article 2 uses multiple datasets:  IMDb,  CoNLL 2003  English , and  Penn Treebank.","sent: Article 1 uses MEAN approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis."
986,986,18_paper_6,18_paper_16,197,1910,1807.0499,1811.09386,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  MR, and  SST-5 Fine-grained classification, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers.","sent: Article 1 uses MEAN approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis."
987,987,18_paper_6,18_paper_22,197,1911,1807.0499,1511.0083,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  MR, and  SST-5 Fine-grained classification, while Article 2 uses Multi-Domain Sentiment Dataset dataset.","sent: Article 1 uses MEAN approach for Sentiment Analysis, while Article 2 uses VFAE for Sentiment Analysis."
988,988,18_paper_6,18_paper_17,197,1912,1807.0499,1509.01626,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  MR, and  SST-5 Fine-grained classification, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification.","sent: Article 1 uses MEAN approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis."
989,989,18_paper_6,18_paper_8,197,1913,1807.0499,1511.0863,Sentiment Analysis,"sent: Article 1 and Article 2 use SST-5 Fine-grained classification dataset. sent: Article 1 uses MR dataset, while Article 2 uses multiple datasets:  SST-2 Binary classification, and  TREC-6.","sent: Article 1 uses MEAN approach for Sentiment Analysis, while Article 2 uses C-LSTM for Sentiment Analysis."
990,990,18_paper_6,18_paper_25,197,1914,1807.0499,1412.1058,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  MR, and  SST-5 Fine-grained classification, while Article 2 uses IMDb dataset.","sent: Article 1 uses MEAN approach for Sentiment Analysis, while Article 2 uses seq2-bown-CNN for Sentiment Analysis."
991,991,18_paper_6,18_paper_4,197,1915,1807.0499,1607.01759,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  MR, and  SST-5 Fine-grained classification, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers.","sent: Article 1 uses MEAN approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses MEAN approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis."
992,992,18_paper_6,18_paper_28,197,1922,1807.0499,1802.05365,Sentiment Analysis,"sent: Article 1 and Article 2 use SST-5 Fine-grained classification dataset. sent: Article 1 uses MR dataset, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  ACL-ARC,  SQuAD2 0, and  SQuAD1 1.","sent: Article 1 uses MEAN approach for Sentiment Analysis, while Article 2 uses BiLSTM-CRF ELMo for Sentiment Analysis. sent: Article 1 uses MEAN approach for Sentiment Analysis, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Sentiment Analysis. sent: Article 1 uses MEAN approach for Sentiment Analysis, while Article 2 uses  Lee et al   2017  ELMo for Sentiment Analysis. sent: Article 1 uses MEAN approach for Sentiment Analysis, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Sentiment Analysis. sent: Article 1 uses MEAN approach for Sentiment Analysis, while Article 2 uses BCN ELMo for Sentiment Analysis. sent: Article 1 uses MEAN approach for Sentiment Analysis, while Article 2 uses ESIM   ELMo Ensemble for Sentiment Analysis. sent: Article 1 uses MEAN approach for Sentiment Analysis, while Article 2 uses  He et al   2017    ELMo for Sentiment Analysis. sent: Article 1 uses MEAN approach for Sentiment Analysis, while Article 2 uses ESIM   ELMo for Sentiment Analysis. sent: Article 1 uses MEAN approach for Sentiment Analysis, while Article 2 uses BiLSTM-Attention   ELMo for Sentiment Analysis."
993,993,18_paper_6,18_paper_9,197,1927,1807.0499,1812.01207,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  MR, and  SST-5 Fine-grained classification, while Article 2 uses multiple datasets:  SST-2 Binary classification, and  SemEval 2018 Task 1E-c.","sent: Article 1 uses MEAN approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis."
994,994,18_paper_26,18_paper_16,199,1910,1805.02474,1811.09386,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers.","sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis."
995,995,18_paper_26,18_paper_22,199,1911,1805.02474,1511.0083,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses Multi-Domain Sentiment Dataset dataset.","sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses VFAE for Sentiment Analysis."
996,996,18_paper_26,18_paper_17,199,1912,1805.02474,1509.01626,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification.","sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis."
997,997,18_paper_26,18_paper_8,199,1913,1805.02474,1511.0863,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses multiple datasets:  SST-2 Binary classification,  SST-5 Fine-grained classification, and  TREC-6.","sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses C-LSTM for Sentiment Analysis."
998,998,18_paper_26,18_paper_25,199,1914,1805.02474,1412.1058,Sentiment Analysis,"sent: Article 1 and Article 2 use IMDb dataset. sent: Article 1 uses multiple datasets:  MR,  CoNLL 2003  English , and  Penn Treebank, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses seq2-bown-CNN for Sentiment Analysis."
999,999,18_paper_26,18_paper_4,199,1915,1805.02474,1607.01759,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers.","sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis."
1000,1000,18_paper_26,18_paper_11,199,1921,1805.02474,1810.09311,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses Multi-Domain Sentiment Dataset dataset.","sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses Distributional Correspondence Indexing for Sentiment Analysis."
1001,1001,18_paper_26,18_paper_28,199,1922,1805.02474,1802.05365,Sentiment Analysis,"sent: Article 1 and Article 2 use CoNLL 2003  English  dataset. sent: Article 1 uses multiple datasets:  MR,  IMDb, and  Penn Treebank, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  ACL-ARC,  SQuAD2 0,  SST-5 Fine-grained classification, and  SQuAD1 1.","sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses BiLSTM-CRF ELMo for Sentiment Analysis. sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Sentiment Analysis. sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses  Lee et al   2017  ELMo for Sentiment Analysis. sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Sentiment Analysis. sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses BCN ELMo for Sentiment Analysis. sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses ESIM   ELMo Ensemble for Sentiment Analysis. sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses  He et al   2017    ELMo for Sentiment Analysis. sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses ESIM   ELMo for Sentiment Analysis. sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses BiLSTM-Attention   ELMo for Sentiment Analysis. sent: Article 1 uses S-LSTM approach for Named Entity Recognition  NER , while Article 2 uses BiLSTM-CRF ELMo for Named Entity Recognition  NER ."
1002,1002,18_paper_26,18_paper_5,199,1925,1805.02474,1807.0499,Sentiment Analysis,"sent: Article 1 and Article 2 use MR dataset. sent: Article 1 uses multiple datasets:  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses SST-5 Fine-grained classification dataset.","sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses MEAN for Sentiment Analysis."
1003,1003,18_paper_26,18_paper_9,199,1927,1805.02474,1812.01207,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses multiple datasets:  SST-2 Binary classification, and  SemEval 2018 Task 1E-c.","sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis."
1004,1004,18_paper_16,18_paper_22,1910,1911,1811.09386,1511.0083,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers, while Article 2 uses Multi-Domain Sentiment Dataset dataset.","sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses VFAE for Sentiment Analysis."
1005,1005,18_paper_16,18_paper_25,1910,1914,1811.09386,1412.1058,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers, while Article 2 uses IMDb dataset.","sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses seq2-bown-CNN for Sentiment Analysis."
1006,1006,18_paper_16,18_paper_11,1910,1921,1811.09386,1810.09311,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers, while Article 2 uses Multi-Domain Sentiment Dataset dataset.","sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses Distributional Correspondence Indexing for Sentiment Analysis."
1007,1007,18_paper_16,18_paper_28,1910,1922,1811.09386,1802.05365,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  ACL-ARC,  SQuAD2 0,  SST-5 Fine-grained classification, and  SQuAD1 1.","sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses BiLSTM-CRF ELMo for Sentiment Analysis. sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Sentiment Analysis. sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses  Lee et al   2017  ELMo for Sentiment Analysis. sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Sentiment Analysis. sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses BCN ELMo for Sentiment Analysis. sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses ESIM   ELMo Ensemble for Sentiment Analysis. sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses  He et al   2017    ELMo for Sentiment Analysis. sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses ESIM   ELMo for Sentiment Analysis. sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses BiLSTM-Attention   ELMo for Sentiment Analysis."
1008,1008,18_paper_16,18_paper_27,1910,1924,1811.09386,1805.02474,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.","sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis."
1009,1009,18_paper_16,18_paper_5,1910,1925,1811.09386,1807.0499,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers, while Article 2 uses multiple datasets:  MR, and  SST-5 Fine-grained classification.","sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses MEAN for Sentiment Analysis."
1010,1010,18_paper_16,18_paper_9,1910,1927,1811.09386,1812.01207,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers, while Article 2 uses multiple datasets:  SST-2 Binary classification, and  SemEval 2018 Task 1E-c.","sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis."
1011,1011,18_paper_17,18_paper_25,1912,1914,1509.01626,1412.1058,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification, while Article 2 uses IMDb dataset.","sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses seq2-bown-CNN for Sentiment Analysis."
1012,1012,18_paper_17,18_paper_23,1912,1918,1509.01626,1511.0083,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification, while Article 2 uses Multi-Domain Sentiment Dataset dataset.","sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses VFAE for Sentiment Analysis."
1013,1013,18_paper_17,18_paper_11,1912,1921,1509.01626,1810.09311,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification, while Article 2 uses Multi-Domain Sentiment Dataset dataset.","sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses Distributional Correspondence Indexing for Sentiment Analysis."
1014,1014,18_paper_17,18_paper_28,1912,1922,1509.01626,1802.05365,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  ACL-ARC,  SQuAD2 0,  SST-5 Fine-grained classification, and  SQuAD1 1.","sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses BiLSTM-CRF ELMo for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses  Lee et al   2017  ELMo for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses BCN ELMo for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses ESIM   ELMo Ensemble for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses  He et al   2017    ELMo for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses ESIM   ELMo for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses BiLSTM-Attention   ELMo for Sentiment Analysis."
1015,1015,18_paper_17,18_paper_27,1912,1924,1509.01626,1805.02474,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.","sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis."
1016,1016,18_paper_17,18_paper_5,1912,1925,1509.01626,1807.0499,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification, while Article 2 uses multiple datasets:  MR, and  SST-5 Fine-grained classification.","sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses MEAN for Sentiment Analysis."
1017,1017,18_paper_17,18_paper_9,1912,1927,1509.01626,1812.01207,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification, while Article 2 uses multiple datasets:  SST-2 Binary classification, and  SemEval 2018 Task 1E-c.","sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis."
1018,1018,18_paper_25,18_paper_4,1914,1915,1412.1058,1607.01759,Sentiment Analysis,"sent: Article 1 uses IMDb dataset, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers.","sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis."
1019,1019,18_paper_25,18_paper_18,1914,1916,1412.1058,1509.01626,Sentiment Analysis,"sent: Article 1 uses IMDb dataset, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification.","sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis."
1020,1020,18_paper_25,18_paper_15,1914,1917,1412.1058,1811.09386,Sentiment Analysis,"sent: Article 1 uses IMDb dataset, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers.","sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis."
1021,1021,18_paper_25,18_paper_23,1914,1918,1412.1058,1511.0083,Sentiment Analysis,"sent: Article 1 uses IMDb dataset, while Article 2 uses Multi-Domain Sentiment Dataset dataset.","sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses VFAE for Sentiment Analysis."
1022,1022,18_paper_25,18_paper_11,1914,1921,1412.1058,1810.09311,Sentiment Analysis,"sent: Article 1 uses IMDb dataset, while Article 2 uses Multi-Domain Sentiment Dataset dataset.","sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses Distributional Correspondence Indexing for Sentiment Analysis."
1023,1023,18_paper_25,18_paper_28,1914,1922,1412.1058,1802.05365,Sentiment Analysis,"sent: Article 1 uses IMDb dataset, while Article 2 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC.","sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses BiLSTM-CRF ELMo for Sentiment Analysis. sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Sentiment Analysis. sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses  Lee et al   2017  ELMo for Sentiment Analysis. sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Sentiment Analysis. sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses BCN ELMo for Sentiment Analysis. sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses ESIM   ELMo Ensemble for Sentiment Analysis. sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses  He et al   2017    ELMo for Sentiment Analysis. sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses ESIM   ELMo for Sentiment Analysis. sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses BiLSTM-Attention   ELMo for Sentiment Analysis."
1024,1024,18_paper_25,18_paper_27,1914,1924,1412.1058,1805.02474,Sentiment Analysis,"sent: Article 1 and Article 2 use IMDb dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MR,  CoNLL 2003  English , and  Penn Treebank.","sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis."
1025,1025,18_paper_25,18_paper_5,1914,1925,1412.1058,1807.0499,Sentiment Analysis,"sent: Article 1 uses IMDb dataset, while Article 2 uses multiple datasets:  MR, and  SST-5 Fine-grained classification.","sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses MEAN for Sentiment Analysis."
1026,1026,18_paper_25,18_paper_9,1914,1927,1412.1058,1812.01207,Sentiment Analysis,"sent: Article 1 uses IMDb dataset, while Article 2 uses multiple datasets:  SST-2 Binary classification, and  SemEval 2018 Task 1E-c.","sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis."
1027,1027,18_paper_28,18_paper_1,1922,1923,1802.05365,1607.01759,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  ACL-ARC,  SQuAD2 0,  SST-5 Fine-grained classification, and  SQuAD1 1, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses BiLSTM-CRF ELMo approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis. sent: Article 1 uses  Lee et al   2017  ELMo approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses  Lee et al   2017  ELMo approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis. sent: Article 1 uses BCN ELMo approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses BCN ELMo approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis. sent: Article 1 uses ESIM   ELMo Ensemble approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses ESIM   ELMo Ensemble approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis. sent: Article 1 uses  He et al   2017    ELMo approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses  He et al   2017    ELMo approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis. sent: Article 1 uses ESIM   ELMo approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses ESIM   ELMo approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis."
1028,1028,18_paper_28,18_paper_27,1922,1924,1802.05365,1805.02474,Sentiment Analysis,"sent: Article 1 and Article 2 use CoNLL 2003  English  dataset. sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  ACL-ARC,  SQuAD2 0,  SST-5 Fine-grained classification, and  SQuAD1 1, while Article 2 uses multiple datasets:  MR,  IMDb, and  Penn Treebank.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis. sent: Article 1 uses  Lee et al   2017  ELMo approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis. sent: Article 1 uses BCN ELMo approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis. sent: Article 1 uses ESIM   ELMo Ensemble approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis. sent: Article 1 uses  He et al   2017    ELMo approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis. sent: Article 1 uses ESIM   ELMo approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis. sent: Article 1 uses BiLSTM-CRF ELMo approach for Named Entity Recognition  NER , while Article 2 uses S-LSTM for Named Entity Recognition  NER ."
1029,1029,18_paper_28,18_paper_5,1922,1925,1802.05365,1807.0499,Sentiment Analysis,"sent: Article 1 and Article 2 use SST-5 Fine-grained classification dataset. sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  ACL-ARC,  SQuAD2 0, and  SQuAD1 1, while Article 2 uses MR dataset.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Sentiment Analysis, while Article 2 uses MEAN for Sentiment Analysis. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Sentiment Analysis, while Article 2 uses MEAN for Sentiment Analysis. sent: Article 1 uses  Lee et al   2017  ELMo approach for Sentiment Analysis, while Article 2 uses MEAN for Sentiment Analysis. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Sentiment Analysis, while Article 2 uses MEAN for Sentiment Analysis. sent: Article 1 uses BCN ELMo approach for Sentiment Analysis, while Article 2 uses MEAN for Sentiment Analysis. sent: Article 1 uses ESIM   ELMo Ensemble approach for Sentiment Analysis, while Article 2 uses MEAN for Sentiment Analysis. sent: Article 1 uses  He et al   2017    ELMo approach for Sentiment Analysis, while Article 2 uses MEAN for Sentiment Analysis. sent: Article 1 uses ESIM   ELMo approach for Sentiment Analysis, while Article 2 uses MEAN for Sentiment Analysis. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Sentiment Analysis, while Article 2 uses MEAN for Sentiment Analysis."
1030,1030,18_paper_28,18_paper_21,1922,1926,1802.05365,1511.0083,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  CoNLL 2003  English ,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 uses Multi-Domain Sentiment Dataset dataset.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Sentiment Analysis, while Article 2 uses VFAE for Sentiment Analysis. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Sentiment Analysis, while Article 2 uses VFAE for Sentiment Analysis. sent: Article 1 uses  Lee et al   2017  ELMo approach for Sentiment Analysis, while Article 2 uses VFAE for Sentiment Analysis. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Sentiment Analysis, while Article 2 uses VFAE for Sentiment Analysis. sent: Article 1 uses BCN ELMo approach for Sentiment Analysis, while Article 2 uses VFAE for Sentiment Analysis. sent: Article 1 uses ESIM   ELMo Ensemble approach for Sentiment Analysis, while Article 2 uses VFAE for Sentiment Analysis. sent: Article 1 uses  He et al   2017    ELMo approach for Sentiment Analysis, while Article 2 uses VFAE for Sentiment Analysis. sent: Article 1 uses ESIM   ELMo approach for Sentiment Analysis, while Article 2 uses VFAE for Sentiment Analysis. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Sentiment Analysis, while Article 2 uses VFAE for Sentiment Analysis."
1031,1031,18_paper_28,18_paper_9,1922,1927,1802.05365,1812.01207,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  ACL-ARC,  SQuAD2 0,  SST-5 Fine-grained classification, and  SQuAD1 1, while Article 2 uses multiple datasets:  SST-2 Binary classification, and  SemEval 2018 Task 1E-c.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis. sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis. sent: Article 1 uses  Lee et al   2017  ELMo approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis. sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis. sent: Article 1 uses BCN ELMo approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis. sent: Article 1 uses ESIM   ELMo Ensemble approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis. sent: Article 1 uses  He et al   2017    ELMo approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis. sent: Article 1 uses ESIM   ELMo approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis. sent: Article 1 uses BiLSTM-Attention   ELMo approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis."
1032,1032,18_paper_9,18_paper_2,1927,1928,1812.01207,1607.01759,Sentiment Analysis,"sent: Article 1 uses multiple datasets:  SST-2 Binary classification, and  SemEval 2018 Task 1E-c, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers.","sent: Article 1 uses Transformer  finetune  approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses Transformer  finetune  approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis."
1033,1033,20_paper_0,20_paper_2,210,211,1603.05959,1505.0354,Brain Tumor Segmentation,"sent: Article 1 uses BRATS-2015 dataset, while Article 2 uses BRATS-2013 dataset.","sent: Article 1 uses Multi-Scale 3D   FC-CRF approach for Brain Tumor Segmentation, while Article 2 uses InputCascadeCNN for Brain Tumor Segmentation."
1034,1034,20_paper_0,20_paper_1,210,212,1603.05959,1810.11654,Brain Tumor Segmentation,"sent: Article 1 uses BRATS-2015 dataset, while Article 2 uses BRATS 2018 dataset.","sent: Article 1 uses Multi-Scale 3D   FC-CRF approach for Brain Tumor Segmentation, while Article 2 uses NVDLMED for Brain Tumor Segmentation."
1035,1035,20_paper_2,20_paper_1,211,212,1505.0354,1810.11654,Brain Tumor Segmentation,"sent: Article 1 uses BRATS-2013 dataset, while Article 2 uses BRATS 2018 dataset.","sent: Article 1 uses InputCascadeCNN approach for Brain Tumor Segmentation, while Article 2 uses NVDLMED for Brain Tumor Segmentation."
1036,1036,21_paper_11,21_paper_7,220,221,1606.02185,1703.034,Few-Shot Image Classification,"sent: Article 1 and Article 2 use OMNIGLOT - 1-Shot Learning dataset.sent: Article 1 and Article 2 use OMNIGLOT - 5-Shot Learning dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Mini-ImageNet - 1-Shot Learning, and  Mini-ImageNet - 5-Shot Learning.","sent: Article 1 uses Neural Statistician approach for Few-Shot Image Classification, while Article 2 uses MAML for Few-Shot Image Classification."
1037,1037,21_paper_11,21_paper_2,220,222,1606.02185,1812.02391,Few-Shot Image Classification,"sent: Article 1 uses multiple datasets:  OMNIGLOT - 1-Shot Learning, and  OMNIGLOT - 5-Shot Learning, while Article 2 uses multiple datasets:  Mini-ImageNet - 1-Shot Learning, and  Mini-ImageNet - 5-Shot Learning.","sent: Article 1 uses Neural Statistician approach for Few-Shot Image Classification, while Article 2 uses MTL for Few-Shot Image Classification."
1038,1038,21_paper_11,21_paper_4,220,223,1606.02185,1606.0408,Few-Shot Image Classification,"sent: Article 1 and Article 2 use OMNIGLOT - 1-Shot Learning dataset.sent: Article 1 and Article 2 use OMNIGLOT - 5-Shot Learning dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Mini-ImageNet - 1-Shot Learning, and  Mini-ImageNet - 5-Shot Learning.","sent: Article 1 uses Neural Statistician approach for Few-Shot Image Classification, while Article 2 uses Matching Nets   C64F feature extractor for Few-Shot Image Classification. sent: Article 1 uses Neural Statistician approach for Few-Shot Image Classification, while Article 2 uses Matching Nets for Few-Shot Image Classification."
1039,1039,21_paper_11,21_paper_0,220,229,1606.02185,1409.8403,Few-Shot Image Classification,"sent: Article 1 uses multiple datasets:  OMNIGLOT - 1-Shot Learning, and  OMNIGLOT - 5-Shot Learning, while Article 2 uses CUB-200 - 0-Shot Learning dataset.","sent: Article 1 uses Neural Statistician approach for Few-Shot Image Classification, while Article 2 uses SJE for Few-Shot Image Classification."
1040,1040,21_paper_7,21_paper_2,221,222,1703.034,1812.02391,Few-Shot Image Classification,"sent: Article 1 and Article 2 use Mini-ImageNet - 1-Shot Learning dataset.sent: Article 1 and Article 2 use Mini-ImageNet - 5-Shot Learning dataset. sent: Article 1 uses multiple datasets:  OMNIGLOT - 1-Shot Learning, and  OMNIGLOT - 5-Shot Learning, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses MAML approach for Few-Shot Image Classification, while Article 2 uses MTL for Few-Shot Image Classification."
1041,1041,21_paper_7,21_paper_4,221,223,1703.034,1606.0408,Few-Shot Image Classification,sent: Article 1 and Article 2 use OMNIGLOT - 1-Shot Learning dataset.sent: Article 1 and Article 2 use OMNIGLOT - 5-Shot Learning dataset.sent: Article 1 and Article 2 use Mini-ImageNet - 1-Shot Learning dataset.sent: Article 1 and Article 2 use Mini-ImageNet - 5-Shot Learning dataset.,"sent: Article 1 uses MAML approach for Few-Shot Image Classification, while Article 2 uses Matching Nets   C64F feature extractor for Few-Shot Image Classification. sent: Article 1 uses MAML approach for Few-Shot Image Classification, while Article 2 uses Matching Nets for Few-Shot Image Classification."
1042,1042,21_paper_7,21_paper_12,221,224,1703.034,1606.02185,Few-Shot Image Classification,"sent: Article 1 and Article 2 use OMNIGLOT - 1-Shot Learning dataset.sent: Article 1 and Article 2 use OMNIGLOT - 5-Shot Learning dataset. sent: Article 1 uses multiple datasets:  Mini-ImageNet - 1-Shot Learning, and  Mini-ImageNet - 5-Shot Learning, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses MAML approach for Few-Shot Image Classification, while Article 2 uses Neural Statistician for Few-Shot Image Classification."
1043,1043,21_paper_7,21_paper_0,221,229,1703.034,1409.8403,Few-Shot Image Classification,"sent: Article 1 uses multiple datasets:  OMNIGLOT - 1-Shot Learning,  OMNIGLOT - 5-Shot Learning,  Mini-ImageNet - 1-Shot Learning, and  Mini-ImageNet - 5-Shot Learning, while Article 2 uses CUB-200 - 0-Shot Learning dataset.","sent: Article 1 uses MAML approach for Few-Shot Image Classification, while Article 2 uses SJE for Few-Shot Image Classification."
1044,1044,21_paper_2,21_paper_4,222,223,1812.02391,1606.0408,Few-Shot Image Classification,"sent: Article 1 and Article 2 use Mini-ImageNet - 1-Shot Learning dataset.sent: Article 1 and Article 2 use Mini-ImageNet - 5-Shot Learning dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  OMNIGLOT - 1-Shot Learning, and  OMNIGLOT - 5-Shot Learning.","sent: Article 1 uses MTL approach for Few-Shot Image Classification, while Article 2 uses Matching Nets   C64F feature extractor for Few-Shot Image Classification. sent: Article 1 uses MTL approach for Few-Shot Image Classification, while Article 2 uses Matching Nets for Few-Shot Image Classification."
1045,1045,21_paper_2,21_paper_12,222,224,1812.02391,1606.02185,Few-Shot Image Classification,"sent: Article 1 uses multiple datasets:  Mini-ImageNet - 1-Shot Learning, and  Mini-ImageNet - 5-Shot Learning, while Article 2 uses multiple datasets:  OMNIGLOT - 1-Shot Learning, and  OMNIGLOT - 5-Shot Learning.","sent: Article 1 uses MTL approach for Few-Shot Image Classification, while Article 2 uses Neural Statistician for Few-Shot Image Classification."
1046,1046,21_paper_2,21_paper_8,222,225,1812.02391,1703.034,Few-Shot Image Classification,"sent: Article 1 and Article 2 use Mini-ImageNet - 1-Shot Learning dataset.sent: Article 1 and Article 2 use Mini-ImageNet - 5-Shot Learning dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  OMNIGLOT - 1-Shot Learning, and  OMNIGLOT - 5-Shot Learning.","sent: Article 1 uses MTL approach for Few-Shot Image Classification, while Article 2 uses MAML for Few-Shot Image Classification."
1047,1047,21_paper_2,21_paper_0,222,229,1812.02391,1409.8403,Few-Shot Image Classification,"sent: Article 1 uses multiple datasets:  Mini-ImageNet - 1-Shot Learning, and  Mini-ImageNet - 5-Shot Learning, while Article 2 uses CUB-200 - 0-Shot Learning dataset.","sent: Article 1 uses MTL approach for Few-Shot Image Classification, while Article 2 uses SJE for Few-Shot Image Classification."
1048,1048,21_paper_4,21_paper_12,223,224,1606.0408,1606.02185,Few-Shot Image Classification,"sent: Article 1 and Article 2 use OMNIGLOT - 1-Shot Learning dataset.sent: Article 1 and Article 2 use OMNIGLOT - 5-Shot Learning dataset. sent: Article 1 uses multiple datasets:  Mini-ImageNet - 1-Shot Learning, and  Mini-ImageNet - 5-Shot Learning, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Matching Nets   C64F feature extractor approach for Few-Shot Image Classification, while Article 2 uses Neural Statistician for Few-Shot Image Classification. sent: Article 1 uses Matching Nets approach for Few-Shot Image Classification, while Article 2 uses Neural Statistician for Few-Shot Image Classification."
1049,1049,21_paper_4,21_paper_8,223,225,1606.0408,1703.034,Few-Shot Image Classification,sent: Article 1 and Article 2 use OMNIGLOT - 1-Shot Learning dataset.sent: Article 1 and Article 2 use OMNIGLOT - 5-Shot Learning dataset.sent: Article 1 and Article 2 use Mini-ImageNet - 1-Shot Learning dataset.sent: Article 1 and Article 2 use Mini-ImageNet - 5-Shot Learning dataset.,"sent: Article 1 uses Matching Nets   C64F feature extractor approach for Few-Shot Image Classification, while Article 2 uses MAML for Few-Shot Image Classification. sent: Article 1 uses Matching Nets approach for Few-Shot Image Classification, while Article 2 uses MAML for Few-Shot Image Classification."
1050,1050,21_paper_4,21_paper_0,223,229,1606.0408,1409.8403,Few-Shot Image Classification,"sent: Article 1 uses multiple datasets:  OMNIGLOT - 1-Shot Learning,  OMNIGLOT - 5-Shot Learning,  Mini-ImageNet - 1-Shot Learning, and  Mini-ImageNet - 5-Shot Learning, while Article 2 uses CUB-200 - 0-Shot Learning dataset.","sent: Article 1 uses Matching Nets   C64F feature extractor approach for Few-Shot Image Classification, while Article 2 uses SJE for Few-Shot Image Classification. sent: Article 1 uses Matching Nets approach for Few-Shot Image Classification, while Article 2 uses SJE for Few-Shot Image Classification."
1051,1051,21_paper_4,21_paper_1,223,2211,1606.0408,1812.02391,Few-Shot Image Classification,"sent: Article 1 and Article 2 use Mini-ImageNet - 1-Shot Learning dataset.sent: Article 1 and Article 2 use Mini-ImageNet - 5-Shot Learning dataset. sent: Article 1 uses multiple datasets:  OMNIGLOT - 1-Shot Learning, and  OMNIGLOT - 5-Shot Learning, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Matching Nets   C64F feature extractor approach for Few-Shot Image Classification, while Article 2 uses MTL for Few-Shot Image Classification. sent: Article 1 uses Matching Nets approach for Few-Shot Image Classification, while Article 2 uses MTL for Few-Shot Image Classification."
1052,1052,21_paper_0,21_paper_9,229,2210,1409.8403,1703.034,Few-Shot Image Classification,"sent: Article 1 uses CUB-200 - 0-Shot Learning dataset, while Article 2 uses multiple datasets:  OMNIGLOT - 1-Shot Learning,  OMNIGLOT - 5-Shot Learning,  Mini-ImageNet - 1-Shot Learning, and  Mini-ImageNet - 5-Shot Learning.","sent: Article 1 uses SJE approach for Few-Shot Image Classification, while Article 2 uses MAML for Few-Shot Image Classification."
1053,1053,21_paper_0,21_paper_1,229,2211,1409.8403,1812.02391,Few-Shot Image Classification,"sent: Article 1 uses CUB-200 - 0-Shot Learning dataset, while Article 2 uses multiple datasets:  Mini-ImageNet - 1-Shot Learning, and  Mini-ImageNet - 5-Shot Learning.","sent: Article 1 uses SJE approach for Few-Shot Image Classification, while Article 2 uses MTL for Few-Shot Image Classification."
1054,1054,21_paper_0,21_paper_3,229,2212,1409.8403,1606.0408,Few-Shot Image Classification,"sent: Article 1 uses CUB-200 - 0-Shot Learning dataset, while Article 2 uses multiple datasets:  OMNIGLOT - 1-Shot Learning,  OMNIGLOT - 5-Shot Learning,  Mini-ImageNet - 1-Shot Learning, and  Mini-ImageNet - 5-Shot Learning.","sent: Article 1 uses SJE approach for Few-Shot Image Classification, while Article 2 uses Matching Nets   C64F feature extractor for Few-Shot Image Classification. sent: Article 1 uses SJE approach for Few-Shot Image Classification, while Article 2 uses Matching Nets for Few-Shot Image Classification."
1055,1055,22_paper_30,22_paper_33,230,231,1501.00092,1811.11482,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses multiple datasets:  Manga109 - 4x upscaling,  Urban100 - 4x upscaling,  Xiph HD - 4x upscaling,  Ultra Video Group HD - 4x upscaling,  Vid4 - 4x upscaling, and  BSD100 - 4x upscaling, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SRCNN approach for Image Super-Resolution, while Article 2 uses PFF for Image Super-Resolution."
1056,1056,22_paper_30,22_paper_49,230,232,1501.00092,1712.06116,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset.sent: Article 1 and Article 2 use Urban100 - 4x upscaling dataset. sent: Article 1 uses multiple datasets:  Manga109 - 4x upscaling,  Ultra Video Group HD - 4x upscaling,  Vid4 - 4x upscaling, and  Xiph HD - 4x upscaling, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SRCNN approach for Image Super-Resolution, while Article 2 uses SRMDNF for Image Super-Resolution."
1057,1057,22_paper_30,22_paper_6,230,233,1501.00092,1609.05158,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset.sent: Article 1 and Article 2 use Xiph HD - 4x upscaling dataset.sent: Article 1 and Article 2 use Ultra Video Group HD - 4x upscaling dataset.sent: Article 1 and Article 2 use Vid4 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset. sent: Article 1 uses multiple datasets:  Manga109 - 4x upscaling, and  Urban100 - 4x upscaling, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SRCNN approach for Image Super-Resolution, while Article 2 uses ESPCN for Image Super-Resolution. sent: Article 1 uses SRCNN approach for Image Super-Resolution, while Article 2 uses bicubic for Image Super-Resolution. sent: Article 1 uses SRCNN approach for Video Super-Resolution, while Article 2 uses ESPCN for Video Super-Resolution. sent: Article 1 uses SRCNN approach for Video Super-Resolution, while Article 2 uses bicubic for Video Super-Resolution."
1058,1058,22_paper_30,22_paper_11,230,234,1501.00092,1511.02228,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses multiple datasets:  Manga109 - 4x upscaling,  Urban100 - 4x upscaling,  Xiph HD - 4x upscaling,  Ultra Video Group HD - 4x upscaling, and  Vid4 - 4x upscaling, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SRCNN approach for Image Super-Resolution, while Article 2 uses IA for Image Super-Resolution."
1059,1059,22_paper_30,22_paper_16,230,235,1501.00092,1511.04491,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses multiple datasets:  Manga109 - 4x upscaling,  Urban100 - 4x upscaling,  Xiph HD - 4x upscaling,  Ultra Video Group HD - 4x upscaling, and  Vid4 - 4x upscaling, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SRCNN approach for Image Super-Resolution, while Article 2 uses DRCN for Image Super-Resolution."
1060,1060,22_paper_30,22_paper_38,230,238,1501.00092,1707.07128,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses multiple datasets:  Manga109 - 4x upscaling,  Urban100 - 4x upscaling,  Xiph HD - 4x upscaling,  Ultra Video Group HD - 4x upscaling,  Vid4 - 4x upscaling, and  BSD100 - 4x upscaling, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SRCNN approach for Image Super-Resolution, while Article 2 uses MSSRNet for Image Super-Resolution."
1061,1061,22_paper_30,22_paper_37,230,2320,1501.00092,1704.03915,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Urban100 - 4x upscaling dataset. sent: Article 1 uses multiple datasets:  Manga109 - 4x upscaling,  Set5 - 4x upscaling,  Xiph HD - 4x upscaling,  Ultra Video Group HD - 4x upscaling, and  Vid4 - 4x upscaling, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SRCNN approach for Image Super-Resolution, while Article 2 uses LapSRN for Image Super-Resolution. sent: Article 1 uses SRCNN approach for Image Super-Resolution, while Article 2 uses  LapSR for Image Super-Resolution."
1062,1062,22_paper_33,22_paper_49,231,232,1811.11482,1712.06116,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  BSD100 - 4x upscaling, and  Urban100 - 4x upscaling.","sent: Article 1 uses PFF approach for Image Super-Resolution, while Article 2 uses SRMDNF for Image Super-Resolution."
1063,1063,22_paper_33,22_paper_6,231,233,1811.11482,1609.05158,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Ultra Video Group HD - 4x upscaling,  BSD100 - 4x upscaling,  Vid4 - 4x upscaling, and  Xiph HD - 4x upscaling.","sent: Article 1 uses PFF approach for Image Super-Resolution, while Article 2 uses ESPCN for Image Super-Resolution. sent: Article 1 uses PFF approach for Image Super-Resolution, while Article 2 uses bicubic for Image Super-Resolution."
1064,1064,22_paper_33,22_paper_11,231,234,1811.11482,1511.02228,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses BSD100 - 4x upscaling dataset.","sent: Article 1 uses PFF approach for Image Super-Resolution, while Article 2 uses IA for Image Super-Resolution."
1065,1065,22_paper_33,22_paper_16,231,235,1811.11482,1511.04491,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses BSD100 - 4x upscaling dataset.","sent: Article 1 uses PFF approach for Image Super-Resolution, while Article 2 uses DRCN for Image Super-Resolution."
1066,1066,22_paper_33,22_paper_38,231,238,1811.11482,1707.07128,Image Super-Resolution,sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset.,"sent: Article 1 uses PFF approach for Image Super-Resolution, while Article 2 uses MSSRNet for Image Super-Resolution."
1067,1067,22_paper_33,22_paper_21,231,2311,1811.11482,1501.00092,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Manga109 - 4x upscaling,  Urban100 - 4x upscaling,  Xiph HD - 4x upscaling,  Ultra Video Group HD - 4x upscaling,  Vid4 - 4x upscaling, and  BSD100 - 4x upscaling.","sent: Article 1 uses PFF approach for Image Super-Resolution, while Article 2 uses SRCNN for Image Super-Resolution."
1068,1068,22_paper_33,22_paper_37,231,2320,1811.11482,1704.03915,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset. sent: Article 1 uses Set5 - 4x upscaling dataset, while Article 2 uses multiple datasets:  BSD100 - 4x upscaling, and  Urban100 - 4x upscaling.","sent: Article 1 uses PFF approach for Image Super-Resolution, while Article 2 uses LapSRN for Image Super-Resolution. sent: Article 1 uses PFF approach for Image Super-Resolution, while Article 2 uses  LapSR for Image Super-Resolution."
1069,1069,22_paper_49,22_paper_6,232,233,1712.06116,1609.05158,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses Urban100 - 4x upscaling dataset, while Article 2 uses multiple datasets:  Ultra Video Group HD - 4x upscaling,  Vid4 - 4x upscaling, and  Xiph HD - 4x upscaling.","sent: Article 1 uses SRMDNF approach for Image Super-Resolution, while Article 2 uses ESPCN for Image Super-Resolution. sent: Article 1 uses SRMDNF approach for Image Super-Resolution, while Article 2 uses bicubic for Image Super-Resolution."
1070,1070,22_paper_49,22_paper_11,232,234,1712.06116,1511.02228,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses Urban100 - 4x upscaling dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SRMDNF approach for Image Super-Resolution, while Article 2 uses IA for Image Super-Resolution."
1071,1071,22_paper_49,22_paper_16,232,235,1712.06116,1511.04491,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses Urban100 - 4x upscaling dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SRMDNF approach for Image Super-Resolution, while Article 2 uses DRCN for Image Super-Resolution."
1072,1072,22_paper_49,22_paper_38,232,238,1712.06116,1707.07128,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses multiple datasets:  BSD100 - 4x upscaling, and  Urban100 - 4x upscaling, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SRMDNF approach for Image Super-Resolution, while Article 2 uses MSSRNet for Image Super-Resolution."
1073,1073,22_paper_49,22_paper_21,232,2311,1712.06116,1501.00092,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset.sent: Article 1 and Article 2 use Urban100 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Manga109 - 4x upscaling,  Ultra Video Group HD - 4x upscaling,  Vid4 - 4x upscaling, and  Xiph HD - 4x upscaling.","sent: Article 1 uses SRMDNF approach for Image Super-Resolution, while Article 2 uses SRCNN for Image Super-Resolution."
1074,1074,22_paper_49,22_paper_37,232,2320,1712.06116,1704.03915,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Urban100 - 4x upscaling dataset. sent: Article 1 uses Set5 - 4x upscaling dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SRMDNF approach for Image Super-Resolution, while Article 2 uses LapSRN for Image Super-Resolution. sent: Article 1 uses SRMDNF approach for Image Super-Resolution, while Article 2 uses  LapSR for Image Super-Resolution."
1075,1075,22_paper_49,22_paper_32,232,2330,1712.06116,1811.11482,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses multiple datasets:  BSD100 - 4x upscaling, and  Urban100 - 4x upscaling, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SRMDNF approach for Image Super-Resolution, while Article 2 uses PFF for Image Super-Resolution."
1076,1076,22_paper_6,22_paper_11,233,234,1609.05158,1511.02228,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses multiple datasets:  Ultra Video Group HD - 4x upscaling,  Vid4 - 4x upscaling, and  Xiph HD - 4x upscaling, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ESPCN approach for Image Super-Resolution, while Article 2 uses IA for Image Super-Resolution. sent: Article 1 uses bicubic approach for Image Super-Resolution, while Article 2 uses IA for Image Super-Resolution."
1077,1077,22_paper_6,22_paper_16,233,235,1609.05158,1511.04491,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses multiple datasets:  Ultra Video Group HD - 4x upscaling,  Vid4 - 4x upscaling, and  Xiph HD - 4x upscaling, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ESPCN approach for Image Super-Resolution, while Article 2 uses DRCN for Image Super-Resolution. sent: Article 1 uses bicubic approach for Image Super-Resolution, while Article 2 uses DRCN for Image Super-Resolution."
1078,1078,22_paper_6,22_paper_38,233,238,1609.05158,1707.07128,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses multiple datasets:  Ultra Video Group HD - 4x upscaling,  BSD100 - 4x upscaling,  Vid4 - 4x upscaling, and  Xiph HD - 4x upscaling, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ESPCN approach for Image Super-Resolution, while Article 2 uses MSSRNet for Image Super-Resolution. sent: Article 1 uses bicubic approach for Image Super-Resolution, while Article 2 uses MSSRNet for Image Super-Resolution."
1079,1079,22_paper_6,22_paper_21,233,2311,1609.05158,1501.00092,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset.sent: Article 1 and Article 2 use Xiph HD - 4x upscaling dataset.sent: Article 1 and Article 2 use Ultra Video Group HD - 4x upscaling dataset.sent: Article 1 and Article 2 use Vid4 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Manga109 - 4x upscaling, and  Urban100 - 4x upscaling.","sent: Article 1 uses ESPCN approach for Image Super-Resolution, while Article 2 uses SRCNN for Image Super-Resolution. sent: Article 1 uses bicubic approach for Image Super-Resolution, while Article 2 uses SRCNN for Image Super-Resolution. sent: Article 1 uses ESPCN approach for Video Super-Resolution, while Article 2 uses SRCNN for Video Super-Resolution. sent: Article 1 uses bicubic approach for Video Super-Resolution, while Article 2 uses SRCNN for Video Super-Resolution."
1080,1080,22_paper_6,22_paper_45,233,2318,1609.05158,1712.06116,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses multiple datasets:  Ultra Video Group HD - 4x upscaling,  Vid4 - 4x upscaling, and  Xiph HD - 4x upscaling, while Article 2 uses Urban100 - 4x upscaling dataset.","sent: Article 1 uses ESPCN approach for Image Super-Resolution, while Article 2 uses SRMDNF for Image Super-Resolution. sent: Article 1 uses bicubic approach for Image Super-Resolution, while Article 2 uses SRMDNF for Image Super-Resolution."
1081,1081,22_paper_6,22_paper_37,233,2320,1609.05158,1704.03915,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset. sent: Article 1 uses multiple datasets:  Ultra Video Group HD - 4x upscaling,  Set5 - 4x upscaling,  Vid4 - 4x upscaling, and  Xiph HD - 4x upscaling, while Article 2 uses Urban100 - 4x upscaling dataset.","sent: Article 1 uses ESPCN approach for Image Super-Resolution, while Article 2 uses LapSRN for Image Super-Resolution. sent: Article 1 uses ESPCN approach for Image Super-Resolution, while Article 2 uses  LapSR for Image Super-Resolution. sent: Article 1 uses bicubic approach for Image Super-Resolution, while Article 2 uses LapSRN for Image Super-Resolution. sent: Article 1 uses bicubic approach for Image Super-Resolution, while Article 2 uses  LapSR for Image Super-Resolution."
1082,1082,22_paper_6,22_paper_32,233,2330,1609.05158,1811.11482,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses multiple datasets:  Ultra Video Group HD - 4x upscaling,  BSD100 - 4x upscaling,  Vid4 - 4x upscaling, and  Xiph HD - 4x upscaling, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ESPCN approach for Image Super-Resolution, while Article 2 uses PFF for Image Super-Resolution. sent: Article 1 uses bicubic approach for Image Super-Resolution, while Article 2 uses PFF for Image Super-Resolution."
1083,1083,22_paper_11,22_paper_16,234,235,1511.02228,1511.04491,Image Super-Resolution,sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset.,"sent: Article 1 uses IA approach for Image Super-Resolution, while Article 2 uses DRCN for Image Super-Resolution."
1084,1084,22_paper_11,22_paper_38,234,238,1511.02228,1707.07128,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses BSD100 - 4x upscaling dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses IA approach for Image Super-Resolution, while Article 2 uses MSSRNet for Image Super-Resolution."
1085,1085,22_paper_11,22_paper_2,234,239,1511.02228,1609.05158,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Ultra Video Group HD - 4x upscaling,  Vid4 - 4x upscaling, and  Xiph HD - 4x upscaling.","sent: Article 1 uses IA approach for Image Super-Resolution, while Article 2 uses ESPCN for Image Super-Resolution. sent: Article 1 uses IA approach for Image Super-Resolution, while Article 2 uses bicubic for Image Super-Resolution."
1086,1086,22_paper_11,22_paper_21,234,2311,1511.02228,1501.00092,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Manga109 - 4x upscaling,  Urban100 - 4x upscaling,  Xiph HD - 4x upscaling,  Ultra Video Group HD - 4x upscaling, and  Vid4 - 4x upscaling.","sent: Article 1 uses IA approach for Image Super-Resolution, while Article 2 uses SRCNN for Image Super-Resolution."
1087,1087,22_paper_11,22_paper_45,234,2318,1511.02228,1712.06116,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Urban100 - 4x upscaling dataset.","sent: Article 1 uses IA approach for Image Super-Resolution, while Article 2 uses SRMDNF for Image Super-Resolution."
1088,1088,22_paper_11,22_paper_37,234,2320,1511.02228,1704.03915,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset. sent: Article 1 uses Set5 - 4x upscaling dataset, while Article 2 uses Urban100 - 4x upscaling dataset.","sent: Article 1 uses IA approach for Image Super-Resolution, while Article 2 uses LapSRN for Image Super-Resolution. sent: Article 1 uses IA approach for Image Super-Resolution, while Article 2 uses  LapSR for Image Super-Resolution."
1089,1089,22_paper_11,22_paper_32,234,2330,1511.02228,1811.11482,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses BSD100 - 4x upscaling dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses IA approach for Image Super-Resolution, while Article 2 uses PFF for Image Super-Resolution."
1090,1090,22_paper_16,22_paper_10,235,237,1511.04491,1511.02228,Image Super-Resolution,sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset.,"sent: Article 1 uses DRCN approach for Image Super-Resolution, while Article 2 uses IA for Image Super-Resolution."
1091,1091,22_paper_16,22_paper_38,235,238,1511.04491,1707.07128,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses BSD100 - 4x upscaling dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DRCN approach for Image Super-Resolution, while Article 2 uses MSSRNet for Image Super-Resolution."
1092,1092,22_paper_16,22_paper_2,235,239,1511.04491,1609.05158,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Ultra Video Group HD - 4x upscaling,  Vid4 - 4x upscaling, and  Xiph HD - 4x upscaling.","sent: Article 1 uses DRCN approach for Image Super-Resolution, while Article 2 uses ESPCN for Image Super-Resolution. sent: Article 1 uses DRCN approach for Image Super-Resolution, while Article 2 uses bicubic for Image Super-Resolution."
1093,1093,22_paper_16,22_paper_21,235,2311,1511.04491,1501.00092,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Manga109 - 4x upscaling,  Urban100 - 4x upscaling,  Xiph HD - 4x upscaling,  Ultra Video Group HD - 4x upscaling, and  Vid4 - 4x upscaling.","sent: Article 1 uses DRCN approach for Image Super-Resolution, while Article 2 uses SRCNN for Image Super-Resolution."
1094,1094,22_paper_16,22_paper_45,235,2318,1511.04491,1712.06116,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Urban100 - 4x upscaling dataset.","sent: Article 1 uses DRCN approach for Image Super-Resolution, while Article 2 uses SRMDNF for Image Super-Resolution."
1095,1095,22_paper_16,22_paper_37,235,2320,1511.04491,1704.03915,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset. sent: Article 1 uses Set5 - 4x upscaling dataset, while Article 2 uses Urban100 - 4x upscaling dataset.","sent: Article 1 uses DRCN approach for Image Super-Resolution, while Article 2 uses LapSRN for Image Super-Resolution. sent: Article 1 uses DRCN approach for Image Super-Resolution, while Article 2 uses  LapSR for Image Super-Resolution."
1096,1096,22_paper_16,22_paper_32,235,2330,1511.04491,1811.11482,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses BSD100 - 4x upscaling dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DRCN approach for Image Super-Resolution, while Article 2 uses PFF for Image Super-Resolution."
1097,1097,22_paper_38,22_paper_2,238,239,1707.07128,1609.05158,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Ultra Video Group HD - 4x upscaling,  BSD100 - 4x upscaling,  Vid4 - 4x upscaling, and  Xiph HD - 4x upscaling.","sent: Article 1 uses MSSRNet approach for Image Super-Resolution, while Article 2 uses ESPCN for Image Super-Resolution. sent: Article 1 uses MSSRNet approach for Image Super-Resolution, while Article 2 uses bicubic for Image Super-Resolution."
1098,1098,22_paper_38,22_paper_9,238,2310,1707.07128,1511.02228,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses BSD100 - 4x upscaling dataset.","sent: Article 1 uses MSSRNet approach for Image Super-Resolution, while Article 2 uses IA for Image Super-Resolution."
1099,1099,22_paper_38,22_paper_21,238,2311,1707.07128,1501.00092,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Manga109 - 4x upscaling,  Urban100 - 4x upscaling,  Xiph HD - 4x upscaling,  Ultra Video Group HD - 4x upscaling,  Vid4 - 4x upscaling, and  BSD100 - 4x upscaling.","sent: Article 1 uses MSSRNet approach for Image Super-Resolution, while Article 2 uses SRCNN for Image Super-Resolution."
1100,1100,22_paper_38,22_paper_20,238,2314,1707.07128,1511.04491,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses BSD100 - 4x upscaling dataset.","sent: Article 1 uses MSSRNet approach for Image Super-Resolution, while Article 2 uses DRCN for Image Super-Resolution."
1101,1101,22_paper_38,22_paper_45,238,2318,1707.07128,1712.06116,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  BSD100 - 4x upscaling, and  Urban100 - 4x upscaling.","sent: Article 1 uses MSSRNet approach for Image Super-Resolution, while Article 2 uses SRMDNF for Image Super-Resolution."
1102,1102,22_paper_38,22_paper_37,238,2320,1707.07128,1704.03915,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset. sent: Article 1 uses Set5 - 4x upscaling dataset, while Article 2 uses multiple datasets:  BSD100 - 4x upscaling, and  Urban100 - 4x upscaling.","sent: Article 1 uses MSSRNet approach for Image Super-Resolution, while Article 2 uses LapSRN for Image Super-Resolution. sent: Article 1 uses MSSRNet approach for Image Super-Resolution, while Article 2 uses  LapSR for Image Super-Resolution."
1103,1103,22_paper_38,22_paper_32,238,2330,1707.07128,1811.11482,Image Super-Resolution,sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset.,"sent: Article 1 uses MSSRNet approach for Image Super-Resolution, while Article 2 uses PFF for Image Super-Resolution."
1104,1104,22_paper_37,22_paper_40,2320,2321,1704.03915,1707.07128,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset. sent: Article 1 uses multiple datasets:  BSD100 - 4x upscaling, and  Urban100 - 4x upscaling, while Article 2 uses Set5 - 4x upscaling dataset.","sent: Article 1 uses LapSRN approach for Image Super-Resolution, while Article 2 uses MSSRNet for Image Super-Resolution. sent: Article 1 uses  LapSR approach for Image Super-Resolution, while Article 2 uses MSSRNet for Image Super-Resolution."
1105,1105,22_paper_37,22_paper_44,2320,2322,1704.03915,1712.06116,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Urban100 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Set5 - 4x upscaling dataset.","sent: Article 1 uses LapSRN approach for Image Super-Resolution, while Article 2 uses SRMDNF for Image Super-Resolution. sent: Article 1 uses  LapSR approach for Image Super-Resolution, while Article 2 uses SRMDNF for Image Super-Resolution."
1106,1106,22_paper_37,22_paper_7,2320,2323,1704.03915,1609.05158,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset. sent: Article 1 uses Urban100 - 4x upscaling dataset, while Article 2 uses multiple datasets:  Ultra Video Group HD - 4x upscaling,  Set5 - 4x upscaling,  Vid4 - 4x upscaling, and  Xiph HD - 4x upscaling.","sent: Article 1 uses LapSRN approach for Image Super-Resolution, while Article 2 uses ESPCN for Image Super-Resolution. sent: Article 1 uses LapSRN approach for Image Super-Resolution, while Article 2 uses bicubic for Image Super-Resolution. sent: Article 1 uses  LapSR approach for Image Super-Resolution, while Article 2 uses ESPCN for Image Super-Resolution. sent: Article 1 uses  LapSR approach for Image Super-Resolution, while Article 2 uses bicubic for Image Super-Resolution."
1107,1107,22_paper_37,22_paper_29,2320,2326,1704.03915,1501.00092,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Urban100 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Manga109 - 4x upscaling,  Set5 - 4x upscaling,  Xiph HD - 4x upscaling,  Ultra Video Group HD - 4x upscaling, and  Vid4 - 4x upscaling.","sent: Article 1 uses LapSRN approach for Image Super-Resolution, while Article 2 uses SRCNN for Image Super-Resolution. sent: Article 1 uses  LapSR approach for Image Super-Resolution, while Article 2 uses SRCNN for Image Super-Resolution."
1108,1108,22_paper_37,22_paper_19,2320,2329,1704.03915,1511.04491,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset. sent: Article 1 uses Urban100 - 4x upscaling dataset, while Article 2 uses Set5 - 4x upscaling dataset.","sent: Article 1 uses LapSRN approach for Image Super-Resolution, while Article 2 uses DRCN for Image Super-Resolution. sent: Article 1 uses  LapSR approach for Image Super-Resolution, while Article 2 uses DRCN for Image Super-Resolution."
1109,1109,22_paper_37,22_paper_32,2320,2330,1704.03915,1811.11482,Image Super-Resolution,"sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset. sent: Article 1 uses multiple datasets:  BSD100 - 4x upscaling, and  Urban100 - 4x upscaling, while Article 2 uses Set5 - 4x upscaling dataset.","sent: Article 1 uses LapSRN approach for Image Super-Resolution, while Article 2 uses PFF for Image Super-Resolution. sent: Article 1 uses  LapSR approach for Image Super-Resolution, while Article 2 uses PFF for Image Super-Resolution."
1110,1110,24_paper_1,24_paper_7,250,251,1904.07418,1602.06023,Text Summarization,"sent: Article 1 and Article 2 use DUC 2004 Task 1 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses GigaWord dataset.","sent: Article 1 uses Transformer LRPE PE Re-ranking Ensemble approach for Text Summarization, while Article 2 uses words-lvt5k-1sent for Text Summarization."
1111,1111,24_paper_1,24_paper_9,250,252,1904.07418,1711.04434,Text Summarization,"sent: Article 1 uses DUC 2004 Task 1 dataset, while Article 2 uses GigaWord dataset.","sent: Article 1 uses Transformer LRPE PE Re-ranking Ensemble approach for Text Summarization, while Article 2 uses FTSum g for Text Summarization."
1112,1112,24_paper_7,24_paper_9,251,252,1602.06023,1711.04434,Text Summarization,"sent: Article 1 and Article 2 use GigaWord dataset. sent: Article 1 uses DUC 2004 Task 1 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses words-lvt5k-1sent approach for Text Summarization, while Article 2 uses FTSum g for Text Summarization."
1113,1113,24_paper_7,24_paper_2,251,254,1602.06023,1904.07418,Text Summarization,"sent: Article 1 and Article 2 use DUC 2004 Task 1 dataset. sent: Article 1 uses GigaWord dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses words-lvt5k-1sent approach for Text Summarization, while Article 2 uses Transformer LRPE PE Re-ranking Ensemble for Text Summarization."
1114,1114,24_paper_9,24_paper_2,252,254,1711.04434,1904.07418,Text Summarization,"sent: Article 1 uses GigaWord dataset, while Article 2 uses DUC 2004 Task 1 dataset.","sent: Article 1 uses FTSum g approach for Text Summarization, while Article 2 uses Transformer LRPE PE Re-ranking Ensemble for Text Summarization."
1115,1115,24_paper_9,24_paper_8,252,255,1711.04434,1602.06023,Text Summarization,"sent: Article 1 and Article 2 use GigaWord dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses DUC 2004 Task 1 dataset.","sent: Article 1 uses FTSum g approach for Text Summarization, while Article 2 uses words-lvt5k-1sent for Text Summarization."
1116,1116,25_paper_13,25_paper_11,260,261,1904.089,1805.093,Object Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses PASCAL VOC 2007 dataset.","sent: Article 1 uses CornerNet-Saccade approach for Object Detection, while Article 2 uses SNIPER for Object Detection. sent: Article 1 uses CornerNet-Squeeze approach for Object Detection, while Article 2 uses SNIPER for Object Detection."
1117,1117,25_paper_13,25_paper_3,260,262,1904.089,1811.05181,Object Detection,sent: Article 1 and Article 2 use COCO dataset.,"sent: Article 1 uses CornerNet-Saccade approach for Object Detection, while Article 2 uses GHM-C   GHM-R for Object Detection. sent: Article 1 uses CornerNet-Squeeze approach for Object Detection, while Article 2 uses GHM-C   GHM-R for Object Detection."
1118,1118,25_paper_13,25_paper_8,260,264,1904.089,1703.06211,Object Detection,sent: Article 1 and Article 2 use COCO dataset.,"sent: Article 1 uses CornerNet-Saccade approach for Object Detection, while Article 2 uses D-RFCN   ResNet-101  6 scales  for Object Detection. sent: Article 1 uses CornerNet-Squeeze approach for Object Detection, while Article 2 uses D-RFCN   ResNet-101  6 scales  for Object Detection."
1119,1119,25_paper_13,25_paper_14,260,265,1904.089,1504.08083,Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses PASCAL VOC 2007 dataset.","sent: Article 1 uses CornerNet-Saccade approach for Object Detection, while Article 2 uses FRCN for Object Detection. sent: Article 1 uses CornerNet-Squeeze approach for Object Detection, while Article 2 uses FRCN for Object Detection."
1120,1120,25_paper_13,25_paper_2,260,267,1904.089,1703.0687,Object Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MHP v1 0, and  MHP v2 0.","sent: Article 1 uses CornerNet-Saccade approach for Object Detection, while Article 2 uses Mask R-CNN for Object Detection. sent: Article 1 uses CornerNet-Squeeze approach for Object Detection, while Article 2 uses Mask R-CNN for Object Detection."
1121,1121,25_paper_13,25_paper_0,260,268,1904.089,1712.00726,Object Detection,sent: Article 1 and Article 2 use COCO dataset.,"sent: Article 1 uses CornerNet-Saccade approach for Object Detection, while Article 2 uses Cascade R-CNN for Object Detection. sent: Article 1 uses CornerNet-Squeeze approach for Object Detection, while Article 2 uses Cascade R-CNN for Object Detection."
1122,1122,25_paper_13,25_paper_1,260,269,1904.089,1312.6229,Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses ImageNet Detection dataset.","sent: Article 1 uses CornerNet-Saccade approach for Object Detection, while Article 2 uses OverFeat for Object Detection. sent: Article 1 uses CornerNet-Squeeze approach for Object Detection, while Article 2 uses OverFeat for Object Detection."
1123,1123,25_paper_13,25_paper_6,260,2610,1904.089,1512.03385,Object Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  CIFAR-10, and  PASCAL VOC 2007.","sent: Article 1 uses CornerNet-Saccade approach for Object Detection, while Article 2 uses ResNet-101 for Object Detection. sent: Article 1 uses CornerNet-Saccade approach for Object Detection, while Article 2 uses ResNet for Object Detection. sent: Article 1 uses CornerNet-Saccade approach for Object Detection, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Object Detection. sent: Article 1 uses CornerNet-Squeeze approach for Object Detection, while Article 2 uses ResNet-101 for Object Detection. sent: Article 1 uses CornerNet-Squeeze approach for Object Detection, while Article 2 uses ResNet for Object Detection. sent: Article 1 uses CornerNet-Squeeze approach for Object Detection, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Object Detection."
1124,1124,25_paper_13,25_paper_15,260,2611,1904.089,1506.01497,Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses PASCAL VOC 2007 dataset.","sent: Article 1 uses CornerNet-Squeeze approach for Real-Time Object Detection, while Article 2 uses Faster R-CNN for Real-Time Object Detection. sent: Article 1 uses CornerNet-Saccade approach for Object Detection, while Article 2 uses Faster R-CNN for Object Detection. sent: Article 1 uses CornerNet-Squeeze approach for Object Detection, while Article 2 uses Faster R-CNN for Object Detection."
1125,1125,25_paper_13,25_paper_9,260,2612,1904.089,1704.03414,Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses PASCAL VOC 2007 dataset.","sent: Article 1 uses CornerNet-Saccade approach for Object Detection, while Article 2 uses FRCN for Object Detection. sent: Article 1 uses CornerNet-Squeeze approach for Object Detection, while Article 2 uses FRCN for Object Detection."
1126,1126,25_paper_13,25_paper_5,260,2613,1904.089,1612.03144,Object Detection,sent: Article 1 and Article 2 use COCO dataset.,"sent: Article 1 uses CornerNet-Saccade approach for Object Detection, while Article 2 uses Faster R-CNN   FPN for Object Detection. sent: Article 1 uses CornerNet-Squeeze approach for Object Detection, while Article 2 uses Faster R-CNN   FPN for Object Detection."
1127,1127,25_paper_13,25_paper_4,260,2614,1904.089,1708.04896,Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses CornerNet-Saccade approach for Object Detection, while Article 2 uses TriNet   Random Erasing for Object Detection. sent: Article 1 uses CornerNet-Saccade approach for Object Detection, while Article 2 uses SVDNet   Random Erasing for Object Detection. sent: Article 1 uses CornerNet-Saccade approach for Object Detection, while Article 2 uses I ORE for Object Detection. sent: Article 1 uses CornerNet-Saccade approach for Object Detection, while Article 2 uses Random Erasing for Object Detection. sent: Article 1 uses CornerNet-Squeeze approach for Object Detection, while Article 2 uses TriNet   Random Erasing for Object Detection. sent: Article 1 uses CornerNet-Squeeze approach for Object Detection, while Article 2 uses SVDNet   Random Erasing for Object Detection. sent: Article 1 uses CornerNet-Squeeze approach for Object Detection, while Article 2 uses I ORE for Object Detection. sent: Article 1 uses CornerNet-Squeeze approach for Object Detection, while Article 2 uses Random Erasing for Object Detection."
1128,1128,25_paper_11,25_paper_3,261,262,1805.093,1811.05181,Object Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 uses PASCAL VOC 2007 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SNIPER approach for Object Detection, while Article 2 uses GHM-C   GHM-R for Object Detection."
1129,1129,25_paper_11,25_paper_8,261,264,1805.093,1703.06211,Object Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 uses PASCAL VOC 2007 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SNIPER approach for Object Detection, while Article 2 uses D-RFCN   ResNet-101  6 scales  for Object Detection."
1130,1130,25_paper_11,25_paper_14,261,265,1805.093,1504.08083,Object Detection,"sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 uses COCO dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SNIPER approach for Object Detection, while Article 2 uses FRCN for Object Detection."
1131,1131,25_paper_11,25_paper_12,261,266,1805.093,1904.089,Object Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 uses PASCAL VOC 2007 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SNIPER approach for Object Detection, while Article 2 uses CornerNet-Saccade for Object Detection. sent: Article 1 uses SNIPER approach for Object Detection, while Article 2 uses CornerNet-Squeeze for Object Detection."
1132,1132,25_paper_11,25_paper_2,261,267,1805.093,1703.0687,Object Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 uses PASCAL VOC 2007 dataset, while Article 2 uses multiple datasets:  MHP v1 0, and  MHP v2 0.","sent: Article 1 uses SNIPER approach for Object Detection, while Article 2 uses Mask R-CNN for Object Detection."
1133,1133,25_paper_11,25_paper_0,261,268,1805.093,1712.00726,Object Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 uses PASCAL VOC 2007 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SNIPER approach for Object Detection, while Article 2 uses Cascade R-CNN for Object Detection."
1134,1134,25_paper_11,25_paper_1,261,269,1805.093,1312.6229,Object Detection,"sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses ImageNet Detection dataset.","sent: Article 1 uses SNIPER approach for Object Detection, while Article 2 uses OverFeat for Object Detection."
1135,1135,25_paper_11,25_paper_6,261,2610,1805.093,1512.03385,Object Detection,"sent: Article 1 and Article 2 use COCO dataset.sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CIFAR-10 dataset.","sent: Article 1 uses SNIPER approach for Object Detection, while Article 2 uses ResNet-101 for Object Detection. sent: Article 1 uses SNIPER approach for Object Detection, while Article 2 uses ResNet for Object Detection. sent: Article 1 uses SNIPER approach for Object Detection, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Object Detection."
1136,1136,25_paper_11,25_paper_15,261,2611,1805.093,1506.01497,Object Detection,"sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 uses COCO dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SNIPER approach for Object Detection, while Article 2 uses Faster R-CNN for Object Detection."
1137,1137,25_paper_11,25_paper_9,261,2612,1805.093,1704.03414,Object Detection,"sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 uses COCO dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SNIPER approach for Object Detection, while Article 2 uses FRCN for Object Detection."
1138,1138,25_paper_11,25_paper_5,261,2613,1805.093,1612.03144,Object Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 uses PASCAL VOC 2007 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SNIPER approach for Object Detection, while Article 2 uses Faster R-CNN   FPN for Object Detection."
1139,1139,25_paper_11,25_paper_4,261,2614,1805.093,1708.04896,Object Detection,"sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 uses COCO dataset, while Article 2 uses multiple datasets:  DukeMTMC-reID, and  Fashion-MNIST.","sent: Article 1 uses SNIPER approach for Object Detection, while Article 2 uses TriNet   Random Erasing for Object Detection. sent: Article 1 uses SNIPER approach for Object Detection, while Article 2 uses SVDNet   Random Erasing for Object Detection. sent: Article 1 uses SNIPER approach for Object Detection, while Article 2 uses I ORE for Object Detection. sent: Article 1 uses SNIPER approach for Object Detection, while Article 2 uses Random Erasing for Object Detection."
1140,1140,25_paper_3,25_paper_10,262,263,1811.05181,1805.093,Object Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses PASCAL VOC 2007 dataset.","sent: Article 1 uses GHM-C   GHM-R approach for Object Detection, while Article 2 uses SNIPER for Object Detection."
1141,1141,25_paper_3,25_paper_8,262,264,1811.05181,1703.06211,Object Detection,sent: Article 1 and Article 2 use COCO dataset.,"sent: Article 1 uses GHM-C   GHM-R approach for Object Detection, while Article 2 uses D-RFCN   ResNet-101  6 scales  for Object Detection."
1142,1142,25_paper_3,25_paper_14,262,265,1811.05181,1504.08083,Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses PASCAL VOC 2007 dataset.","sent: Article 1 uses GHM-C   GHM-R approach for Object Detection, while Article 2 uses FRCN for Object Detection."
1143,1143,25_paper_3,25_paper_12,262,266,1811.05181,1904.089,Object Detection,sent: Article 1 and Article 2 use COCO dataset.,"sent: Article 1 uses GHM-C   GHM-R approach for Object Detection, while Article 2 uses CornerNet-Saccade for Object Detection. sent: Article 1 uses GHM-C   GHM-R approach for Object Detection, while Article 2 uses CornerNet-Squeeze for Object Detection."
1144,1144,25_paper_3,25_paper_2,262,267,1811.05181,1703.0687,Object Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MHP v1 0, and  MHP v2 0.","sent: Article 1 uses GHM-C   GHM-R approach for Object Detection, while Article 2 uses Mask R-CNN for Object Detection."
1145,1145,25_paper_3,25_paper_0,262,268,1811.05181,1712.00726,Object Detection,sent: Article 1 and Article 2 use COCO dataset.,"sent: Article 1 uses GHM-C   GHM-R approach for Object Detection, while Article 2 uses Cascade R-CNN for Object Detection."
1146,1146,25_paper_3,25_paper_1,262,269,1811.05181,1312.6229,Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses ImageNet Detection dataset.","sent: Article 1 uses GHM-C   GHM-R approach for Object Detection, while Article 2 uses OverFeat for Object Detection."
1147,1147,25_paper_3,25_paper_6,262,2610,1811.05181,1512.03385,Object Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  CIFAR-10, and  PASCAL VOC 2007.","sent: Article 1 uses GHM-C   GHM-R approach for Object Detection, while Article 2 uses ResNet-101 for Object Detection. sent: Article 1 uses GHM-C   GHM-R approach for Object Detection, while Article 2 uses ResNet for Object Detection. sent: Article 1 uses GHM-C   GHM-R approach for Object Detection, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Object Detection."
1148,1148,25_paper_3,25_paper_15,262,2611,1811.05181,1506.01497,Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses PASCAL VOC 2007 dataset.","sent: Article 1 uses GHM-C   GHM-R approach for Object Detection, while Article 2 uses Faster R-CNN for Object Detection."
1149,1149,25_paper_3,25_paper_9,262,2612,1811.05181,1704.03414,Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses PASCAL VOC 2007 dataset.","sent: Article 1 uses GHM-C   GHM-R approach for Object Detection, while Article 2 uses FRCN for Object Detection."
1150,1150,25_paper_3,25_paper_5,262,2613,1811.05181,1612.03144,Object Detection,sent: Article 1 and Article 2 use COCO dataset.,"sent: Article 1 uses GHM-C   GHM-R approach for Object Detection, while Article 2 uses Faster R-CNN   FPN for Object Detection."
1151,1151,25_paper_3,25_paper_4,262,2614,1811.05181,1708.04896,Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses GHM-C   GHM-R approach for Object Detection, while Article 2 uses TriNet   Random Erasing for Object Detection. sent: Article 1 uses GHM-C   GHM-R approach for Object Detection, while Article 2 uses SVDNet   Random Erasing for Object Detection. sent: Article 1 uses GHM-C   GHM-R approach for Object Detection, while Article 2 uses I ORE for Object Detection. sent: Article 1 uses GHM-C   GHM-R approach for Object Detection, while Article 2 uses Random Erasing for Object Detection."
1152,1152,25_paper_8,25_paper_14,264,265,1703.06211,1504.08083,Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses PASCAL VOC 2007 dataset.","sent: Article 1 uses D-RFCN   ResNet-101  6 scales  approach for Object Detection, while Article 2 uses FRCN for Object Detection."
1153,1153,25_paper_8,25_paper_12,264,266,1703.06211,1904.089,Object Detection,sent: Article 1 and Article 2 use COCO dataset.,"sent: Article 1 uses D-RFCN   ResNet-101  6 scales  approach for Object Detection, while Article 2 uses CornerNet-Saccade for Object Detection. sent: Article 1 uses D-RFCN   ResNet-101  6 scales  approach for Object Detection, while Article 2 uses CornerNet-Squeeze for Object Detection."
1154,1154,25_paper_8,25_paper_2,264,267,1703.06211,1703.0687,Object Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MHP v1 0, and  MHP v2 0.","sent: Article 1 uses D-RFCN   ResNet-101  6 scales  approach for Object Detection, while Article 2 uses Mask R-CNN for Object Detection."
1155,1155,25_paper_8,25_paper_0,264,268,1703.06211,1712.00726,Object Detection,sent: Article 1 and Article 2 use COCO dataset.,"sent: Article 1 uses D-RFCN   ResNet-101  6 scales  approach for Object Detection, while Article 2 uses Cascade R-CNN for Object Detection."
1156,1156,25_paper_8,25_paper_1,264,269,1703.06211,1312.6229,Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses ImageNet Detection dataset.","sent: Article 1 uses D-RFCN   ResNet-101  6 scales  approach for Object Detection, while Article 2 uses OverFeat for Object Detection."
1157,1157,25_paper_8,25_paper_6,264,2610,1703.06211,1512.03385,Object Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  CIFAR-10, and  PASCAL VOC 2007.","sent: Article 1 uses D-RFCN   ResNet-101  6 scales  approach for Object Detection, while Article 2 uses ResNet-101 for Object Detection. sent: Article 1 uses D-RFCN   ResNet-101  6 scales  approach for Object Detection, while Article 2 uses ResNet for Object Detection. sent: Article 1 uses D-RFCN   ResNet-101  6 scales  approach for Object Detection, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Object Detection."
1158,1158,25_paper_8,25_paper_15,264,2611,1703.06211,1506.01497,Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses PASCAL VOC 2007 dataset.","sent: Article 1 uses D-RFCN   ResNet-101  6 scales  approach for Object Detection, while Article 2 uses Faster R-CNN for Object Detection."
1159,1159,25_paper_8,25_paper_9,264,2612,1703.06211,1704.03414,Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses PASCAL VOC 2007 dataset.","sent: Article 1 uses D-RFCN   ResNet-101  6 scales  approach for Object Detection, while Article 2 uses FRCN for Object Detection."
1160,1160,25_paper_8,25_paper_5,264,2613,1703.06211,1612.03144,Object Detection,sent: Article 1 and Article 2 use COCO dataset.,"sent: Article 1 uses D-RFCN   ResNet-101  6 scales  approach for Object Detection, while Article 2 uses Faster R-CNN   FPN for Object Detection."
1161,1161,25_paper_8,25_paper_4,264,2614,1703.06211,1708.04896,Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses D-RFCN   ResNet-101  6 scales  approach for Object Detection, while Article 2 uses TriNet   Random Erasing for Object Detection. sent: Article 1 uses D-RFCN   ResNet-101  6 scales  approach for Object Detection, while Article 2 uses SVDNet   Random Erasing for Object Detection. sent: Article 1 uses D-RFCN   ResNet-101  6 scales  approach for Object Detection, while Article 2 uses I ORE for Object Detection. sent: Article 1 uses D-RFCN   ResNet-101  6 scales  approach for Object Detection, while Article 2 uses Random Erasing for Object Detection."
1162,1162,25_paper_14,25_paper_12,265,266,1504.08083,1904.089,Object Detection,"sent: Article 1 uses PASCAL VOC 2007 dataset, while Article 2 uses COCO dataset.","sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses CornerNet-Saccade for Object Detection. sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses CornerNet-Squeeze for Object Detection."
1163,1163,25_paper_14,25_paper_2,265,267,1504.08083,1703.0687,Object Detection,"sent: Article 1 uses PASCAL VOC 2007 dataset, while Article 2 uses multiple datasets:  COCO,  MHP v1 0, and  MHP v2 0.","sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses Mask R-CNN for Object Detection."
1164,1164,25_paper_14,25_paper_0,265,268,1504.08083,1712.00726,Object Detection,"sent: Article 1 uses PASCAL VOC 2007 dataset, while Article 2 uses COCO dataset.","sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses Cascade R-CNN for Object Detection."
1165,1165,25_paper_14,25_paper_1,265,269,1504.08083,1312.6229,Object Detection,"sent: Article 1 uses PASCAL VOC 2007 dataset, while Article 2 uses ImageNet Detection dataset.","sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses OverFeat for Object Detection."
1166,1166,25_paper_14,25_paper_6,265,2610,1504.08083,1512.03385,Object Detection,"sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  CIFAR-10, and  COCO.","sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses ResNet-101 for Object Detection. sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses ResNet for Object Detection. sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Object Detection."
1167,1167,25_paper_14,25_paper_15,265,2611,1504.08083,1506.01497,Object Detection,sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset.,"sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses Faster R-CNN for Object Detection."
1168,1168,25_paper_14,25_paper_9,265,2612,1504.08083,1704.03414,Object Detection,sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset.,sent: Article 1 and Article 2 use FRCNapproach for Object Detection.
1169,1169,25_paper_14,25_paper_5,265,2613,1504.08083,1612.03144,Object Detection,"sent: Article 1 uses PASCAL VOC 2007 dataset, while Article 2 uses COCO dataset.","sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses Faster R-CNN   FPN for Object Detection."
1170,1170,25_paper_14,25_paper_4,265,2614,1504.08083,1708.04896,Object Detection,"sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  DukeMTMC-reID, and  Fashion-MNIST.","sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses TriNet   Random Erasing for Object Detection. sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses SVDNet   Random Erasing for Object Detection. sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses I ORE for Object Detection. sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses Random Erasing for Object Detection."
1171,1171,25_paper_2,25_paper_0,267,268,1703.0687,1712.00726,Object Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 uses multiple datasets:  MHP v1 0, and  MHP v2 0, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Mask R-CNN approach for Object Detection, while Article 2 uses Cascade R-CNN for Object Detection."
1172,1172,25_paper_2,25_paper_1,267,269,1703.0687,1312.6229,Object Detection,"sent: Article 1 uses multiple datasets:  COCO,  MHP v1 0, and  MHP v2 0, while Article 2 uses ImageNet Detection dataset.","sent: Article 1 uses Mask R-CNN approach for Object Detection, while Article 2 uses OverFeat for Object Detection."
1173,1173,25_paper_2,25_paper_6,267,2610,1703.0687,1512.03385,Object Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 uses multiple datasets:  MHP v1 0, and  MHP v2 0, while Article 2 uses multiple datasets:  CIFAR-10, and  PASCAL VOC 2007.","sent: Article 1 uses Mask R-CNN approach for Object Detection, while Article 2 uses ResNet-101 for Object Detection. sent: Article 1 uses Mask R-CNN approach for Object Detection, while Article 2 uses ResNet for Object Detection. sent: Article 1 uses Mask R-CNN approach for Object Detection, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Object Detection."
1174,1174,25_paper_2,25_paper_15,267,2611,1703.0687,1506.01497,Object Detection,"sent: Article 1 uses multiple datasets:  COCO,  MHP v1 0, and  MHP v2 0, while Article 2 uses PASCAL VOC 2007 dataset.","sent: Article 1 uses Mask R-CNN approach for Object Detection, while Article 2 uses Faster R-CNN for Object Detection."
1175,1175,25_paper_2,25_paper_9,267,2612,1703.0687,1704.03414,Object Detection,"sent: Article 1 uses multiple datasets:  COCO,  MHP v1 0, and  MHP v2 0, while Article 2 uses PASCAL VOC 2007 dataset.","sent: Article 1 uses Mask R-CNN approach for Object Detection, while Article 2 uses FRCN for Object Detection."
1176,1176,25_paper_2,25_paper_5,267,2613,1703.0687,1612.03144,Object Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 uses multiple datasets:  MHP v1 0, and  MHP v2 0, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Mask R-CNN approach for Object Detection, while Article 2 uses Faster R-CNN   FPN for Object Detection."
1177,1177,25_paper_2,25_paper_4,267,2614,1703.0687,1708.04896,Object Detection,"sent: Article 1 uses multiple datasets:  COCO,  MHP v1 0, and  MHP v2 0, while Article 2 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses Mask R-CNN approach for Object Detection, while Article 2 uses TriNet   Random Erasing for Object Detection. sent: Article 1 uses Mask R-CNN approach for Object Detection, while Article 2 uses SVDNet   Random Erasing for Object Detection. sent: Article 1 uses Mask R-CNN approach for Object Detection, while Article 2 uses I ORE for Object Detection. sent: Article 1 uses Mask R-CNN approach for Object Detection, while Article 2 uses Random Erasing for Object Detection."
1178,1178,25_paper_0,25_paper_1,268,269,1712.00726,1312.6229,Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses ImageNet Detection dataset.","sent: Article 1 uses Cascade R-CNN approach for Object Detection, while Article 2 uses OverFeat for Object Detection."
1179,1179,25_paper_0,25_paper_6,268,2610,1712.00726,1512.03385,Object Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  CIFAR-10, and  PASCAL VOC 2007.","sent: Article 1 uses Cascade R-CNN approach for Object Detection, while Article 2 uses ResNet-101 for Object Detection. sent: Article 1 uses Cascade R-CNN approach for Object Detection, while Article 2 uses ResNet for Object Detection. sent: Article 1 uses Cascade R-CNN approach for Object Detection, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Object Detection."
1180,1180,25_paper_0,25_paper_15,268,2611,1712.00726,1506.01497,Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses PASCAL VOC 2007 dataset.","sent: Article 1 uses Cascade R-CNN approach for Object Detection, while Article 2 uses Faster R-CNN for Object Detection."
1181,1181,25_paper_0,25_paper_9,268,2612,1712.00726,1704.03414,Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses PASCAL VOC 2007 dataset.","sent: Article 1 uses Cascade R-CNN approach for Object Detection, while Article 2 uses FRCN for Object Detection."
1182,1182,25_paper_0,25_paper_5,268,2613,1712.00726,1612.03144,Object Detection,sent: Article 1 and Article 2 use COCO dataset.,"sent: Article 1 uses Cascade R-CNN approach for Object Detection, while Article 2 uses Faster R-CNN   FPN for Object Detection."
1183,1183,25_paper_0,25_paper_4,268,2614,1712.00726,1708.04896,Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses Cascade R-CNN approach for Object Detection, while Article 2 uses TriNet   Random Erasing for Object Detection. sent: Article 1 uses Cascade R-CNN approach for Object Detection, while Article 2 uses SVDNet   Random Erasing for Object Detection. sent: Article 1 uses Cascade R-CNN approach for Object Detection, while Article 2 uses I ORE for Object Detection. sent: Article 1 uses Cascade R-CNN approach for Object Detection, while Article 2 uses Random Erasing for Object Detection."
1184,1184,25_paper_1,25_paper_6,269,2610,1312.6229,1512.03385,Object Detection,"sent: Article 1 uses ImageNet Detection dataset, while Article 2 uses multiple datasets:  CIFAR-10,  COCO, and  PASCAL VOC 2007.","sent: Article 1 uses OverFeat approach for Object Detection, while Article 2 uses ResNet-101 for Object Detection. sent: Article 1 uses OverFeat approach for Object Detection, while Article 2 uses ResNet for Object Detection. sent: Article 1 uses OverFeat approach for Object Detection, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Object Detection."
1185,1185,25_paper_1,25_paper_15,269,2611,1312.6229,1506.01497,Object Detection,"sent: Article 1 uses ImageNet Detection dataset, while Article 2 uses PASCAL VOC 2007 dataset.","sent: Article 1 uses OverFeat approach for Object Detection, while Article 2 uses Faster R-CNN for Object Detection."
1186,1186,25_paper_1,25_paper_9,269,2612,1312.6229,1704.03414,Object Detection,"sent: Article 1 uses ImageNet Detection dataset, while Article 2 uses PASCAL VOC 2007 dataset.","sent: Article 1 uses OverFeat approach for Object Detection, while Article 2 uses FRCN for Object Detection."
1187,1187,25_paper_1,25_paper_5,269,2613,1312.6229,1612.03144,Object Detection,"sent: Article 1 uses ImageNet Detection dataset, while Article 2 uses COCO dataset.","sent: Article 1 uses OverFeat approach for Object Detection, while Article 2 uses Faster R-CNN   FPN for Object Detection."
1188,1188,25_paper_1,25_paper_4,269,2614,1312.6229,1708.04896,Object Detection,"sent: Article 1 uses ImageNet Detection dataset, while Article 2 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses OverFeat approach for Object Detection, while Article 2 uses TriNet   Random Erasing for Object Detection. sent: Article 1 uses OverFeat approach for Object Detection, while Article 2 uses SVDNet   Random Erasing for Object Detection. sent: Article 1 uses OverFeat approach for Object Detection, while Article 2 uses I ORE for Object Detection. sent: Article 1 uses OverFeat approach for Object Detection, while Article 2 uses Random Erasing for Object Detection."
1189,1189,25_paper_6,25_paper_15,2610,2611,1512.03385,1506.01497,Object Detection,"sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 uses multiple datasets:  CIFAR-10, and  COCO, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ResNet-101 approach for Object Detection, while Article 2 uses Faster R-CNN for Object Detection. sent: Article 1 uses ResNet approach for Object Detection, while Article 2 uses Faster R-CNN for Object Detection. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Object Detection, while Article 2 uses Faster R-CNN for Object Detection."
1190,1190,25_paper_6,25_paper_9,2610,2612,1512.03385,1704.03414,Object Detection,"sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 uses multiple datasets:  CIFAR-10, and  COCO, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ResNet-101 approach for Object Detection, while Article 2 uses FRCN for Object Detection. sent: Article 1 uses ResNet approach for Object Detection, while Article 2 uses FRCN for Object Detection. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Object Detection, while Article 2 uses FRCN for Object Detection."
1191,1191,25_paper_6,25_paper_5,2610,2613,1512.03385,1612.03144,Object Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 uses multiple datasets:  CIFAR-10, and  PASCAL VOC 2007, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ResNet-101 approach for Object Detection, while Article 2 uses Faster R-CNN   FPN for Object Detection. sent: Article 1 uses ResNet approach for Object Detection, while Article 2 uses Faster R-CNN   FPN for Object Detection. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Object Detection, while Article 2 uses Faster R-CNN   FPN for Object Detection."
1192,1192,25_paper_6,25_paper_4,2610,2614,1512.03385,1708.04896,Object Detection,"sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 uses multiple datasets:  CIFAR-10, and  COCO, while Article 2 uses multiple datasets:  DukeMTMC-reID, and  Fashion-MNIST.","sent: Article 1 uses ResNet-101 approach for Object Detection, while Article 2 uses TriNet   Random Erasing for Object Detection. sent: Article 1 uses ResNet-101 approach for Object Detection, while Article 2 uses SVDNet   Random Erasing for Object Detection. sent: Article 1 uses ResNet-101 approach for Object Detection, while Article 2 uses I ORE for Object Detection. sent: Article 1 uses ResNet-101 approach for Object Detection, while Article 2 uses Random Erasing for Object Detection. sent: Article 1 uses ResNet approach for Object Detection, while Article 2 uses TriNet   Random Erasing for Object Detection. sent: Article 1 uses ResNet approach for Object Detection, while Article 2 uses SVDNet   Random Erasing for Object Detection. sent: Article 1 uses ResNet approach for Object Detection, while Article 2 uses I ORE for Object Detection. sent: Article 1 uses ResNet approach for Object Detection, while Article 2 uses Random Erasing for Object Detection. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Object Detection, while Article 2 uses TriNet   Random Erasing for Object Detection. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Object Detection, while Article 2 uses SVDNet   Random Erasing for Object Detection. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Object Detection, while Article 2 uses I ORE for Object Detection. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Object Detection, while Article 2 uses Random Erasing for Object Detection. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses Random Erasing for Image Classification."
1193,1193,25_paper_15,25_paper_9,2611,2612,1506.01497,1704.03414,Object Detection,sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset.,"sent: Article 1 uses Faster R-CNN approach for Object Detection, while Article 2 uses FRCN for Object Detection."
1194,1194,25_paper_15,25_paper_5,2611,2613,1506.01497,1612.03144,Object Detection,"sent: Article 1 uses PASCAL VOC 2007 dataset, while Article 2 uses COCO dataset.","sent: Article 1 uses Faster R-CNN approach for Object Detection, while Article 2 uses Faster R-CNN   FPN for Object Detection."
1195,1195,25_paper_15,25_paper_4,2611,2614,1506.01497,1708.04896,Object Detection,"sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  DukeMTMC-reID, and  Fashion-MNIST.","sent: Article 1 uses Faster R-CNN approach for Object Detection, while Article 2 uses TriNet   Random Erasing for Object Detection. sent: Article 1 uses Faster R-CNN approach for Object Detection, while Article 2 uses SVDNet   Random Erasing for Object Detection. sent: Article 1 uses Faster R-CNN approach for Object Detection, while Article 2 uses I ORE for Object Detection. sent: Article 1 uses Faster R-CNN approach for Object Detection, while Article 2 uses Random Erasing for Object Detection."
1196,1196,25_paper_15,25_paper_7,2611,2615,1506.01497,1512.03385,Object Detection,"sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  CIFAR-10, and  COCO.","sent: Article 1 uses Faster R-CNN approach for Object Detection, while Article 2 uses ResNet-101 for Object Detection. sent: Article 1 uses Faster R-CNN approach for Object Detection, while Article 2 uses ResNet for Object Detection. sent: Article 1 uses Faster R-CNN approach for Object Detection, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Object Detection."
1197,1197,25_paper_9,25_paper_5,2612,2613,1704.03414,1612.03144,Object Detection,"sent: Article 1 uses PASCAL VOC 2007 dataset, while Article 2 uses COCO dataset.","sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses Faster R-CNN   FPN for Object Detection."
1198,1198,25_paper_9,25_paper_4,2612,2614,1704.03414,1708.04896,Object Detection,"sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  DukeMTMC-reID, and  Fashion-MNIST.","sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses TriNet   Random Erasing for Object Detection. sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses SVDNet   Random Erasing for Object Detection. sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses I ORE for Object Detection. sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses Random Erasing for Object Detection."
1199,1199,25_paper_9,25_paper_7,2612,2615,1704.03414,1512.03385,Object Detection,"sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  CIFAR-10, and  COCO.","sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses ResNet-101 for Object Detection. sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses ResNet for Object Detection. sent: Article 1 uses FRCN approach for Object Detection, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Object Detection."
1200,1200,25_paper_5,25_paper_4,2613,2614,1612.03144,1708.04896,Object Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses Faster R-CNN   FPN approach for Object Detection, while Article 2 uses TriNet   Random Erasing for Object Detection. sent: Article 1 uses Faster R-CNN   FPN approach for Object Detection, while Article 2 uses SVDNet   Random Erasing for Object Detection. sent: Article 1 uses Faster R-CNN   FPN approach for Object Detection, while Article 2 uses I ORE for Object Detection. sent: Article 1 uses Faster R-CNN   FPN approach for Object Detection, while Article 2 uses Random Erasing for Object Detection."
1201,1201,25_paper_5,25_paper_7,2613,2615,1612.03144,1512.03385,Object Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  CIFAR-10, and  PASCAL VOC 2007.","sent: Article 1 uses Faster R-CNN   FPN approach for Object Detection, while Article 2 uses ResNet-101 for Object Detection. sent: Article 1 uses Faster R-CNN   FPN approach for Object Detection, while Article 2 uses ResNet for Object Detection. sent: Article 1 uses Faster R-CNN   FPN approach for Object Detection, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Object Detection."
1202,1202,27_paper_5,27_paper_1,280,281,1802.05751,1605.08803,Image Generation,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 uses Image Transformer approach for Image Generation, while Article 2 uses Real NVP for Image Generation. sent: Article 1 uses Image Transformer approach for Image Generation, while Article 2 uses PixelRNN for Image Generation."
1203,1203,27_paper_5,27_paper_6,280,282,1802.05751,1601.06759,Image Generation,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 uses Image Transformer approach for Image Generation, while Article 2 uses PixelCNN for Image Generation. sent: Article 1 uses Image Transformer approach for Image Generation, while Article 2 uses NICE for Image Generation. sent: Article 1 uses Image Transformer approach for Image Generation, while Article 2 uses PixelRNN for Image Generation."
1204,1204,27_paper_5,27_paper_3,280,285,1802.05751,1812.09916,Image Generation,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 uses Image Transformer approach for Image Generation, while Article 2 uses MMD-GAN-rep for Image Generation."
1205,1205,27_paper_5,27_paper_11,280,288,1802.05751,1409.366,Image Generation,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 uses Image Transformer approach for Image Generation, while Article 2 uses Deep Diffusion for Image Generation."
1206,1206,27_paper_5,27_paper_12,280,289,1802.05751,1611.01673,Image Generation,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 uses Image Transformer approach for Image Generation, while Article 2 uses GMAN for Image Generation."
1207,1207,27_paper_1,27_paper_6,281,282,1605.08803,1601.06759,Image Generation,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 and Article 2 use PixelRNNapproach for Image Generation. sent: Article 1 uses Real NVP approach for Image Generation, while Article 2 uses PixelCNN for Image Generation. sent: Article 1 uses Real NVP approach for Image Generation, while Article 2 uses NICE for Image Generation."
1208,1208,27_paper_1,27_paper_3,281,285,1605.08803,1812.09916,Image Generation,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 uses Real NVP approach for Image Generation, while Article 2 uses MMD-GAN-rep for Image Generation. sent: Article 1 uses PixelRNN approach for Image Generation, while Article 2 uses MMD-GAN-rep for Image Generation."
1209,1209,27_paper_1,27_paper_11,281,288,1605.08803,1409.366,Image Generation,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 uses Real NVP approach for Image Generation, while Article 2 uses Deep Diffusion for Image Generation. sent: Article 1 uses PixelRNN approach for Image Generation, while Article 2 uses Deep Diffusion for Image Generation."
1210,1210,27_paper_1,27_paper_12,281,289,1605.08803,1611.01673,Image Generation,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 uses Real NVP approach for Image Generation, while Article 2 uses GMAN for Image Generation. sent: Article 1 uses PixelRNN approach for Image Generation, while Article 2 uses GMAN for Image Generation."
1211,1211,27_paper_6,27_paper_2,282,283,1601.06759,1605.08803,Image Generation,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 and Article 2 use PixelRNNapproach for Image Generation. sent: Article 1 uses PixelCNN approach for Image Generation, while Article 2 uses Real NVP for Image Generation. sent: Article 1 uses NICE approach for Image Generation, while Article 2 uses Real NVP for Image Generation."
1212,1212,27_paper_6,27_paper_3,282,285,1601.06759,1812.09916,Image Generation,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 uses PixelCNN approach for Image Generation, while Article 2 uses MMD-GAN-rep for Image Generation. sent: Article 1 uses NICE approach for Image Generation, while Article 2 uses MMD-GAN-rep for Image Generation. sent: Article 1 uses PixelRNN approach for Image Generation, while Article 2 uses MMD-GAN-rep for Image Generation."
1213,1213,27_paper_6,27_paper_11,282,288,1601.06759,1409.366,Image Generation,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 uses PixelCNN approach for Image Generation, while Article 2 uses Deep Diffusion for Image Generation. sent: Article 1 uses NICE approach for Image Generation, while Article 2 uses Deep Diffusion for Image Generation. sent: Article 1 uses PixelRNN approach for Image Generation, while Article 2 uses Deep Diffusion for Image Generation."
1214,1214,27_paper_6,27_paper_12,282,289,1601.06759,1611.01673,Image Generation,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 uses PixelCNN approach for Image Generation, while Article 2 uses GMAN for Image Generation. sent: Article 1 uses NICE approach for Image Generation, while Article 2 uses GMAN for Image Generation. sent: Article 1 uses PixelRNN approach for Image Generation, while Article 2 uses GMAN for Image Generation."
1215,1215,27_paper_3,27_paper_8,285,286,1812.09916,1601.06759,Image Generation,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 uses MMD-GAN-rep approach for Image Generation, while Article 2 uses PixelCNN for Image Generation. sent: Article 1 uses MMD-GAN-rep approach for Image Generation, while Article 2 uses NICE for Image Generation. sent: Article 1 uses MMD-GAN-rep approach for Image Generation, while Article 2 uses PixelRNN for Image Generation."
1216,1216,27_paper_3,27_paper_11,285,288,1812.09916,1409.366,Image Generation,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 uses MMD-GAN-rep approach for Image Generation, while Article 2 uses Deep Diffusion for Image Generation."
1217,1217,27_paper_3,27_paper_12,285,289,1812.09916,1611.01673,Image Generation,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 uses MMD-GAN-rep approach for Image Generation, while Article 2 uses GMAN for Image Generation."
1218,1218,27_paper_11,27_paper_12,288,289,1409.366,1611.01673,Image Generation,sent: Article 1 and Article 2 use CIFAR-10 dataset.,"sent: Article 1 uses Deep Diffusion approach for Image Generation, while Article 2 uses GMAN for Image Generation."
1219,1219,28_paper_2,28_paper_3,290,293,1704.00138,1511.02853,Weakly Supervised Object Detection,"sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Online Instance Classifier Refinement approach for Weakly Supervised Object Detection, while Article 2 uses WSDDN-Ens for Weakly Supervised Object Detection. sent: Article 1 uses OICR-Ens   FRCNN approach for Weakly Supervised Object Detection, while Article 2 uses WSDDN-Ens for Weakly Supervised Object Detection."
1220,1220,31_paper_2,31_paper_0,320,321,1901.0528,1804.08199,Semantic Role Labeling,"sent: Article 1 and Article 2 use CoNLL 2005 dataset. sent: Article 1 uses OntoNotes dataset, while Article 2 uses CoNLL 2012 dataset.","sent: Article 1 uses Li et al  approach for Semantic Role Labeling, while Article 2 uses LISA for Semantic Role Labeling. sent: Article 1 uses Li et al  approach for Semantic Role Labeling, while Article 2 uses LISA   ELMo for Semantic Role Labeling."
1221,1221,31_paper_2,31_paper_3,320,323,1901.0528,1802.05365,Semantic Role Labeling,"sent: Article 1 and Article 2 use OntoNotes dataset. sent: Article 1 uses CoNLL 2005 dataset, while Article 2 uses multiple datasets:  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  ACL-ARC,  SQuAD2 0,  SST-5 Fine-grained classification, and  SQuAD1 1.","sent: Article 1 uses Li et al  approach for Semantic Role Labeling, while Article 2 uses BiLSTM-CRF ELMo for Semantic Role Labeling. sent: Article 1 uses Li et al  approach for Semantic Role Labeling, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Semantic Role Labeling. sent: Article 1 uses Li et al  approach for Semantic Role Labeling, while Article 2 uses  Lee et al   2017  ELMo for Semantic Role Labeling. sent: Article 1 uses Li et al  approach for Semantic Role Labeling, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Semantic Role Labeling. sent: Article 1 uses Li et al  approach for Semantic Role Labeling, while Article 2 uses BCN ELMo for Semantic Role Labeling. sent: Article 1 uses Li et al  approach for Semantic Role Labeling, while Article 2 uses ESIM   ELMo Ensemble for Semantic Role Labeling. sent: Article 1 uses Li et al  approach for Semantic Role Labeling, while Article 2 uses  He et al   2017    ELMo for Semantic Role Labeling. sent: Article 1 uses Li et al  approach for Semantic Role Labeling, while Article 2 uses ESIM   ELMo for Semantic Role Labeling. sent: Article 1 uses Li et al  approach for Semantic Role Labeling, while Article 2 uses BiLSTM-Attention   ELMo for Semantic Role Labeling."
1222,1222,31_paper_0,31_paper_1,321,322,1804.08199,1901.0528,Semantic Role Labeling,"sent: Article 1 and Article 2 use CoNLL 2005 dataset. sent: Article 1 uses CoNLL 2012 dataset, while Article 2 uses OntoNotes dataset.","sent: Article 1 uses LISA approach for Semantic Role Labeling, while Article 2 uses Li et al  for Semantic Role Labeling. sent: Article 1 uses LISA   ELMo approach for Semantic Role Labeling, while Article 2 uses Li et al  for Semantic Role Labeling."
1223,1223,31_paper_0,31_paper_3,321,323,1804.08199,1802.05365,Semantic Role Labeling,"sent: Article 1 and Article 2 use CoNLL 2012 dataset. sent: Article 1 uses CoNLL 2005 dataset, while Article 2 uses multiple datasets:  OntoNotes,  SNLI,  CoNLL 2003  English ,  ACL-ARC,  SQuAD2 0,  SST-5 Fine-grained classification, and  SQuAD1 1.","sent: Article 1 uses LISA approach for Semantic Role Labeling, while Article 2 uses BiLSTM-CRF ELMo for Semantic Role Labeling. sent: Article 1 uses LISA approach for Semantic Role Labeling, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Semantic Role Labeling. sent: Article 1 uses LISA approach for Semantic Role Labeling, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Semantic Role Labeling. sent: Article 1 uses LISA approach for Semantic Role Labeling, while Article 2 uses  Lee et al   2017  ELMo for Semantic Role Labeling. sent: Article 1 uses LISA approach for Semantic Role Labeling, while Article 2 uses BCN ELMo for Semantic Role Labeling. sent: Article 1 uses LISA approach for Semantic Role Labeling, while Article 2 uses ESIM   ELMo Ensemble for Semantic Role Labeling. sent: Article 1 uses LISA approach for Semantic Role Labeling, while Article 2 uses  He et al   2017    ELMo for Semantic Role Labeling. sent: Article 1 uses LISA approach for Semantic Role Labeling, while Article 2 uses ESIM   ELMo for Semantic Role Labeling. sent: Article 1 uses LISA approach for Semantic Role Labeling, while Article 2 uses BiLSTM-Attention   ELMo for Semantic Role Labeling. sent: Article 1 uses LISA   ELMo approach for Semantic Role Labeling, while Article 2 uses BiLSTM-CRF ELMo for Semantic Role Labeling. sent: Article 1 uses LISA   ELMo approach for Semantic Role Labeling, while Article 2 uses BiDAF   Self Attention   ELMo  ensemble  for Semantic Role Labeling. sent: Article 1 uses LISA   ELMo approach for Semantic Role Labeling, while Article 2 uses BiDAF   Self Attention   ELMo  single model  for Semantic Role Labeling. sent: Article 1 uses LISA   ELMo approach for Semantic Role Labeling, while Article 2 uses  Lee et al   2017  ELMo for Semantic Role Labeling. sent: Article 1 uses LISA   ELMo approach for Semantic Role Labeling, while Article 2 uses BCN ELMo for Semantic Role Labeling. sent: Article 1 uses LISA   ELMo approach for Semantic Role Labeling, while Article 2 uses ESIM   ELMo Ensemble for Semantic Role Labeling. sent: Article 1 uses LISA   ELMo approach for Semantic Role Labeling, while Article 2 uses  He et al   2017    ELMo for Semantic Role Labeling. sent: Article 1 uses LISA   ELMo approach for Semantic Role Labeling, while Article 2 uses ESIM   ELMo for Semantic Role Labeling. sent: Article 1 uses LISA   ELMo approach for Semantic Role Labeling, while Article 2 uses BiLSTM-Attention   ELMo for Semantic Role Labeling."
1224,1224,35_paper_0,35_paper_11,360,362,1505.04597,1802.06955,Retinal Vessel Segmentation,"sent: Article 1 and Article 2 use LUNA dataset.sent: Article 1 and Article 2 use DRIVE dataset.sent: Article 1 and Article 2 use CHASE DB1 dataset.sent: Article 1 and Article 2 use STARE dataset.sent: Article 1 and Article 2 use Kaggle Skin Lesion Segmentation dataset. sent: Article 1 uses multiple datasets:  PhC-U373,  ISBI 2012 EM Segmentation,  DIC-HeLa, and  CT-150, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses U-Net approach for Lung Nodule Segmentation, while Article 2 uses R2U-Net for Lung Nodule Segmentation. sent: Article 1 uses U-Net approach for Skin Cancer Segmentation, while Article 2 uses R2U-Net for Skin Cancer Segmentation. sent: Article 1 uses U-Net approach for Retinal Vessel Segmentation, while Article 2 uses R2U-Net for Retinal Vessel Segmentation."
1225,1225,35_paper_11,35_paper_4,362,365,1802.06955,1505.04597,Retinal Vessel Segmentation,"sent: Article 1 and Article 2 use LUNA dataset.sent: Article 1 and Article 2 use DRIVE dataset.sent: Article 1 and Article 2 use CHASE DB1 dataset.sent: Article 1 and Article 2 use STARE dataset.sent: Article 1 and Article 2 use Kaggle Skin Lesion Segmentation dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  PhC-U373,  ISBI 2012 EM Segmentation,  DIC-HeLa, and  CT-150.","sent: Article 1 uses R2U-Net approach for Lung Nodule Segmentation, while Article 2 uses U-Net for Lung Nodule Segmentation. sent: Article 1 uses R2U-Net approach for Skin Cancer Segmentation, while Article 2 uses U-Net for Skin Cancer Segmentation. sent: Article 1 uses R2U-Net approach for Retinal Vessel Segmentation, while Article 2 uses U-Net for Retinal Vessel Segmentation."
1226,1226,41_paper_3,41_paper_15,420,421,1603.06147,1508.07909,Machine Translation,"sent: Article 1 and Article 2 use WMT2015 English-German dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses WMT2015 English-Russian dataset.","sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses C2-50k Segmentation for Machine Translation. sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses BPE word segmentation for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses C2-50k Segmentation for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses BPE word segmentation for Machine Translation."
1227,1227,41_paper_3,41_paper_37,420,422,1603.06147,1409.3215,Machine Translation,"sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation."
1228,1228,41_paper_3,41_paper_12,420,423,1603.06147,1607.07086,Machine Translation,"sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses multiple datasets:  IWSLT2015 German-English, and  IWSLT2015 English-German.","sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses RNNsearch for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses RNNsearch for Machine Translation."
1229,1229,41_paper_3,41_paper_24,420,424,1603.06147,1606.02891,Machine Translation,"sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.","sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation."
1230,1230,41_paper_3,41_paper_18,420,425,1603.06147,1711.02281,Machine Translation,"sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian.","sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation."
1231,1231,41_paper_3,41_paper_32,420,426,1603.06147,1606.07947,Machine Translation,"sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English.","sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation."
1232,1232,41_paper_3,41_paper_8,420,427,1603.06147,1706.03762,Machine Translation,"sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 German-English,  IWSLT2015 English-German,  Penn Treebank, and  WMT2014 English-French.","sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation."
1233,1233,41_paper_3,41_paper_39,420,4210,1603.06147,1705.03122,Machine Translation,"sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 German-English,  IWSLT2015 English-German,  WMT2014 English-French, and  WMT2016 English-Romanian.","sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation."
1234,1234,41_paper_3,41_paper_48,420,4211,1603.06147,1610.10099,Machine Translation,"sent: Article 1 and Article 2 use WMT2015 English-German dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French.","sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation."
1235,1235,41_paper_3,41_paper_35,420,4212,1603.06147,1711.02132,Machine Translation,"sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French.","sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation."
1236,1236,41_paper_3,41_paper_34,420,4220,1603.06147,1511.06732,Machine Translation,"sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation."
1237,1237,41_paper_3,41_paper_0,420,4221,1603.06147,1409.0473,Machine Translation,"sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses multiple datasets:  IWSLT2015 German-English, and  WMT2014 English-French.","sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses Bi-GRU  MLE SLE  for Machine Translation. sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses RNN-search50  for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses Bi-GRU  MLE SLE  for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses RNN-search50  for Machine Translation."
1238,1238,41_paper_3,41_paper_31,420,4227,1603.06147,1611.01576,Machine Translation,"sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses QRNN for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses QRNN for Machine Translation."
1239,1239,41_paper_3,41_paper_30,420,4229,1603.06147,1606.0296,Machine Translation,"sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation."
1240,1240,41_paper_3,41_paper_21,420,4231,1603.06147,1711.01068,Machine Translation,"sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses DCCL for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses DCCL for Machine Translation."
1241,1241,41_paper_3,41_paper_2,420,4234,1603.06147,1706.03059,Machine Translation,"sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses WMT2014 English-German dataset.","sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation."
1242,1242,41_paper_3,41_paper_11,420,4239,1603.06147,1406.1078,Machine Translation,"sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation."
1243,1243,41_paper_15,41_paper_37,421,422,1508.07909,1409.3215,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation."
1244,1244,41_paper_15,41_paper_12,421,423,1508.07909,1607.07086,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German, while Article 2 uses multiple datasets:  IWSLT2015 German-English, and  IWSLT2015 English-German.","sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses RNNsearch for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses RNNsearch for Machine Translation."
1245,1245,41_paper_15,41_paper_24,421,424,1508.07909,1606.02891,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.","sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation."
1246,1246,41_paper_15,41_paper_18,421,425,1508.07909,1711.02281,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian.","sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation."
1247,1247,41_paper_15,41_paper_32,421,426,1508.07909,1606.07947,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German, while Article 2 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English.","sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation."
1248,1248,41_paper_15,41_paper_8,421,427,1508.07909,1706.03762,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 German-English,  IWSLT2015 English-German,  Penn Treebank, and  WMT2014 English-French.","sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation."
1249,1249,41_paper_15,41_paper_39,421,4210,1508.07909,1705.03122,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 German-English,  IWSLT2015 English-German,  WMT2014 English-French, and  WMT2016 English-Romanian.","sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation."
1250,1250,41_paper_15,41_paper_48,421,4211,1508.07909,1610.10099,Machine Translation,"sent: Article 1 and Article 2 use WMT2015 English-German dataset. sent: Article 1 uses WMT2015 English-Russian dataset, while Article 2 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French.","sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation."
1251,1251,41_paper_15,41_paper_35,421,4212,1508.07909,1711.02132,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German, while Article 2 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French.","sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation."
1252,1252,41_paper_15,41_paper_34,421,4220,1508.07909,1511.06732,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation."
1253,1253,41_paper_15,41_paper_0,421,4221,1508.07909,1409.0473,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German, while Article 2 uses multiple datasets:  IWSLT2015 German-English, and  WMT2014 English-French.","sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses Bi-GRU  MLE SLE  for Machine Translation. sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses RNN-search50  for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses Bi-GRU  MLE SLE  for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses RNN-search50  for Machine Translation."
1254,1254,41_paper_15,41_paper_4,421,4222,1508.07909,1603.06147,Machine Translation,"sent: Article 1 and Article 2 use WMT2015 English-German dataset. sent: Article 1 uses WMT2015 English-Russian dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation."
1255,1255,41_paper_15,41_paper_31,421,4227,1508.07909,1611.01576,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses QRNN for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses QRNN for Machine Translation."
1256,1256,41_paper_15,41_paper_30,421,4229,1508.07909,1606.0296,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation."
1257,1257,41_paper_15,41_paper_21,421,4231,1508.07909,1711.01068,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses DCCL for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses DCCL for Machine Translation."
1258,1258,41_paper_15,41_paper_2,421,4234,1508.07909,1706.03059,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German, while Article 2 uses WMT2014 English-German dataset.","sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation."
1259,1259,41_paper_15,41_paper_11,421,4239,1508.07909,1406.1078,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses C2-50k Segmentation approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation. sent: Article 1 uses BPE word segmentation approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation."
1260,1260,41_paper_37,41_paper_12,422,423,1409.3215,1607.07086,Machine Translation,"sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses multiple datasets:  IWSLT2015 German-English, and  IWSLT2015 English-German.","sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses RNNsearch for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses RNNsearch for Machine Translation."
1261,1261,41_paper_37,41_paper_24,422,424,1409.3215,1606.02891,Machine Translation,"sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.","sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation."
1262,1262,41_paper_37,41_paper_18,422,425,1409.3215,1711.02281,Machine Translation,"sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian.","sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation."
1263,1263,41_paper_37,41_paper_32,422,426,1409.3215,1606.07947,Machine Translation,"sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English.","sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation."
1264,1264,41_paper_37,41_paper_8,422,427,1409.3215,1706.03762,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  Penn Treebank, and  IWSLT2015 German-English.","sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation."
1265,1265,41_paper_37,41_paper_39,422,4210,1409.3215,1705.03122,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2016 English-Romanian,  IWSLT2015 English-German, and  IWSLT2015 German-English.","sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation."
1266,1266,41_paper_37,41_paper_48,422,4211,1409.3215,1610.10099,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German, and  WMT2015 English-German.","sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation."
1267,1267,41_paper_37,41_paper_35,422,4212,1409.3215,1711.02132,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses WMT2014 English-German dataset.","sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation."
1268,1268,41_paper_37,41_paper_14,422,4214,1409.3215,1508.07909,Machine Translation,"sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German.","sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses C2-50k Segmentation for Machine Translation. sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses BPE word segmentation for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses C2-50k Segmentation for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses BPE word segmentation for Machine Translation."
1269,1269,41_paper_37,41_paper_34,422,4220,1409.3215,1511.06732,Machine Translation,"sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation."
1270,1270,41_paper_37,41_paper_0,422,4221,1409.3215,1409.0473,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses Bi-GRU  MLE SLE  for Machine Translation. sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses RNN-search50  for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses Bi-GRU  MLE SLE  for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses RNN-search50  for Machine Translation."
1271,1271,41_paper_37,41_paper_4,422,4222,1409.3215,1603.06147,Machine Translation,"sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses WMT2015 English-German dataset.","sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation."
1272,1272,41_paper_37,41_paper_31,422,4227,1409.3215,1611.01576,Machine Translation,"sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses QRNN for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses QRNN for Machine Translation."
1273,1273,41_paper_37,41_paper_30,422,4229,1409.3215,1606.0296,Machine Translation,"sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation."
1274,1274,41_paper_37,41_paper_21,422,4231,1409.3215,1711.01068,Machine Translation,"sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses DCCL for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses DCCL for Machine Translation."
1275,1275,41_paper_37,41_paper_2,422,4234,1409.3215,1706.03059,Machine Translation,"sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses WMT2014 English-German dataset.","sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation."
1276,1276,41_paper_37,41_paper_11,422,4239,1409.3215,1406.1078,Machine Translation,sent: Article 1 and Article 2 use WMT2014 English-French dataset.,"sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation."
1277,1277,41_paper_12,41_paper_24,423,424,1607.07086,1606.02891,Machine Translation,"sent: Article 1 uses multiple datasets:  IWSLT2015 German-English, and  IWSLT2015 English-German, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.","sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation."
1278,1278,41_paper_12,41_paper_18,423,425,1607.07086,1711.02281,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 English-German dataset. sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2016 English-Romanian,  WMT2014 German-English, and  WMT2016 Romanian-English.","sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation."
1279,1279,41_paper_12,41_paper_32,423,426,1607.07086,1606.07947,Machine Translation,"sent: Article 1 uses multiple datasets:  IWSLT2015 German-English, and  IWSLT2015 English-German, while Article 2 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English.","sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation."
1280,1280,41_paper_12,41_paper_8,423,427,1607.07086,1706.03762,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset.sent: Article 1 and Article 2 use IWSLT2015 English-German dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  Penn Treebank.","sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation."
1281,1281,41_paper_12,41_paper_39,423,4210,1607.07086,1705.03122,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset.sent: Article 1 and Article 2 use IWSLT2015 English-German dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  WMT2016 English-Romanian.","sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation."
1282,1282,41_paper_12,41_paper_48,423,4211,1607.07086,1610.10099,Machine Translation,"sent: Article 1 uses multiple datasets:  IWSLT2015 German-English, and  IWSLT2015 English-German, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  WMT2015 English-German.","sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation."
1283,1283,41_paper_12,41_paper_35,423,4212,1607.07086,1711.02132,Machine Translation,"sent: Article 1 uses multiple datasets:  IWSLT2015 German-English, and  IWSLT2015 English-German, while Article 2 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French.","sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation."
1284,1284,41_paper_12,41_paper_14,423,4214,1607.07086,1508.07909,Machine Translation,"sent: Article 1 uses multiple datasets:  IWSLT2015 German-English, and  IWSLT2015 English-German, while Article 2 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German.","sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses C2-50k Segmentation for Machine Translation. sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses BPE word segmentation for Machine Translation."
1285,1285,41_paper_12,41_paper_38,423,4218,1607.07086,1409.3215,Machine Translation,"sent: Article 1 uses multiple datasets:  IWSLT2015 German-English, and  IWSLT2015 English-German, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation."
1286,1286,41_paper_12,41_paper_34,423,4220,1607.07086,1511.06732,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 uses IWSLT2015 English-German dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation."
1287,1287,41_paper_12,41_paper_0,423,4221,1607.07086,1409.0473,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 uses IWSLT2015 English-German dataset, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses Bi-GRU  MLE SLE  for Machine Translation. sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses RNN-search50  for Machine Translation."
1288,1288,41_paper_12,41_paper_4,423,4222,1607.07086,1603.06147,Machine Translation,"sent: Article 1 uses multiple datasets:  IWSLT2015 German-English, and  IWSLT2015 English-German, while Article 2 uses WMT2015 English-German dataset.","sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation."
1289,1289,41_paper_12,41_paper_31,423,4227,1607.07086,1611.01576,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 uses IWSLT2015 English-German dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses QRNN for Machine Translation."
1290,1290,41_paper_12,41_paper_30,423,4229,1607.07086,1606.0296,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 uses IWSLT2015 English-German dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation."
1291,1291,41_paper_12,41_paper_21,423,4231,1607.07086,1711.01068,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 uses IWSLT2015 English-German dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses DCCL for Machine Translation."
1292,1292,41_paper_12,41_paper_2,423,4234,1607.07086,1706.03059,Machine Translation,"sent: Article 1 uses multiple datasets:  IWSLT2015 German-English, and  IWSLT2015 English-German, while Article 2 uses WMT2014 English-German dataset.","sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation."
1293,1293,41_paper_12,41_paper_11,423,4239,1607.07086,1406.1078,Machine Translation,"sent: Article 1 uses multiple datasets:  IWSLT2015 German-English, and  IWSLT2015 English-German, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses RNNsearch approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation."
1294,1294,41_paper_24,41_paper_18,424,425,1606.02891,1711.02281,Machine Translation,"sent: Article 1 and Article 2 use WMT2016 Romanian-English dataset.sent: Article 1 and Article 2 use WMT2016 English-Romanian dataset. sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Russian-English,  WMT2016 English-Czech, and  WMT2016 English-German, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German, and  WMT2014 German-English.","sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation."
1295,1295,41_paper_24,41_paper_32,424,426,1606.02891,1606.07947,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English.","sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation."
1296,1296,41_paper_24,41_paper_8,424,427,1606.02891,1706.03762,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 German-English,  IWSLT2015 English-German,  Penn Treebank, and  WMT2014 English-French.","sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation."
1297,1297,41_paper_24,41_paper_13,424,428,1606.02891,1607.07086,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses multiple datasets:  IWSLT2015 German-English, and  IWSLT2015 English-German.","sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses RNNsearch for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses RNNsearch for Machine Translation."
1298,1298,41_paper_24,41_paper_39,424,4210,1606.02891,1705.03122,Machine Translation,"sent: Article 1 and Article 2 use WMT2016 English-Romanian dataset. sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech, and  WMT2016 English-German, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French,  IWSLT2015 English-German, and  IWSLT2015 German-English.","sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation."
1299,1299,41_paper_24,41_paper_48,424,4211,1606.02891,1610.10099,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  WMT2015 English-German.","sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation."
1300,1300,41_paper_24,41_paper_35,424,4212,1606.02891,1711.02132,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French.","sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation."
1301,1301,41_paper_24,41_paper_14,424,4214,1606.02891,1508.07909,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German.","sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses C2-50k Segmentation for Machine Translation. sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses BPE word segmentation for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses C2-50k Segmentation for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses BPE word segmentation for Machine Translation."
1302,1302,41_paper_24,41_paper_38,424,4218,1606.02891,1409.3215,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation."
1303,1303,41_paper_24,41_paper_34,424,4220,1606.02891,1511.06732,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation."
1304,1304,41_paper_24,41_paper_0,424,4221,1606.02891,1409.0473,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses multiple datasets:  IWSLT2015 German-English, and  WMT2014 English-French.","sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses Bi-GRU  MLE SLE  for Machine Translation. sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses RNN-search50  for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses Bi-GRU  MLE SLE  for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses RNN-search50  for Machine Translation."
1305,1305,41_paper_24,41_paper_4,424,4222,1606.02891,1603.06147,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses WMT2015 English-German dataset.","sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation."
1306,1306,41_paper_24,41_paper_31,424,4227,1606.02891,1611.01576,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses QRNN for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses QRNN for Machine Translation."
1307,1307,41_paper_24,41_paper_30,424,4229,1606.02891,1606.0296,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation."
1308,1308,41_paper_24,41_paper_21,424,4231,1606.02891,1711.01068,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses DCCL for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses DCCL for Machine Translation."
1309,1309,41_paper_24,41_paper_2,424,4234,1606.02891,1706.03059,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses WMT2014 English-German dataset.","sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation."
1310,1310,41_paper_24,41_paper_11,424,4239,1606.02891,1406.1078,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation."
1311,1311,41_paper_18,41_paper_32,425,426,1711.02281,1606.07947,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses multiple datasets:  WMT2016 Romanian-English,  WMT2016 English-Romanian,  IWSLT2015 English-German, and  WMT2014 German-English, while Article 2 uses IWSLT2015 Thai-English dataset.","sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation."
1312,1312,41_paper_18,41_paper_8,425,427,1711.02281,1706.03762,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset.sent: Article 1 and Article 2 use IWSLT2015 English-German dataset. sent: Article 1 uses multiple datasets:  WMT2016 Romanian-English,  WMT2016 English-Romanian, and  WMT2014 German-English, while Article 2 uses multiple datasets:  IWSLT2015 German-English,  WMT2014 English-French, and  Penn Treebank.","sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation."
1313,1313,41_paper_18,41_paper_13,425,428,1711.02281,1607.07086,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 English-German dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2016 English-Romanian,  WMT2014 German-English, and  WMT2016 Romanian-English, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses RNNsearch for Machine Translation."
1314,1314,41_paper_18,41_paper_39,425,4210,1711.02281,1705.03122,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset.sent: Article 1 and Article 2 use WMT2016 English-Romanian dataset.sent: Article 1 and Article 2 use IWSLT2015 English-German dataset. sent: Article 1 uses multiple datasets:  WMT2016 Romanian-English, and  WMT2014 German-English, while Article 2 uses multiple datasets:  IWSLT2015 German-English, and  WMT2014 English-French.","sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation."
1315,1315,41_paper_18,41_paper_48,425,4211,1711.02281,1610.10099,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses multiple datasets:  WMT2016 Romanian-English,  WMT2016 English-Romanian,  IWSLT2015 English-German, and  WMT2014 German-English, while Article 2 uses multiple datasets:  WMT2014 English-French, and  WMT2015 English-German.","sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation."
1316,1316,41_paper_18,41_paper_35,425,4212,1711.02281,1711.02132,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses multiple datasets:  WMT2016 Romanian-English,  WMT2016 English-Romanian,  IWSLT2015 English-German, and  WMT2014 German-English, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation."
1317,1317,41_paper_18,41_paper_14,425,4214,1711.02281,1508.07909,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian, while Article 2 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German.","sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses C2-50k Segmentation for Machine Translation. sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses BPE word segmentation for Machine Translation."
1318,1318,41_paper_18,41_paper_26,425,4215,1711.02281,1606.02891,Machine Translation,"sent: Article 1 and Article 2 use WMT2016 Romanian-English dataset.sent: Article 1 and Article 2 use WMT2016 English-Romanian dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German, and  WMT2014 German-English, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Russian-English,  WMT2016 English-Czech, and  WMT2016 English-German.","sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation."
1319,1319,41_paper_18,41_paper_38,425,4218,1711.02281,1409.3215,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation."
1320,1320,41_paper_18,41_paper_34,425,4220,1711.02281,1511.06732,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation."
1321,1321,41_paper_18,41_paper_0,425,4221,1711.02281,1409.0473,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian, while Article 2 uses multiple datasets:  IWSLT2015 German-English, and  WMT2014 English-French.","sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses Bi-GRU  MLE SLE  for Machine Translation. sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses RNN-search50  for Machine Translation."
1322,1322,41_paper_18,41_paper_4,425,4222,1711.02281,1603.06147,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian, while Article 2 uses WMT2015 English-German dataset.","sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation."
1323,1323,41_paper_18,41_paper_31,425,4227,1711.02281,1611.01576,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses QRNN for Machine Translation."
1324,1324,41_paper_18,41_paper_30,425,4229,1711.02281,1606.0296,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation."
1325,1325,41_paper_18,41_paper_21,425,4231,1711.02281,1711.01068,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses DCCL for Machine Translation."
1326,1326,41_paper_18,41_paper_2,425,4234,1711.02281,1706.03059,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses multiple datasets:  WMT2016 Romanian-English,  WMT2016 English-Romanian,  IWSLT2015 English-German, and  WMT2014 German-English, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation."
1327,1327,41_paper_18,41_paper_11,425,4239,1711.02281,1406.1078,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation."
1328,1328,41_paper_32,41_paper_8,426,427,1606.07947,1706.03762,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses IWSLT2015 Thai-English dataset, while Article 2 uses multiple datasets:  IWSLT2015 German-English,  WMT2014 English-French,  IWSLT2015 English-German, and  Penn Treebank.","sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation."
1329,1329,41_paper_32,41_paper_13,426,428,1606.07947,1607.07086,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English, while Article 2 uses multiple datasets:  IWSLT2015 German-English, and  IWSLT2015 English-German.","sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses RNNsearch for Machine Translation."
1330,1330,41_paper_32,41_paper_17,426,429,1606.07947,1711.02281,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses IWSLT2015 Thai-English dataset, while Article 2 uses multiple datasets:  WMT2016 Romanian-English,  WMT2016 English-Romanian,  IWSLT2015 English-German, and  WMT2014 German-English.","sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation."
1331,1331,41_paper_32,41_paper_39,426,4210,1606.07947,1705.03122,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses IWSLT2015 Thai-English dataset, while Article 2 uses multiple datasets:  IWSLT2015 German-English,  WMT2016 English-Romanian,  WMT2014 English-French, and  IWSLT2015 English-German.","sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation."
1332,1332,41_paper_32,41_paper_48,426,4211,1606.07947,1610.10099,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses IWSLT2015 Thai-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-French, and  WMT2015 English-German.","sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation."
1333,1333,41_paper_32,41_paper_35,426,4212,1606.07947,1711.02132,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses IWSLT2015 Thai-English dataset, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation."
1334,1334,41_paper_32,41_paper_14,426,4214,1606.07947,1508.07909,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English, while Article 2 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German.","sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses C2-50k Segmentation for Machine Translation. sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses BPE word segmentation for Machine Translation."
1335,1335,41_paper_32,41_paper_26,426,4215,1606.07947,1606.02891,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.","sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation."
1336,1336,41_paper_32,41_paper_38,426,4218,1606.07947,1409.3215,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation."
1337,1337,41_paper_32,41_paper_34,426,4220,1606.07947,1511.06732,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation."
1338,1338,41_paper_32,41_paper_0,426,4221,1606.07947,1409.0473,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English, while Article 2 uses multiple datasets:  IWSLT2015 German-English, and  WMT2014 English-French.","sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses Bi-GRU  MLE SLE  for Machine Translation. sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses RNN-search50  for Machine Translation."
1339,1339,41_paper_32,41_paper_4,426,4222,1606.07947,1603.06147,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English, while Article 2 uses WMT2015 English-German dataset.","sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation."
1340,1340,41_paper_32,41_paper_31,426,4227,1606.07947,1611.01576,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses QRNN for Machine Translation."
1341,1341,41_paper_32,41_paper_30,426,4229,1606.07947,1606.0296,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation."
1342,1342,41_paper_32,41_paper_21,426,4231,1606.07947,1711.01068,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses DCCL for Machine Translation."
1343,1343,41_paper_32,41_paper_2,426,4234,1606.07947,1706.03059,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses IWSLT2015 Thai-English dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation."
1344,1344,41_paper_32,41_paper_11,426,4239,1606.07947,1406.1078,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation."
1345,1345,41_paper_8,41_paper_13,427,428,1706.03762,1607.07086,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset.sent: Article 1 and Article 2 use IWSLT2015 English-German dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  Penn Treebank, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses RNNsearch for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses RNNsearch for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses RNNsearch for Machine Translation."
1346,1346,41_paper_8,41_paper_17,427,429,1706.03762,1711.02281,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset.sent: Article 1 and Article 2 use IWSLT2015 English-German dataset. sent: Article 1 uses multiple datasets:  IWSLT2015 German-English,  WMT2014 English-French, and  Penn Treebank, while Article 2 uses multiple datasets:  WMT2016 Romanian-English,  WMT2016 English-Romanian, and  WMT2014 German-English.","sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation."
1347,1347,41_paper_8,41_paper_39,427,4210,1706.03762,1705.03122,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset.sent: Article 1 and Article 2 use WMT2014 English-French dataset.sent: Article 1 and Article 2 use IWSLT2015 English-German dataset.sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 uses Penn Treebank dataset, while Article 2 uses WMT2016 English-Romanian dataset.","sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation."
1348,1348,41_paper_8,41_paper_48,427,4211,1706.03762,1610.10099,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset.sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses multiple datasets:  IWSLT2015 German-English,  IWSLT2015 English-German, and  Penn Treebank, while Article 2 uses WMT2015 English-German dataset.","sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation."
1349,1349,41_paper_8,41_paper_35,427,4212,1706.03762,1711.02132,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset.sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses multiple datasets:  IWSLT2015 German-English,  IWSLT2015 English-German, and  Penn Treebank, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation."
1350,1350,41_paper_8,41_paper_14,427,4214,1706.03762,1508.07909,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 German-English,  IWSLT2015 English-German,  Penn Treebank, and  WMT2014 English-French, while Article 2 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German.","sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses C2-50k Segmentation for Machine Translation. sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses BPE word segmentation for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses C2-50k Segmentation for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses BPE word segmentation for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses C2-50k Segmentation for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses BPE word segmentation for Machine Translation."
1351,1351,41_paper_8,41_paper_26,427,4215,1706.03762,1606.02891,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 German-English,  IWSLT2015 English-German,  Penn Treebank, and  WMT2014 English-French, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.","sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation."
1352,1352,41_paper_8,41_paper_38,427,4218,1706.03762,1409.3215,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  Penn Treebank, and  IWSLT2015 German-English, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation."
1353,1353,41_paper_8,41_paper_34,427,4220,1706.03762,1511.06732,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French,  IWSLT2015 English-German, and  Penn Treebank, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation."
1354,1354,41_paper_8,41_paper_0,427,4221,1706.03762,1409.0473,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset.sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German, and  Penn Treebank, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses Bi-GRU  MLE SLE  for Machine Translation. sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses RNN-search50  for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses Bi-GRU  MLE SLE  for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses RNN-search50  for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses Bi-GRU  MLE SLE  for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses RNN-search50  for Machine Translation."
1355,1355,41_paper_8,41_paper_4,427,4222,1706.03762,1603.06147,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 German-English,  IWSLT2015 English-German,  Penn Treebank, and  WMT2014 English-French, while Article 2 uses WMT2015 English-German dataset.","sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation."
1356,1356,41_paper_8,41_paper_33,427,4224,1706.03762,1606.07947,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses multiple datasets:  IWSLT2015 German-English,  WMT2014 English-French,  IWSLT2015 English-German, and  Penn Treebank, while Article 2 uses IWSLT2015 Thai-English dataset.","sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation."
1357,1357,41_paper_8,41_paper_31,427,4227,1706.03762,1611.01576,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French,  IWSLT2015 English-German, and  Penn Treebank, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses QRNN for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses QRNN for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses QRNN for Machine Translation."
1358,1358,41_paper_8,41_paper_30,427,4229,1706.03762,1606.0296,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French,  IWSLT2015 English-German, and  Penn Treebank, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation."
1359,1359,41_paper_8,41_paper_21,427,4231,1706.03762,1711.01068,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French,  IWSLT2015 English-German, and  Penn Treebank, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses DCCL for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses DCCL for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses DCCL for Machine Translation."
1360,1360,41_paper_8,41_paper_2,427,4234,1706.03762,1706.03059,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses multiple datasets:  IWSLT2015 German-English,  WMT2014 English-French,  IWSLT2015 English-German, and  Penn Treebank, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation."
1361,1361,41_paper_8,41_paper_11,427,4239,1706.03762,1406.1078,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  Penn Treebank, and  IWSLT2015 German-English, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Transformer approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation. sent: Article 1 uses Transformer Base approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation. sent: Article 1 uses Transformer Big approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation."
1362,1362,41_paper_39,41_paper_48,4210,4211,1705.03122,1610.10099,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset.sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses multiple datasets:  IWSLT2015 German-English,  WMT2016 English-Romanian, and  IWSLT2015 English-German, while Article 2 uses WMT2015 English-German dataset.","sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation."
1363,1363,41_paper_39,41_paper_35,4210,4212,1705.03122,1711.02132,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset.sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses multiple datasets:  IWSLT2015 German-English,  WMT2016 English-Romanian, and  IWSLT2015 English-German, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation."
1364,1364,41_paper_39,41_paper_14,4210,4214,1705.03122,1508.07909,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 German-English,  IWSLT2015 English-German,  WMT2014 English-French, and  WMT2016 English-Romanian, while Article 2 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German.","sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses C2-50k Segmentation for Machine Translation. sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses BPE word segmentation for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses C2-50k Segmentation for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses BPE word segmentation for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses C2-50k Segmentation for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses BPE word segmentation for Machine Translation."
1365,1365,41_paper_39,41_paper_26,4210,4215,1705.03122,1606.02891,Machine Translation,"sent: Article 1 and Article 2 use WMT2016 English-Romanian dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French,  IWSLT2015 English-German, and  IWSLT2015 German-English, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech, and  WMT2016 English-German.","sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation."
1366,1366,41_paper_39,41_paper_5,4210,4216,1705.03122,1706.03762,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset.sent: Article 1 and Article 2 use WMT2014 English-French dataset.sent: Article 1 and Article 2 use IWSLT2015 English-German dataset.sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 uses WMT2016 English-Romanian dataset, while Article 2 uses Penn Treebank dataset.","sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation."
1367,1367,41_paper_39,41_paper_38,4210,4218,1705.03122,1409.3215,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2016 English-Romanian,  IWSLT2015 English-German, and  IWSLT2015 German-English, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation."
1368,1368,41_paper_39,41_paper_34,4210,4220,1705.03122,1511.06732,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2016 English-Romanian,  WMT2014 English-French, and  IWSLT2015 English-German, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation."
1369,1369,41_paper_39,41_paper_0,4210,4221,1705.03122,1409.0473,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset.sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2016 English-Romanian, and  IWSLT2015 English-German, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses Bi-GRU  MLE SLE  for Machine Translation. sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses RNN-search50  for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses Bi-GRU  MLE SLE  for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses RNN-search50  for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses Bi-GRU  MLE SLE  for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses RNN-search50  for Machine Translation."
1370,1370,41_paper_39,41_paper_4,4210,4222,1705.03122,1603.06147,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 German-English,  IWSLT2015 English-German,  WMT2014 English-French, and  WMT2016 English-Romanian, while Article 2 uses WMT2015 English-German dataset.","sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation."
1371,1371,41_paper_39,41_paper_33,4210,4224,1705.03122,1606.07947,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses multiple datasets:  IWSLT2015 German-English,  WMT2016 English-Romanian,  WMT2014 English-French, and  IWSLT2015 English-German, while Article 2 uses IWSLT2015 Thai-English dataset.","sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation."
1372,1372,41_paper_39,41_paper_31,4210,4227,1705.03122,1611.01576,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2016 English-Romanian,  WMT2014 English-French, and  IWSLT2015 English-German, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses QRNN for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses QRNN for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses QRNN for Machine Translation."
1373,1373,41_paper_39,41_paper_30,4210,4229,1705.03122,1606.0296,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2016 English-Romanian,  WMT2014 English-French, and  IWSLT2015 English-German, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation."
1374,1374,41_paper_39,41_paper_21,4210,4231,1705.03122,1711.01068,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2016 English-Romanian,  WMT2014 English-French, and  IWSLT2015 English-German, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses DCCL for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses DCCL for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses DCCL for Machine Translation."
1375,1375,41_paper_39,41_paper_2,4210,4234,1705.03122,1706.03059,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses multiple datasets:  IWSLT2015 German-English,  WMT2016 English-Romanian,  WMT2014 English-French, and  IWSLT2015 English-German, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation."
1376,1376,41_paper_39,41_paper_16,4210,4235,1705.03122,1711.02281,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset.sent: Article 1 and Article 2 use WMT2016 English-Romanian dataset.sent: Article 1 and Article 2 use IWSLT2015 English-German dataset. sent: Article 1 uses multiple datasets:  IWSLT2015 German-English, and  WMT2014 English-French, while Article 2 uses multiple datasets:  WMT2016 Romanian-English, and  WMT2014 German-English.","sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation."
1377,1377,41_paper_39,41_paper_11,4210,4239,1705.03122,1406.1078,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2016 English-Romanian,  IWSLT2015 English-German, and  IWSLT2015 German-English, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ConvS2S  ensemble  approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation. sent: Article 1 uses ConvS2S approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation. sent: Article 1 uses ConvS2S BPE40k approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation."
1378,1378,41_paper_48,41_paper_35,4211,4212,1610.10099,1711.02132,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset.sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses WMT2015 English-German dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation."
1379,1379,41_paper_48,41_paper_40,4211,4213,1610.10099,1705.03122,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset.sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses multiple datasets:  IWSLT2015 German-English,  WMT2016 English-Romanian, and  IWSLT2015 English-German.","sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation."
1380,1380,41_paper_48,41_paper_14,4211,4214,1610.10099,1508.07909,Machine Translation,"sent: Article 1 and Article 2 use WMT2015 English-German dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French, while Article 2 uses WMT2015 English-Russian dataset.","sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses C2-50k Segmentation for Machine Translation. sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses BPE word segmentation for Machine Translation."
1381,1381,41_paper_48,41_paper_26,4211,4215,1610.10099,1606.02891,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  WMT2015 English-German, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.","sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation."
1382,1382,41_paper_48,41_paper_5,4211,4216,1610.10099,1706.03762,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset.sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses multiple datasets:  IWSLT2015 German-English,  IWSLT2015 English-German, and  Penn Treebank.","sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation."
1383,1383,41_paper_48,41_paper_38,4211,4218,1610.10099,1409.3215,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  WMT2015 English-German, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation."
1384,1384,41_paper_48,41_paper_34,4211,4220,1610.10099,1511.06732,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  WMT2015 English-German, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation."
1385,1385,41_paper_48,41_paper_0,4211,4221,1610.10099,1409.0473,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  WMT2015 English-German, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses Bi-GRU  MLE SLE  for Machine Translation. sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses RNN-search50  for Machine Translation."
1386,1386,41_paper_48,41_paper_4,4211,4222,1610.10099,1603.06147,Machine Translation,"sent: Article 1 and Article 2 use WMT2015 English-German dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation."
1387,1387,41_paper_48,41_paper_33,4211,4224,1610.10099,1606.07947,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-French, and  WMT2015 English-German, while Article 2 uses IWSLT2015 Thai-English dataset.","sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation."
1388,1388,41_paper_48,41_paper_31,4211,4227,1610.10099,1611.01576,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  WMT2015 English-German, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses QRNN for Machine Translation."
1389,1389,41_paper_48,41_paper_30,4211,4229,1610.10099,1606.0296,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  WMT2015 English-German, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation."
1390,1390,41_paper_48,41_paper_21,4211,4231,1610.10099,1711.01068,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  WMT2015 English-German, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses DCCL for Machine Translation."
1391,1391,41_paper_48,41_paper_2,4211,4234,1610.10099,1706.03059,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-French, and  WMT2015 English-German, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation."
1392,1392,41_paper_48,41_paper_16,4211,4235,1610.10099,1711.02281,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-French, and  WMT2015 English-German, while Article 2 uses multiple datasets:  WMT2016 Romanian-English,  WMT2016 English-Romanian,  IWSLT2015 English-German, and  WMT2014 German-English.","sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation."
1393,1393,41_paper_48,41_paper_11,4211,4239,1610.10099,1406.1078,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  WMT2015 English-German, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation."
1394,1394,41_paper_35,41_paper_40,4212,4213,1711.02132,1705.03122,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset.sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  IWSLT2015 German-English,  WMT2016 English-Romanian, and  IWSLT2015 English-German.","sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation."
1395,1395,41_paper_35,41_paper_14,4212,4214,1711.02132,1508.07909,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French, while Article 2 uses multiple datasets:  WMT2015 English-Russian, and  WMT2015 English-German.","sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses C2-50k Segmentation for Machine Translation. sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses BPE word segmentation for Machine Translation."
1396,1396,41_paper_35,41_paper_26,4212,4215,1711.02132,1606.02891,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.","sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation."
1397,1397,41_paper_35,41_paper_5,4212,4216,1711.02132,1706.03762,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset.sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  IWSLT2015 German-English,  IWSLT2015 English-German, and  Penn Treebank.","sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation."
1398,1398,41_paper_35,41_paper_47,4212,4217,1711.02132,1610.10099,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset.sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses WMT2015 English-German dataset.","sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation."
1399,1399,41_paper_35,41_paper_38,4212,4218,1711.02132,1409.3215,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses WMT2014 English-German dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation."
1400,1400,41_paper_35,41_paper_34,4212,4220,1711.02132,1511.06732,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation."
1401,1401,41_paper_35,41_paper_0,4212,4221,1711.02132,1409.0473,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses WMT2014 English-German dataset, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses Bi-GRU  MLE SLE  for Machine Translation. sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses RNN-search50  for Machine Translation."
1402,1402,41_paper_35,41_paper_4,4212,4222,1711.02132,1603.06147,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French, while Article 2 uses WMT2015 English-German dataset.","sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation."
1403,1403,41_paper_35,41_paper_33,4212,4224,1711.02132,1606.07947,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses IWSLT2015 Thai-English dataset.","sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation."
1404,1404,41_paper_35,41_paper_31,4212,4227,1711.02132,1611.01576,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses QRNN for Machine Translation."
1405,1405,41_paper_35,41_paper_30,4212,4229,1711.02132,1606.0296,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation."
1406,1406,41_paper_35,41_paper_21,4212,4231,1711.02132,1711.01068,Machine Translation,"sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French, while Article 2 uses IWSLT2015 German-English dataset.","sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses DCCL for Machine Translation."
1407,1407,41_paper_35,41_paper_2,4212,4234,1711.02132,1706.03059,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses WMT2014 English-French dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation."
1408,1408,41_paper_35,41_paper_16,4212,4235,1711.02132,1711.02281,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses multiple datasets:  WMT2016 Romanian-English,  WMT2016 English-Romanian,  IWSLT2015 English-German, and  WMT2014 German-English.","sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation."
1409,1409,41_paper_35,41_paper_11,4212,4239,1711.02132,1406.1078,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses WMT2014 English-German dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation."
1410,1410,41_paper_34,41_paper_0,4220,4221,1511.06732,1409.0473,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses Bi-GRU  MLE SLE  for Machine Translation. sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses RNN-search50  for Machine Translation."
1411,1411,41_paper_34,41_paper_4,4220,4222,1511.06732,1603.06147,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses WMT2015 English-German dataset.","sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation."
1412,1412,41_paper_34,41_paper_33,4220,4224,1511.06732,1606.07947,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English.","sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation."
1413,1413,41_paper_34,41_paper_22,4220,4225,1511.06732,1606.02891,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.","sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation."
1414,1414,41_paper_34,41_paper_42,4220,4226,1511.06732,1705.03122,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2016 English-Romanian,  WMT2014 English-French, and  IWSLT2015 English-German.","sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation."
1415,1415,41_paper_34,41_paper_31,4220,4227,1511.06732,1611.01576,Machine Translation,sent: Article 1 and Article 2 use IWSLT2015 German-English dataset.,"sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses QRNN for Machine Translation."
1416,1416,41_paper_34,41_paper_30,4220,4229,1511.06732,1606.0296,Machine Translation,sent: Article 1 and Article 2 use IWSLT2015 German-English dataset.,"sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation."
1417,1417,41_paper_34,41_paper_36,4220,4230,1511.06732,1711.02132,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French.","sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation."
1418,1418,41_paper_34,41_paper_21,4220,4231,1511.06732,1711.01068,Machine Translation,sent: Article 1 and Article 2 use IWSLT2015 German-English dataset.,"sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses DCCL for Machine Translation."
1419,1419,41_paper_34,41_paper_46,4220,4232,1511.06732,1610.10099,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  WMT2015 English-German.","sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation."
1420,1420,41_paper_34,41_paper_2,4220,4234,1511.06732,1706.03059,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses WMT2014 English-German dataset.","sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation."
1421,1421,41_paper_34,41_paper_16,4220,4235,1511.06732,1711.02281,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian.","sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation."
1422,1422,41_paper_34,41_paper_9,4220,4236,1511.06732,1706.03762,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French,  IWSLT2015 English-German, and  Penn Treebank.","sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation."
1423,1423,41_paper_34,41_paper_11,4220,4239,1511.06732,1406.1078,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation."
1424,1424,41_paper_0,41_paper_4,4221,4222,1409.0473,1603.06147,Machine Translation,"sent: Article 1 uses multiple datasets:  IWSLT2015 German-English, and  WMT2014 English-French, while Article 2 uses WMT2015 English-German dataset.","sent: Article 1 uses Bi-GRU  MLE SLE  approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses Bi-GRU  MLE SLE  approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation. sent: Article 1 uses RNN-search50  approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses RNN-search50  approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation."
1425,1425,41_paper_0,41_paper_33,4221,4224,1409.0473,1606.07947,Machine Translation,"sent: Article 1 uses multiple datasets:  IWSLT2015 German-English, and  WMT2014 English-French, while Article 2 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English.","sent: Article 1 uses Bi-GRU  MLE SLE  approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation. sent: Article 1 uses RNN-search50  approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation."
1426,1426,41_paper_0,41_paper_22,4221,4225,1409.0473,1606.02891,Machine Translation,"sent: Article 1 uses multiple datasets:  IWSLT2015 German-English, and  WMT2014 English-French, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.","sent: Article 1 uses Bi-GRU  MLE SLE  approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses Bi-GRU  MLE SLE  approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation. sent: Article 1 uses RNN-search50  approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses RNN-search50  approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation."
1427,1427,41_paper_0,41_paper_42,4221,4226,1409.0473,1705.03122,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset.sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2016 English-Romanian, and  IWSLT2015 English-German.","sent: Article 1 uses Bi-GRU  MLE SLE  approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses Bi-GRU  MLE SLE  approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses Bi-GRU  MLE SLE  approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation. sent: Article 1 uses RNN-search50  approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses RNN-search50  approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses RNN-search50  approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation."
1428,1428,41_paper_0,41_paper_31,4221,4227,1409.0473,1611.01576,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 uses WMT2014 English-French dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Bi-GRU  MLE SLE  approach for Machine Translation, while Article 2 uses QRNN for Machine Translation. sent: Article 1 uses RNN-search50  approach for Machine Translation, while Article 2 uses QRNN for Machine Translation."
1429,1429,41_paper_0,41_paper_30,4221,4229,1409.0473,1606.0296,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 uses WMT2014 English-French dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Bi-GRU  MLE SLE  approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation. sent: Article 1 uses RNN-search50  approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation."
1430,1430,41_paper_0,41_paper_36,4221,4230,1409.0473,1711.02132,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses WMT2014 English-German dataset.","sent: Article 1 uses Bi-GRU  MLE SLE  approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation. sent: Article 1 uses RNN-search50  approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation."
1431,1431,41_paper_0,41_paper_21,4221,4231,1409.0473,1711.01068,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 uses WMT2014 English-French dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Bi-GRU  MLE SLE  approach for Machine Translation, while Article 2 uses DCCL for Machine Translation. sent: Article 1 uses RNN-search50  approach for Machine Translation, while Article 2 uses DCCL for Machine Translation."
1432,1432,41_paper_0,41_paper_46,4221,4232,1409.0473,1610.10099,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German, and  WMT2015 English-German.","sent: Article 1 uses Bi-GRU  MLE SLE  approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation. sent: Article 1 uses RNN-search50  approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation."
1433,1433,41_paper_0,41_paper_2,4221,4234,1409.0473,1706.03059,Machine Translation,"sent: Article 1 uses multiple datasets:  IWSLT2015 German-English, and  WMT2014 English-French, while Article 2 uses WMT2014 English-German dataset.","sent: Article 1 uses Bi-GRU  MLE SLE  approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation. sent: Article 1 uses RNN-search50  approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation."
1434,1434,41_paper_0,41_paper_16,4221,4235,1409.0473,1711.02281,Machine Translation,"sent: Article 1 uses multiple datasets:  IWSLT2015 German-English, and  WMT2014 English-French, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian.","sent: Article 1 uses Bi-GRU  MLE SLE  approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation. sent: Article 1 uses RNN-search50  approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation."
1435,1435,41_paper_0,41_paper_9,4221,4236,1409.0473,1706.03762,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset.sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German, and  Penn Treebank.","sent: Article 1 uses Bi-GRU  MLE SLE  approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses Bi-GRU  MLE SLE  approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses Bi-GRU  MLE SLE  approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation. sent: Article 1 uses RNN-search50  approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses RNN-search50  approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses RNN-search50  approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation."
1436,1436,41_paper_0,41_paper_11,4221,4239,1409.0473,1406.1078,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Bi-GRU  MLE SLE  approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation. sent: Article 1 uses RNN-search50  approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation."
1437,1437,41_paper_31,41_paper_29,4227,4228,1611.01576,1606.02891,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.","sent: Article 1 uses QRNN approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses QRNN approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation."
1438,1438,41_paper_31,41_paper_30,4227,4229,1611.01576,1606.0296,Machine Translation,sent: Article 1 and Article 2 use IWSLT2015 German-English dataset.,"sent: Article 1 uses QRNN approach for Machine Translation, while Article 2 uses Word-level CNN w attn  input feeding for Machine Translation."
1439,1439,41_paper_31,41_paper_36,4227,4230,1611.01576,1711.02132,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French.","sent: Article 1 uses QRNN approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation."
1440,1440,41_paper_31,41_paper_21,4227,4231,1611.01576,1711.01068,Machine Translation,sent: Article 1 and Article 2 use IWSLT2015 German-English dataset.,"sent: Article 1 uses QRNN approach for Machine Translation, while Article 2 uses DCCL for Machine Translation."
1441,1441,41_paper_31,41_paper_46,4227,4232,1611.01576,1610.10099,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  WMT2015 English-German.","sent: Article 1 uses QRNN approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation."
1442,1442,41_paper_31,41_paper_2,4227,4234,1611.01576,1706.03059,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses WMT2014 English-German dataset.","sent: Article 1 uses QRNN approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation."
1443,1443,41_paper_31,41_paper_16,4227,4235,1611.01576,1711.02281,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian.","sent: Article 1 uses QRNN approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation."
1444,1444,41_paper_31,41_paper_9,4227,4236,1611.01576,1706.03762,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French,  IWSLT2015 English-German, and  Penn Treebank.","sent: Article 1 uses QRNN approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses QRNN approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses QRNN approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation."
1445,1445,41_paper_31,41_paper_11,4227,4239,1611.01576,1406.1078,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses QRNN approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation."
1446,1446,41_paper_31,41_paper_41,4227,4241,1611.01576,1705.03122,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2016 English-Romanian,  WMT2014 English-French, and  IWSLT2015 English-German.","sent: Article 1 uses QRNN approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses QRNN approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses QRNN approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation."
1447,1447,41_paper_30,41_paper_36,4229,4230,1606.0296,1711.02132,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French.","sent: Article 1 uses Word-level CNN w attn  input feeding approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation."
1448,1448,41_paper_30,41_paper_21,4229,4231,1606.0296,1711.01068,Machine Translation,sent: Article 1 and Article 2 use IWSLT2015 German-English dataset.,"sent: Article 1 uses Word-level CNN w attn  input feeding approach for Machine Translation, while Article 2 uses DCCL for Machine Translation."
1449,1449,41_paper_30,41_paper_46,4229,4232,1606.0296,1610.10099,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  WMT2015 English-German.","sent: Article 1 uses Word-level CNN w attn  input feeding approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation."
1450,1450,41_paper_30,41_paper_23,4229,4233,1606.0296,1606.02891,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.","sent: Article 1 uses Word-level CNN w attn  input feeding approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses Word-level CNN w attn  input feeding approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation."
1451,1451,41_paper_30,41_paper_2,4229,4234,1606.0296,1706.03059,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses WMT2014 English-German dataset.","sent: Article 1 uses Word-level CNN w attn  input feeding approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation."
1452,1452,41_paper_30,41_paper_16,4229,4235,1606.0296,1711.02281,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian.","sent: Article 1 uses Word-level CNN w attn  input feeding approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation."
1453,1453,41_paper_30,41_paper_9,4229,4236,1606.0296,1706.03762,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French,  IWSLT2015 English-German, and  Penn Treebank.","sent: Article 1 uses Word-level CNN w attn  input feeding approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses Word-level CNN w attn  input feeding approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses Word-level CNN w attn  input feeding approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation."
1454,1454,41_paper_30,41_paper_11,4229,4239,1606.0296,1406.1078,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses Word-level CNN w attn  input feeding approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation."
1455,1455,41_paper_30,41_paper_41,4229,4241,1606.0296,1705.03122,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2016 English-Romanian,  WMT2014 English-French, and  IWSLT2015 English-German.","sent: Article 1 uses Word-level CNN w attn  input feeding approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses Word-level CNN w attn  input feeding approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses Word-level CNN w attn  input feeding approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation."
1456,1456,41_paper_21,41_paper_46,4231,4232,1711.01068,1610.10099,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  WMT2015 English-German.","sent: Article 1 uses DCCL approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation."
1457,1457,41_paper_21,41_paper_23,4231,4233,1711.01068,1606.02891,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.","sent: Article 1 uses DCCL approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses DCCL approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation."
1458,1458,41_paper_21,41_paper_2,4231,4234,1711.01068,1706.03059,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses WMT2014 English-German dataset.","sent: Article 1 uses DCCL approach for Machine Translation, while Article 2 uses SliceNet for Machine Translation."
1459,1459,41_paper_21,41_paper_16,4231,4235,1711.01068,1711.02281,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian.","sent: Article 1 uses DCCL approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation."
1460,1460,41_paper_21,41_paper_9,4231,4236,1711.01068,1706.03762,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French,  IWSLT2015 English-German, and  Penn Treebank.","sent: Article 1 uses DCCL approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses DCCL approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses DCCL approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation."
1461,1461,41_paper_21,41_paper_11,4231,4239,1711.01068,1406.1078,Machine Translation,"sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses DCCL approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation."
1462,1462,41_paper_21,41_paper_41,4231,4241,1711.01068,1705.03122,Machine Translation,"sent: Article 1 and Article 2 use IWSLT2015 German-English dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2016 English-Romanian,  WMT2014 English-French, and  IWSLT2015 English-German.","sent: Article 1 uses DCCL approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses DCCL approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses DCCL approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation."
1463,1463,41_paper_2,41_paper_16,4234,4235,1706.03059,1711.02281,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2016 Romanian-English,  WMT2016 English-Romanian,  IWSLT2015 English-German, and  WMT2014 German-English.","sent: Article 1 uses SliceNet approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation."
1464,1464,41_paper_2,41_paper_9,4234,4236,1706.03059,1706.03762,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  IWSLT2015 German-English,  WMT2014 English-French,  IWSLT2015 English-German, and  Penn Treebank.","sent: Article 1 uses SliceNet approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses SliceNet approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses SliceNet approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation."
1465,1465,41_paper_2,41_paper_25,4234,4237,1706.03059,1606.02891,Machine Translation,"sent: Article 1 uses WMT2014 English-German dataset, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.","sent: Article 1 uses SliceNet approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses SliceNet approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation."
1466,1466,41_paper_2,41_paper_11,4234,4239,1706.03059,1406.1078,Machine Translation,"sent: Article 1 uses WMT2014 English-German dataset, while Article 2 uses WMT2014 English-French dataset.","sent: Article 1 uses SliceNet approach for Machine Translation, while Article 2 uses CSLM   RNN   WP for Machine Translation."
1467,1467,41_paper_2,41_paper_41,4234,4241,1706.03059,1705.03122,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  IWSLT2015 German-English,  WMT2016 English-Romanian,  WMT2014 English-French, and  IWSLT2015 English-German.","sent: Article 1 uses SliceNet approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses SliceNet approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses SliceNet approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation."
1468,1468,41_paper_11,41_paper_6,4239,4240,1406.1078,1706.03762,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  Penn Treebank, and  IWSLT2015 German-English.","sent: Article 1 uses CSLM   RNN   WP approach for Machine Translation, while Article 2 uses Transformer for Machine Translation. sent: Article 1 uses CSLM   RNN   WP approach for Machine Translation, while Article 2 uses Transformer Base for Machine Translation. sent: Article 1 uses CSLM   RNN   WP approach for Machine Translation, while Article 2 uses Transformer Big for Machine Translation."
1469,1469,41_paper_11,41_paper_41,4239,4241,1406.1078,1705.03122,Machine Translation,"sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2016 English-Romanian,  IWSLT2015 English-German, and  IWSLT2015 German-English.","sent: Article 1 uses CSLM   RNN   WP approach for Machine Translation, while Article 2 uses ConvS2S  ensemble  for Machine Translation. sent: Article 1 uses CSLM   RNN   WP approach for Machine Translation, while Article 2 uses ConvS2S for Machine Translation. sent: Article 1 uses CSLM   RNN   WP approach for Machine Translation, while Article 2 uses ConvS2S BPE40k for Machine Translation."
1470,1470,41_paper_11,41_paper_19,4239,4242,1406.1078,1711.02281,Machine Translation,"sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian.","sent: Article 1 uses CSLM   RNN   WP approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation."
1471,1471,41_paper_11,41_paper_28,4239,4244,1406.1078,1606.02891,Machine Translation,"sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.","sent: Article 1 uses CSLM   RNN   WP approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses CSLM   RNN   WP approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation."
1472,1472,42_paper_6,42_paper_26,430,434,1611.07004,1711.0902,Image-to-Image Translation,"sent: Article 1 uses multiple datasets:  Cityscapes Photo-to-Labels,  Aerial-to-Map, and  Cityscapes Labels-to-Photo, while Article 2 uses RaFD dataset.","sent: Article 1 uses pix2pix approach for Image-to-Image Translation, while Article 2 uses StarGAN for Image-to-Image Translation. sent: Article 1 uses cGAN approach for Image-to-Image Translation, while Article 2 uses StarGAN for Image-to-Image Translation."
1473,1473,42_paper_6,42_paper_28,430,435,1611.07004,1606.07536,Image-to-Image Translation,"sent: Article 1 and Article 2 use Cityscapes Photo-to-Labels dataset.sent: Article 1 and Article 2 use Cityscapes Labels-to-Photo dataset. sent: Article 1 uses Aerial-to-Map dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses pix2pix approach for Image-to-Image Translation, while Article 2 uses CoGAN for Image-to-Image Translation. sent: Article 1 uses cGAN approach for Image-to-Image Translation, while Article 2 uses CoGAN for Image-to-Image Translation."
1474,1474,42_paper_6,42_paper_18,430,437,1611.07004,1707.09405,Image-to-Image Translation,"sent: Article 1 and Article 2 use Cityscapes Labels-to-Photo dataset. sent: Article 1 uses multiple datasets:  Cityscapes Photo-to-Labels, and  Aerial-to-Map, while Article 2 uses multiple datasets:  ADE20K Labels-to-Photos,  ADE20K-Outdoor Labels-to-Photos, and  COCO-Stuff Labels-to-Photos.","sent: Article 1 uses pix2pix approach for Image-to-Image Translation, while Article 2 uses CRN for Image-to-Image Translation. sent: Article 1 uses cGAN approach for Image-to-Image Translation, while Article 2 uses CRN for Image-to-Image Translation."
1475,1475,42_paper_26,42_paper_28,434,435,1711.0902,1606.07536,Image-to-Image Translation,"sent: Article 1 uses RaFD dataset, while Article 2 uses multiple datasets:  Cityscapes Photo-to-Labels, and  Cityscapes Labels-to-Photo.","sent: Article 1 uses StarGAN approach for Image-to-Image Translation, while Article 2 uses CoGAN for Image-to-Image Translation."
1476,1476,42_paper_26,42_paper_7,434,436,1711.0902,1611.07004,Image-to-Image Translation,"sent: Article 1 uses RaFD dataset, while Article 2 uses multiple datasets:  Cityscapes Photo-to-Labels,  Aerial-to-Map, and  Cityscapes Labels-to-Photo.","sent: Article 1 uses StarGAN approach for Image-to-Image Translation, while Article 2 uses pix2pix for Image-to-Image Translation. sent: Article 1 uses StarGAN approach for Image-to-Image Translation, while Article 2 uses cGAN for Image-to-Image Translation."
1477,1477,42_paper_26,42_paper_18,434,437,1711.0902,1707.09405,Image-to-Image Translation,"sent: Article 1 uses RaFD dataset, while Article 2 uses multiple datasets:  ADE20K Labels-to-Photos,  ADE20K-Outdoor Labels-to-Photos,  COCO-Stuff Labels-to-Photos, and  Cityscapes Labels-to-Photo.","sent: Article 1 uses StarGAN approach for Image-to-Image Translation, while Article 2 uses CRN for Image-to-Image Translation."
1478,1478,42_paper_28,42_paper_7,435,436,1606.07536,1611.07004,Image-to-Image Translation,"sent: Article 1 and Article 2 use Cityscapes Photo-to-Labels dataset.sent: Article 1 and Article 2 use Cityscapes Labels-to-Photo dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Aerial-to-Map dataset.","sent: Article 1 uses CoGAN approach for Image-to-Image Translation, while Article 2 uses pix2pix for Image-to-Image Translation. sent: Article 1 uses CoGAN approach for Image-to-Image Translation, while Article 2 uses cGAN for Image-to-Image Translation."
1479,1479,42_paper_28,42_paper_18,435,437,1606.07536,1707.09405,Image-to-Image Translation,"sent: Article 1 and Article 2 use Cityscapes Labels-to-Photo dataset. sent: Article 1 uses Cityscapes Photo-to-Labels dataset, while Article 2 uses multiple datasets:  ADE20K Labels-to-Photos,  ADE20K-Outdoor Labels-to-Photos, and  COCO-Stuff Labels-to-Photos.","sent: Article 1 uses CoGAN approach for Image-to-Image Translation, while Article 2 uses CRN for Image-to-Image Translation."
1480,1480,42_paper_18,42_paper_29,437,439,1707.09405,1606.07536,Image-to-Image Translation,"sent: Article 1 and Article 2 use Cityscapes Labels-to-Photo dataset. sent: Article 1 uses multiple datasets:  ADE20K Labels-to-Photos,  ADE20K-Outdoor Labels-to-Photos, and  COCO-Stuff Labels-to-Photos, while Article 2 uses Cityscapes Photo-to-Labels dataset.","sent: Article 1 uses CRN approach for Image-to-Image Translation, while Article 2 uses CoGAN for Image-to-Image Translation."
1481,1481,42_paper_18,42_paper_4,437,4312,1707.09405,1611.07004,Image-to-Image Translation,"sent: Article 1 and Article 2 use Cityscapes Labels-to-Photo dataset. sent: Article 1 uses multiple datasets:  ADE20K Labels-to-Photos,  ADE20K-Outdoor Labels-to-Photos, and  COCO-Stuff Labels-to-Photos, while Article 2 uses multiple datasets:  Cityscapes Photo-to-Labels, and  Aerial-to-Map.","sent: Article 1 uses CRN approach for Image-to-Image Translation, while Article 2 uses pix2pix for Image-to-Image Translation. sent: Article 1 uses CRN approach for Image-to-Image Translation, while Article 2 uses cGAN for Image-to-Image Translation."
1482,1482,43_paper_3,43_paper_4,440,441,1905.0127,1711.11556,Synthetic-to-Real Translation,sent: Article 1 and Article 2 use GTAV-to-Cityscapes Labels dataset.,"sent: Article 1 uses Domain adaptation   ResNet-101 approach for Synthetic-to-Real Translation, while Article 2 uses ROAD for Synthetic-to-Real Translation."
1483,1483,44_paper_2,44_paper_1,450,451,1409.7495,1611.022,Unsupervised Image-To-Image Translation,sent: Article 1 and Article 2 use SVNH-to-MNIST dataset.,"sent: Article 1 uses DANN approach for Unsupervised Image-To-Image Translation, while Article 2 uses DTN for Unsupervised Image-To-Image Translation."
1484,1484,47_paper_3,47_paper_6,480,482,1805.02474,1808.09075,Named Entity Recognition  NER ,"sent: Article 1 and Article 2 use CoNLL 2003  English  dataset. sent: Article 1 uses multiple datasets:  MR,  IMDb, and  Penn Treebank, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses S-LSTM approach for Named Entity Recognition  NER , while Article 2 uses Neural-CRF AE for Named Entity Recognition  NER . sent: Article 1 uses S-LSTM approach for Named Entity Recognition  NER , while Article 2 uses CRF   AutoEncoder for Named Entity Recognition  NER ."
1485,1485,47_paper_3,47_paper_0,480,484,1805.02474,1603.0136,Named Entity Recognition  NER ,"sent: Article 1 and Article 2 use CoNLL 2003  English  dataset. sent: Article 1 uses multiple datasets:  MR,  IMDb, and  Penn Treebank, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses S-LSTM approach for Named Entity Recognition  NER , while Article 2 uses LSTM-CRF for Named Entity Recognition  NER ."
1486,1486,47_paper_3,47_paper_2,480,485,1805.02474,1809.0795,Named Entity Recognition  NER ,"sent: Article 1 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses multiple datasets:  BC5CDR, and  JNLPBA.","sent: Article 1 uses S-LSTM approach for Named Entity Recognition  NER , while Article 2 uses CollaboNet for Named Entity Recognition  NER ."
1487,1487,47_paper_4,47_paper_6,481,482,1802.05365,1808.09075,Named Entity Recognition  NER ,"sent: Article 1 and Article 2 use CoNLL 2003  English  dataset. sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Named Entity Recognition  NER , while Article 2 uses Neural-CRF AE for Named Entity Recognition  NER . sent: Article 1 uses BiLSTM-CRF ELMo approach for Named Entity Recognition  NER , while Article 2 uses CRF   AutoEncoder for Named Entity Recognition  NER . sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Named Entity Recognition  NER , while Article 2 uses Neural-CRF AE for Named Entity Recognition  NER . sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Named Entity Recognition  NER , while Article 2 uses CRF   AutoEncoder for Named Entity Recognition  NER . sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Named Entity Recognition  NER , while Article 2 uses Neural-CRF AE for Named Entity Recognition  NER . sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Named Entity Recognition  NER , while Article 2 uses CRF   AutoEncoder for Named Entity Recognition  NER . sent: Article 1 uses  Lee et al   2017  ELMo approach for Named Entity Recognition  NER , while Article 2 uses Neural-CRF AE for Named Entity Recognition  NER . sent: Article 1 uses  Lee et al   2017  ELMo approach for Named Entity Recognition  NER , while Article 2 uses CRF   AutoEncoder for Named Entity Recognition  NER . sent: Article 1 uses BCN ELMo approach for Named Entity Recognition  NER , while Article 2 uses Neural-CRF AE for Named Entity Recognition  NER . sent: Article 1 uses BCN ELMo approach for Named Entity Recognition  NER , while Article 2 uses CRF   AutoEncoder for Named Entity Recognition  NER . sent: Article 1 uses ESIM   ELMo Ensemble approach for Named Entity Recognition  NER , while Article 2 uses Neural-CRF AE for Named Entity Recognition  NER . sent: Article 1 uses ESIM   ELMo Ensemble approach for Named Entity Recognition  NER , while Article 2 uses CRF   AutoEncoder for Named Entity Recognition  NER . sent: Article 1 uses  He et al   2017    ELMo approach for Named Entity Recognition  NER , while Article 2 uses Neural-CRF AE for Named Entity Recognition  NER . sent: Article 1 uses  He et al   2017    ELMo approach for Named Entity Recognition  NER , while Article 2 uses CRF   AutoEncoder for Named Entity Recognition  NER . sent: Article 1 uses ESIM   ELMo approach for Named Entity Recognition  NER , while Article 2 uses Neural-CRF AE for Named Entity Recognition  NER . sent: Article 1 uses ESIM   ELMo approach for Named Entity Recognition  NER , while Article 2 uses CRF   AutoEncoder for Named Entity Recognition  NER . sent: Article 1 uses BiLSTM-Attention   ELMo approach for Named Entity Recognition  NER , while Article 2 uses Neural-CRF AE for Named Entity Recognition  NER . sent: Article 1 uses BiLSTM-Attention   ELMo approach for Named Entity Recognition  NER , while Article 2 uses CRF   AutoEncoder for Named Entity Recognition  NER ."
1488,1488,47_paper_4,47_paper_0,481,484,1802.05365,1603.0136,Named Entity Recognition  NER ,"sent: Article 1 and Article 2 use CoNLL 2003  English  dataset. sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  SQuAD1 1,  SQuAD2 0,  SST-5 Fine-grained classification, and  ACL-ARC, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Named Entity Recognition  NER , while Article 2 uses LSTM-CRF for Named Entity Recognition  NER . sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Named Entity Recognition  NER , while Article 2 uses LSTM-CRF for Named Entity Recognition  NER . sent: Article 1 uses  Lee et al   2017  ELMo approach for Named Entity Recognition  NER , while Article 2 uses LSTM-CRF for Named Entity Recognition  NER . sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Named Entity Recognition  NER , while Article 2 uses LSTM-CRF for Named Entity Recognition  NER . sent: Article 1 uses BCN ELMo approach for Named Entity Recognition  NER , while Article 2 uses LSTM-CRF for Named Entity Recognition  NER . sent: Article 1 uses ESIM   ELMo Ensemble approach for Named Entity Recognition  NER , while Article 2 uses LSTM-CRF for Named Entity Recognition  NER . sent: Article 1 uses  He et al   2017    ELMo approach for Named Entity Recognition  NER , while Article 2 uses LSTM-CRF for Named Entity Recognition  NER . sent: Article 1 uses ESIM   ELMo approach for Named Entity Recognition  NER , while Article 2 uses LSTM-CRF for Named Entity Recognition  NER . sent: Article 1 uses BiLSTM-Attention   ELMo approach for Named Entity Recognition  NER , while Article 2 uses LSTM-CRF for Named Entity Recognition  NER ."
1489,1489,47_paper_4,47_paper_2,481,485,1802.05365,1809.0795,Named Entity Recognition  NER ,"sent: Article 1 uses multiple datasets:  OntoNotes,  CoNLL 2012,  SNLI,  CoNLL 2003  English ,  ACL-ARC,  SQuAD2 0,  SST-5 Fine-grained classification, and  SQuAD1 1, while Article 2 uses multiple datasets:  BC5CDR, and  JNLPBA.","sent: Article 1 uses BiLSTM-CRF ELMo approach for Named Entity Recognition  NER , while Article 2 uses CollaboNet for Named Entity Recognition  NER . sent: Article 1 uses BiDAF   Self Attention   ELMo  ensemble  approach for Named Entity Recognition  NER , while Article 2 uses CollaboNet for Named Entity Recognition  NER . sent: Article 1 uses  Lee et al   2017  ELMo approach for Named Entity Recognition  NER , while Article 2 uses CollaboNet for Named Entity Recognition  NER . sent: Article 1 uses BiDAF   Self Attention   ELMo  single model  approach for Named Entity Recognition  NER , while Article 2 uses CollaboNet for Named Entity Recognition  NER . sent: Article 1 uses BCN ELMo approach for Named Entity Recognition  NER , while Article 2 uses CollaboNet for Named Entity Recognition  NER . sent: Article 1 uses ESIM   ELMo Ensemble approach for Named Entity Recognition  NER , while Article 2 uses CollaboNet for Named Entity Recognition  NER . sent: Article 1 uses  He et al   2017    ELMo approach for Named Entity Recognition  NER , while Article 2 uses CollaboNet for Named Entity Recognition  NER . sent: Article 1 uses ESIM   ELMo approach for Named Entity Recognition  NER , while Article 2 uses CollaboNet for Named Entity Recognition  NER . sent: Article 1 uses BiLSTM-Attention   ELMo approach for Named Entity Recognition  NER , while Article 2 uses CollaboNet for Named Entity Recognition  NER ."
1490,1490,47_paper_6,47_paper_0,482,484,1808.09075,1603.0136,Named Entity Recognition  NER ,sent: Article 1 and Article 2 use CoNLL 2003  English  dataset.,"sent: Article 1 uses Neural-CRF AE approach for Named Entity Recognition  NER , while Article 2 uses LSTM-CRF for Named Entity Recognition  NER . sent: Article 1 uses CRF   AutoEncoder approach for Named Entity Recognition  NER , while Article 2 uses LSTM-CRF for Named Entity Recognition  NER ."
1491,1491,47_paper_6,47_paper_2,482,485,1808.09075,1809.0795,Named Entity Recognition  NER ,"sent: Article 1 uses CoNLL 2003  English  dataset, while Article 2 uses multiple datasets:  BC5CDR, and  JNLPBA.","sent: Article 1 uses Neural-CRF AE approach for Named Entity Recognition  NER , while Article 2 uses CollaboNet for Named Entity Recognition  NER . sent: Article 1 uses CRF   AutoEncoder approach for Named Entity Recognition  NER , while Article 2 uses CollaboNet for Named Entity Recognition  NER ."
1492,1492,47_paper_0,47_paper_2,484,485,1603.0136,1809.0795,Named Entity Recognition  NER ,"sent: Article 1 uses CoNLL 2003  English  dataset, while Article 2 uses multiple datasets:  BC5CDR, and  JNLPBA.","sent: Article 1 uses LSTM-CRF approach for Named Entity Recognition  NER , while Article 2 uses CollaboNet for Named Entity Recognition  NER ."
1493,1493,48_paper_3,48_paper_4,490,493,1711.00199,1711.10871,6D Pose Estimation,"sent: Article 1 and Article 2 use YCB-Video dataset. sent: Article 1 uses multiple datasets:  LineMOD, and  OccludedLINEMOD, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses PoseCNN approach for 6D Pose Estimation, while Article 2 uses PointFusion for 6D Pose Estimation. sent: Article 1 uses PoseCNN   ICP approach for 6D Pose Estimation, while Article 2 uses PointFusion for 6D Pose Estimation."
1494,1494,48_paper_3,48_paper_0,490,494,1711.00199,1902.01275,6D Pose Estimation,"sent: Article 1 uses multiple datasets:  LineMOD,  YCB-Video, and  OccludedLINEMOD, while Article 2 uses T-LESS dataset.","sent: Article 1 uses PoseCNN approach for 6D Pose Estimation, while Article 2 uses RetinaNet Augmented Autoencoders ICP for 6D Pose Estimation. sent: Article 1 uses PoseCNN   ICP approach for 6D Pose Estimation, while Article 2 uses RetinaNet Augmented Autoencoders ICP for 6D Pose Estimation."
1495,1495,48_paper_4,48_paper_0,493,494,1711.10871,1902.01275,6D Pose Estimation,"sent: Article 1 uses YCB-Video dataset, while Article 2 uses T-LESS dataset.","sent: Article 1 uses PointFusion approach for 6D Pose Estimation, while Article 2 uses RetinaNet Augmented Autoencoders ICP for 6D Pose Estimation."
1496,1496,49_paper_4,49_paper_0,500,501,1412.1265,1704.03373,Face Verification,"sent: Article 1 and Article 2 use YouTube Faces DB dataset. sent: Article 1 uses Labeled Faces in the Wild dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DeepId2  approach for Face Verification, while Article 2 uses QAN for Face Verification."
1497,1497,49_paper_4,49_paper_5,500,502,1412.1265,1603.05474,Face Verification,"sent: Article 1 uses multiple datasets:  Labeled Faces in the Wild, and  YouTube Faces DB, while Article 2 uses IJB-A dataset.","sent: Article 1 uses DeepId2  approach for Face Verification, while Article 2 uses NAN for Face Verification."
1498,1498,49_paper_4,49_paper_1,500,503,1412.1265,1708.07517,Face Verification,"sent: Article 1 uses multiple datasets:  Labeled Faces in the Wild, and  YouTube Faces DB, while Article 2 uses multiple datasets:  IJB-B,  300W, and  IJB-A.","sent: Article 1 uses DeepId2  approach for Face Verification, while Article 2 uses FPN for Face Verification."
1499,1499,49_paper_4,49_paper_6,500,504,1412.1265,1603.03958,Face Verification,"sent: Article 1 uses multiple datasets:  Labeled Faces in the Wild, and  YouTube Faces DB, while Article 2 uses IJB-A dataset.","sent: Article 1 uses DeepId2  approach for Face Verification, while Article 2 uses Template adaptation for Face Verification."
1500,1500,49_paper_4,49_paper_7,500,505,1412.1265,1406.4773,Face Verification,"sent: Article 1 and Article 2 use Labeled Faces in the Wild dataset. sent: Article 1 uses YouTube Faces DB dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DeepId2  approach for Face Verification, while Article 2 uses DeepId2 for Face Verification."
1501,1501,49_paper_0,49_paper_5,501,502,1704.03373,1603.05474,Face Verification,"sent: Article 1 uses YouTube Faces DB dataset, while Article 2 uses IJB-A dataset.","sent: Article 1 uses QAN approach for Face Verification, while Article 2 uses NAN for Face Verification."
1502,1502,49_paper_0,49_paper_1,501,503,1704.03373,1708.07517,Face Verification,"sent: Article 1 uses YouTube Faces DB dataset, while Article 2 uses multiple datasets:  IJB-B,  300W, and  IJB-A.","sent: Article 1 uses QAN approach for Face Verification, while Article 2 uses FPN for Face Verification."
1503,1503,49_paper_0,49_paper_6,501,504,1704.03373,1603.03958,Face Verification,"sent: Article 1 uses YouTube Faces DB dataset, while Article 2 uses IJB-A dataset.","sent: Article 1 uses QAN approach for Face Verification, while Article 2 uses Template adaptation for Face Verification."
1504,1504,49_paper_0,49_paper_7,501,505,1704.03373,1406.4773,Face Verification,"sent: Article 1 uses YouTube Faces DB dataset, while Article 2 uses Labeled Faces in the Wild dataset.","sent: Article 1 uses QAN approach for Face Verification, while Article 2 uses DeepId2 for Face Verification."
1505,1505,49_paper_0,49_paper_3,501,507,1704.03373,1412.1265,Face Verification,"sent: Article 1 and Article 2 use YouTube Faces DB dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Labeled Faces in the Wild dataset.","sent: Article 1 uses QAN approach for Face Verification, while Article 2 uses DeepId2  for Face Verification."
1506,1506,49_paper_5,49_paper_1,502,503,1603.05474,1708.07517,Face Verification,"sent: Article 1 and Article 2 use IJB-A dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  IJB-B, and  300W.","sent: Article 1 uses NAN approach for Face Verification, while Article 2 uses FPN for Face Verification."
1507,1507,49_paper_5,49_paper_6,502,504,1603.05474,1603.03958,Face Verification,sent: Article 1 and Article 2 use IJB-A dataset.,"sent: Article 1 uses NAN approach for Face Verification, while Article 2 uses Template adaptation for Face Verification."
1508,1508,49_paper_5,49_paper_7,502,505,1603.05474,1406.4773,Face Verification,"sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.","sent: Article 1 uses NAN approach for Face Verification, while Article 2 uses DeepId2 for Face Verification."
1509,1509,49_paper_5,49_paper_3,502,507,1603.05474,1412.1265,Face Verification,"sent: Article 1 uses IJB-A dataset, while Article 2 uses multiple datasets:  Labeled Faces in the Wild, and  YouTube Faces DB.","sent: Article 1 uses NAN approach for Face Verification, while Article 2 uses DeepId2  for Face Verification."
1510,1510,49_paper_1,49_paper_6,503,504,1708.07517,1603.03958,Face Verification,"sent: Article 1 and Article 2 use IJB-A dataset. sent: Article 1 uses multiple datasets:  IJB-B, and  300W, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses FPN approach for Face Verification, while Article 2 uses Template adaptation for Face Verification."
1511,1511,49_paper_1,49_paper_7,503,505,1708.07517,1406.4773,Face Verification,"sent: Article 1 uses multiple datasets:  IJB-B,  300W, and  IJB-A, while Article 2 uses Labeled Faces in the Wild dataset.","sent: Article 1 uses FPN approach for Face Verification, while Article 2 uses DeepId2 for Face Verification."
1512,1512,49_paper_1,49_paper_3,503,507,1708.07517,1412.1265,Face Verification,"sent: Article 1 uses multiple datasets:  IJB-B,  300W, and  IJB-A, while Article 2 uses multiple datasets:  Labeled Faces in the Wild, and  YouTube Faces DB.","sent: Article 1 uses FPN approach for Face Verification, while Article 2 uses DeepId2  for Face Verification."
1513,1513,49_paper_6,49_paper_7,504,505,1603.03958,1406.4773,Face Verification,"sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.","sent: Article 1 uses Template adaptation approach for Face Verification, while Article 2 uses DeepId2 for Face Verification."
1514,1514,49_paper_6,49_paper_2,504,506,1603.03958,1708.07517,Face Verification,"sent: Article 1 and Article 2 use IJB-A dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  IJB-B, and  300W.","sent: Article 1 uses Template adaptation approach for Face Verification, while Article 2 uses FPN for Face Verification."
1515,1515,49_paper_6,49_paper_3,504,507,1603.03958,1412.1265,Face Verification,"sent: Article 1 uses IJB-A dataset, while Article 2 uses multiple datasets:  Labeled Faces in the Wild, and  YouTube Faces DB.","sent: Article 1 uses Template adaptation approach for Face Verification, while Article 2 uses DeepId2  for Face Verification."
1516,1516,49_paper_7,49_paper_2,505,506,1406.4773,1708.07517,Face Verification,"sent: Article 1 uses Labeled Faces in the Wild dataset, while Article 2 uses multiple datasets:  IJB-B,  300W, and  IJB-A.","sent: Article 1 uses DeepId2 approach for Face Verification, while Article 2 uses FPN for Face Verification."
1517,1517,49_paper_7,49_paper_3,505,507,1406.4773,1412.1265,Face Verification,"sent: Article 1 and Article 2 use Labeled Faces in the Wild dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses YouTube Faces DB dataset.","sent: Article 1 uses DeepId2 approach for Face Verification, while Article 2 uses DeepId2  for Face Verification."
1518,1518,50_paper_3,50_paper_1,510,511,1903.09359,1808.01558,Face Alignment,"sent: Article 1 uses AFLW2000-3D dataset, while Article 2 uses AFLW2000 dataset.","sent: Article 1 uses 2DASL approach for Face Alignment, while Article 2 uses MCL for Face Alignment."
1519,1519,50_paper_3,50_paper_2,510,512,1903.09359,1804.01005,Face Alignment,"sent: Article 1 and Article 2 use AFLW2000-3D dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  AFLW2000,  BIWI, and  Florence.","sent: Article 1 uses 2DASL approach for Face Alignment, while Article 2 uses 3DDFA for Face Alignment. sent: Article 1 uses 2DASL approach for Face Alignment, while Article 2 uses 3DDFA   SDM for Face Alignment."
1520,1520,50_paper_3,50_paper_0,510,513,1903.09359,1804.03786,Face Alignment,"sent: Article 1 uses AFLW2000-3D dataset, while Article 2 uses AFLW2000 dataset.","sent: Article 1 uses 2DASL approach for Face Alignment, while Article 2 uses Nonlinear 3D Face Morphable Model for Face Alignment."
1521,1521,50_paper_1,50_paper_2,511,512,1808.01558,1804.01005,Face Alignment,"sent: Article 1 and Article 2 use AFLW2000 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  BIWI,  AFLW2000-3D, and  Florence.","sent: Article 1 uses MCL approach for Face Alignment, while Article 2 uses 3DDFA for Face Alignment. sent: Article 1 uses MCL approach for Face Alignment, while Article 2 uses 3DDFA   SDM for Face Alignment."
1522,1522,50_paper_1,50_paper_0,511,513,1808.01558,1804.03786,Face Alignment,sent: Article 1 and Article 2 use AFLW2000 dataset.,"sent: Article 1 uses MCL approach for Face Alignment, while Article 2 uses Nonlinear 3D Face Morphable Model for Face Alignment."
1523,1523,50_paper_2,50_paper_0,512,513,1804.01005,1804.03786,Face Alignment,"sent: Article 1 and Article 2 use AFLW2000 dataset. sent: Article 1 uses multiple datasets:  BIWI,  AFLW2000-3D, and  Florence, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses 3DDFA approach for Face Alignment, while Article 2 uses Nonlinear 3D Face Morphable Model for Face Alignment. sent: Article 1 uses 3DDFA   SDM approach for Face Alignment, while Article 2 uses Nonlinear 3D Face Morphable Model for Face Alignment."
1524,1524,51_paper_35,51_paper_10,520,521,1808.07018,1901.0959,Link Prediction,sent: Article 1 and Article 2 use FB15k-237 dataset.sent: Article 1 and Article 2 use  FB15k dataset.sent: Article 1 and Article 2 use WN18RR dataset.sent: Article 1 and Article 2 use WN18 dataset.,"sent: Article 1 uses HypER approach for Link Prediction, while Article 2 uses TuckER for Link Prediction."
1525,1525,51_paper_35,51_paper_5,520,524,1808.07018,1606.06357,Link Prediction,"sent: Article 1 and Article 2 use WN18 dataset. sent: Article 1 uses multiple datasets:  FB15k-237,   FB15k, and  WN18RR, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses HypER approach for Link Prediction, while Article 2 uses ComplEx for Link Prediction."
1526,1526,51_paper_35,51_paper_25,520,528,1808.07018,1811.02798,Link Prediction,"sent: Article 1 uses multiple datasets:  FB15k-237,   FB15k,  WN18RR, and  WN18, while Article 2 uses multiple datasets:  Citeseer,  Pubmed, and  Cora.","sent: Article 1 uses HypER approach for Link Prediction, while Article 2 uses MTGAE for Link Prediction."
1527,1527,51_paper_35,51_paper_0,520,5228,1808.07018,1802.04394,Link Prediction,"sent: Article 1 and Article 2 use WN18RR dataset. sent: Article 1 uses multiple datasets:  FB15k-237,   FB15k, and  WN18, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses HypER approach for Link Prediction, while Article 2 uses M-Walk for Link Prediction."
1528,1528,51_paper_10,51_paper_5,521,524,1901.0959,1606.06357,Link Prediction,"sent: Article 1 and Article 2 use WN18 dataset. sent: Article 1 uses multiple datasets:  FB15k-237,   FB15k, and  WN18RR, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses TuckER approach for Link Prediction, while Article 2 uses ComplEx for Link Prediction."
1529,1529,51_paper_10,51_paper_27,521,525,1901.0959,1808.07018,Link Prediction,sent: Article 1 and Article 2 use FB15k-237 dataset.sent: Article 1 and Article 2 use  FB15k dataset.sent: Article 1 and Article 2 use WN18RR dataset.sent: Article 1 and Article 2 use WN18 dataset.,"sent: Article 1 uses TuckER approach for Link Prediction, while Article 2 uses HypER for Link Prediction."
1530,1530,51_paper_10,51_paper_25,521,528,1901.0959,1811.02798,Link Prediction,"sent: Article 1 uses multiple datasets:  FB15k-237,   FB15k,  WN18RR, and  WN18, while Article 2 uses multiple datasets:  Citeseer,  Pubmed, and  Cora.","sent: Article 1 uses TuckER approach for Link Prediction, while Article 2 uses MTGAE for Link Prediction."
1531,1531,51_paper_10,51_paper_0,521,5228,1901.0959,1802.04394,Link Prediction,"sent: Article 1 and Article 2 use WN18RR dataset. sent: Article 1 uses multiple datasets:  FB15k-237,   FB15k, and  WN18, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses TuckER approach for Link Prediction, while Article 2 uses M-Walk for Link Prediction."
1532,1532,51_paper_5,51_paper_27,524,525,1606.06357,1808.07018,Link Prediction,"sent: Article 1 and Article 2 use WN18 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  FB15k-237,   FB15k, and  WN18RR.","sent: Article 1 uses ComplEx approach for Link Prediction, while Article 2 uses HypER for Link Prediction."
1533,1533,51_paper_5,51_paper_11,524,526,1606.06357,1901.0959,Link Prediction,"sent: Article 1 and Article 2 use WN18 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  FB15k-237,   FB15k, and  WN18RR.","sent: Article 1 uses ComplEx approach for Link Prediction, while Article 2 uses TuckER for Link Prediction."
1534,1534,51_paper_5,51_paper_25,524,528,1606.06357,1811.02798,Link Prediction,"sent: Article 1 uses WN18 dataset, while Article 2 uses multiple datasets:  Citeseer,  Pubmed, and  Cora.","sent: Article 1 uses ComplEx approach for Link Prediction, while Article 2 uses MTGAE for Link Prediction."
1535,1535,51_paper_5,51_paper_0,524,5228,1606.06357,1802.04394,Link Prediction,"sent: Article 1 uses WN18 dataset, while Article 2 uses WN18RR dataset.","sent: Article 1 uses ComplEx approach for Link Prediction, while Article 2 uses M-Walk for Link Prediction."
1536,1536,51_paper_25,51_paper_9,528,529,1811.02798,1901.0959,Link Prediction,"sent: Article 1 uses multiple datasets:  Citeseer,  Pubmed, and  Cora, while Article 2 uses multiple datasets:  FB15k-237,   FB15k,  WN18RR, and  WN18.","sent: Article 1 uses MTGAE approach for Link Prediction, while Article 2 uses TuckER for Link Prediction."
1537,1537,51_paper_25,51_paper_30,528,5211,1811.02798,1808.07018,Link Prediction,"sent: Article 1 uses multiple datasets:  Citeseer,  Pubmed, and  Cora, while Article 2 uses multiple datasets:  FB15k-237,   FB15k,  WN18RR, and  WN18.","sent: Article 1 uses MTGAE approach for Link Prediction, while Article 2 uses HypER for Link Prediction."
1538,1538,51_paper_25,51_paper_3,528,5222,1811.02798,1606.06357,Link Prediction,"sent: Article 1 uses multiple datasets:  Citeseer,  Pubmed, and  Cora, while Article 2 uses WN18 dataset.","sent: Article 1 uses MTGAE approach for Link Prediction, while Article 2 uses ComplEx for Link Prediction."
1539,1539,51_paper_25,51_paper_0,528,5228,1811.02798,1802.04394,Link Prediction,"sent: Article 1 uses multiple datasets:  Citeseer,  Pubmed, and  Cora, while Article 2 uses WN18RR dataset.","sent: Article 1 uses MTGAE approach for Link Prediction, while Article 2 uses M-Walk for Link Prediction."
1540,1540,51_paper_0,51_paper_22,5228,5229,1802.04394,1901.0959,Link Prediction,"sent: Article 1 and Article 2 use WN18RR dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  FB15k-237,   FB15k, and  WN18.","sent: Article 1 uses M-Walk approach for Link Prediction, while Article 2 uses TuckER for Link Prediction."
1541,1541,51_paper_0,51_paper_6,5228,5231,1802.04394,1606.06357,Link Prediction,"sent: Article 1 uses WN18RR dataset, while Article 2 uses WN18 dataset.","sent: Article 1 uses M-Walk approach for Link Prediction, while Article 2 uses ComplEx for Link Prediction."
1542,1542,51_paper_0,51_paper_34,5228,5232,1802.04394,1808.07018,Link Prediction,"sent: Article 1 and Article 2 use WN18RR dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  FB15k-237,   FB15k, and  WN18.","sent: Article 1 uses M-Walk approach for Link Prediction, while Article 2 uses HypER for Link Prediction."
1543,1543,51_paper_0,51_paper_24,5228,5238,1802.04394,1811.02798,Link Prediction,"sent: Article 1 uses WN18RR dataset, while Article 2 uses multiple datasets:  Citeseer,  Pubmed, and  Cora.","sent: Article 1 uses M-Walk approach for Link Prediction, while Article 2 uses MTGAE for Link Prediction."
1544,1544,56_paper_2,56_paper_0,570,571,1611.05774,1706.03762,Constituency Parsing,"sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French,  IWSLT2015 English-German, and  IWSLT2015 German-English.","sent: Article 1 uses Stack-only RNNG approach for Constituency Parsing, while Article 2 uses Transformer for Constituency Parsing. sent: Article 1 uses Stack-only RNNG approach for Constituency Parsing, while Article 2 uses Transformer Base for Constituency Parsing. sent: Article 1 uses Stack-only RNNG approach for Constituency Parsing, while Article 2 uses Transformer Big for Constituency Parsing."
1545,1545,56_paper_2,56_paper_1,570,572,1611.05774,1808.10143,Constituency Parsing,"sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Penn Treebank  Word Level , and  WikiText-2.","sent: Article 1 uses Stack-only RNNG approach for Constituency Parsing, while Article 2 uses AWD-LSTM-DOC x5 for Constituency Parsing. sent: Article 1 uses Stack-only RNNG approach for Constituency Parsing, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Constituency Parsing. sent: Article 1 uses Stack-only RNNG approach for Constituency Parsing, while Article 2 uses AWD-LSTM-DOC for Constituency Parsing."
1546,1546,56_paper_0,56_paper_1,571,572,1706.03762,1808.10143,Constituency Parsing,"sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French,  IWSLT2015 English-German, and  IWSLT2015 German-English, while Article 2 uses multiple datasets:  Penn Treebank  Word Level , and  WikiText-2.","sent: Article 1 uses Transformer approach for Constituency Parsing, while Article 2 uses AWD-LSTM-DOC x5 for Constituency Parsing. sent: Article 1 uses Transformer approach for Constituency Parsing, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Constituency Parsing. sent: Article 1 uses Transformer approach for Constituency Parsing, while Article 2 uses AWD-LSTM-DOC for Constituency Parsing. sent: Article 1 uses Transformer Base approach for Constituency Parsing, while Article 2 uses AWD-LSTM-DOC x5 for Constituency Parsing. sent: Article 1 uses Transformer Base approach for Constituency Parsing, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Constituency Parsing. sent: Article 1 uses Transformer Base approach for Constituency Parsing, while Article 2 uses AWD-LSTM-DOC for Constituency Parsing. sent: Article 1 uses Transformer Big approach for Constituency Parsing, while Article 2 uses AWD-LSTM-DOC x5 for Constituency Parsing. sent: Article 1 uses Transformer Big approach for Constituency Parsing, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Constituency Parsing. sent: Article 1 uses Transformer Big approach for Constituency Parsing, while Article 2 uses AWD-LSTM-DOC for Constituency Parsing."
1547,1547,59_paper_78,59_paper_16,600,601,1901.0286,1903.04167,Language Modelling,"sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset. sent: Article 1 uses multiple datasets:  Hutter Prize,  WikiText-103,  Text8,  One Billion Word, and  enwiki8, while Article 2 uses WikiText-2 dataset.","sent: Article 1 uses Transformer-XL - 24 layers approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling. sent: Article 1 uses Transformer-XL approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling. sent: Article 1 uses Transformer-XL - 18 layers approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling. sent: Article 1 uses 12-layer Transformer-XL approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling. sent: Article 1 uses Transformer-XL Standard approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling. sent: Article 1 uses Transformer-XL - 12 layers approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling. sent: Article 1 uses 24-layer Transformer-XL approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling. sent: Article 1 uses Transformer-XL Large approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling. sent: Article 1 uses Transformer-XL Base approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling. sent: Article 1 uses 18-layer Transformer-XL approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling."
1548,1548,59_paper_78,59_paper_38,600,604,1901.0286,1711.03953,Language Modelling,"sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset. sent: Article 1 uses multiple datasets:  Hutter Prize,  WikiText-103,  Text8,  One Billion Word, and  enwiki8, while Article 2 uses WikiText-2 dataset.","sent: Article 1 uses Transformer-XL - 24 layers approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses Transformer-XL - 24 layers approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling. sent: Article 1 uses Transformer-XL approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses Transformer-XL approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling. sent: Article 1 uses 18-layer Transformer-XL approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses 18-layer Transformer-XL approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling. sent: Article 1 uses Transformer-XL - 18 layers approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses Transformer-XL - 18 layers approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling. sent: Article 1 uses 12-layer Transformer-XL approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses 12-layer Transformer-XL approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling. sent: Article 1 uses Transformer-XL Standard approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses Transformer-XL Standard approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling. sent: Article 1 uses 24-layer Transformer-XL approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses 24-layer Transformer-XL approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling. sent: Article 1 uses Transformer-XL Large approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses Transformer-XL Large approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling. sent: Article 1 uses Transformer-XL Base approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses Transformer-XL Base approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling. sent: Article 1 uses Transformer-XL - 12 layers approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses Transformer-XL - 12 layers approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling."
1549,1549,59_paper_78,59_paper_52,600,605,1901.0286,1611.01462,Language Modelling,"sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset. sent: Article 1 uses multiple datasets:  Hutter Prize,  WikiText-103,  Text8,  One Billion Word, and  enwiki8, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Transformer-XL - 24 layers approach for Language Modelling, while Article 2 uses Tied Variational LSTM   augmented loss for Language Modelling. sent: Article 1 uses Transformer-XL approach for Language Modelling, while Article 2 uses Tied Variational LSTM   augmented loss for Language Modelling. sent: Article 1 uses Transformer-XL - 18 layers approach for Language Modelling, while Article 2 uses Tied Variational LSTM   augmented loss for Language Modelling. sent: Article 1 uses 12-layer Transformer-XL approach for Language Modelling, while Article 2 uses Tied Variational LSTM   augmented loss for Language Modelling. sent: Article 1 uses Transformer-XL Standard approach for Language Modelling, while Article 2 uses Tied Variational LSTM   augmented loss for Language Modelling. sent: Article 1 uses Transformer-XL - 12 layers approach for Language Modelling, while Article 2 uses Tied Variational LSTM   augmented loss for Language Modelling. sent: Article 1 uses 24-layer Transformer-XL approach for Language Modelling, while Article 2 uses Tied Variational LSTM   augmented loss for Language Modelling. sent: Article 1 uses Transformer-XL Large approach for Language Modelling, while Article 2 uses Tied Variational LSTM   augmented loss for Language Modelling. sent: Article 1 uses Transformer-XL Base approach for Language Modelling, while Article 2 uses Tied Variational LSTM   augmented loss for Language Modelling. sent: Article 1 uses 18-layer Transformer-XL approach for Language Modelling, while Article 2 uses Tied Variational LSTM   augmented loss for Language Modelling."
1550,1550,59_paper_78,59_paper_35,600,6010,1901.0286,1609.07959,Language Modelling,"sent: Article 1 and Article 2 use enwiki8 dataset.sent: Article 1 and Article 2 use Text8 dataset.sent: Article 1 and Article 2 use Hutter Prize dataset. sent: Article 1 uses multiple datasets:  WikiText-103,  Penn Treebank  Word Level , and  One Billion Word, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Transformer-XL - 24 layers approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses Transformer-XL - 24 layers approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses Transformer-XL - 24 layers approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling. sent: Article 1 uses Transformer-XL approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses Transformer-XL approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses Transformer-XL approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling. sent: Article 1 uses 18-layer Transformer-XL approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses 18-layer Transformer-XL approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses 18-layer Transformer-XL approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling. sent: Article 1 uses Transformer-XL - 18 layers approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses Transformer-XL - 18 layers approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses Transformer-XL - 18 layers approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling. sent: Article 1 uses 12-layer Transformer-XL approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses 12-layer Transformer-XL approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses 12-layer Transformer-XL approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling. sent: Article 1 uses Transformer-XL Standard approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses Transformer-XL Standard approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses Transformer-XL Standard approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling. sent: Article 1 uses 24-layer Transformer-XL approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses 24-layer Transformer-XL approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses 24-layer Transformer-XL approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling. sent: Article 1 uses Transformer-XL Large approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses Transformer-XL Large approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses Transformer-XL Large approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling. sent: Article 1 uses Transformer-XL Base approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses Transformer-XL Base approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses Transformer-XL Base approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling. sent: Article 1 uses Transformer-XL - 12 layers approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses Transformer-XL - 12 layers approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses Transformer-XL - 12 layers approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling."
1551,1551,59_paper_78,59_paper_14,600,6020,1901.0286,1609.01704,Language Modelling,"sent: Article 1 and Article 2 use enwiki8 dataset.sent: Article 1 and Article 2 use Text8 dataset. sent: Article 1 uses multiple datasets:  WikiText-103,  Penn Treebank  Word Level ,  One Billion Word, and  Hutter Prize, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Transformer-XL - 24 layers approach for Language Modelling, while Article 2 uses LayerNorm HM-LSTM for Language Modelling. sent: Article 1 uses Transformer-XL - 24 layers approach for Language Modelling, while Article 2 uses LN HM-LSTM for Language Modelling. sent: Article 1 uses Transformer-XL approach for Language Modelling, while Article 2 uses LayerNorm HM-LSTM for Language Modelling. sent: Article 1 uses Transformer-XL approach for Language Modelling, while Article 2 uses LN HM-LSTM for Language Modelling. sent: Article 1 uses 18-layer Transformer-XL approach for Language Modelling, while Article 2 uses LayerNorm HM-LSTM for Language Modelling. sent: Article 1 uses 18-layer Transformer-XL approach for Language Modelling, while Article 2 uses LN HM-LSTM for Language Modelling. sent: Article 1 uses Transformer-XL - 18 layers approach for Language Modelling, while Article 2 uses LayerNorm HM-LSTM for Language Modelling. sent: Article 1 uses Transformer-XL - 18 layers approach for Language Modelling, while Article 2 uses LN HM-LSTM for Language Modelling. sent: Article 1 uses 12-layer Transformer-XL approach for Language Modelling, while Article 2 uses LayerNorm HM-LSTM for Language Modelling. sent: Article 1 uses 12-layer Transformer-XL approach for Language Modelling, while Article 2 uses LN HM-LSTM for Language Modelling. sent: Article 1 uses Transformer-XL Standard approach for Language Modelling, while Article 2 uses LayerNorm HM-LSTM for Language Modelling. sent: Article 1 uses Transformer-XL Standard approach for Language Modelling, while Article 2 uses LN HM-LSTM for Language Modelling. sent: Article 1 uses 24-layer Transformer-XL approach for Language Modelling, while Article 2 uses LayerNorm HM-LSTM for Language Modelling. sent: Article 1 uses 24-layer Transformer-XL approach for Language Modelling, while Article 2 uses LN HM-LSTM for Language Modelling. sent: Article 1 uses Transformer-XL Large approach for Language Modelling, while Article 2 uses LayerNorm HM-LSTM for Language Modelling. sent: Article 1 uses Transformer-XL Large approach for Language Modelling, while Article 2 uses LN HM-LSTM for Language Modelling. sent: Article 1 uses Transformer-XL Base approach for Language Modelling, while Article 2 uses LayerNorm HM-LSTM for Language Modelling. sent: Article 1 uses Transformer-XL Base approach for Language Modelling, while Article 2 uses LN HM-LSTM for Language Modelling. sent: Article 1 uses Transformer-XL - 12 layers approach for Language Modelling, while Article 2 uses LayerNorm HM-LSTM for Language Modelling. sent: Article 1 uses Transformer-XL - 12 layers approach for Language Modelling, while Article 2 uses LN HM-LSTM for Language Modelling."
1552,1552,59_paper_78,59_paper_10,600,6023,1901.0286,1808.10143,Language Modelling,"sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset. sent: Article 1 uses multiple datasets:  Hutter Prize,  WikiText-103,  Text8,  One Billion Word, and  enwiki8, while Article 2 uses multiple datasets:  WikiText-2, and  Penn Treebank.","sent: Article 1 uses Transformer-XL - 24 layers approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses Transformer-XL - 24 layers approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses Transformer-XL - 24 layers approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling. sent: Article 1 uses Transformer-XL approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses Transformer-XL approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses Transformer-XL approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling. sent: Article 1 uses 18-layer Transformer-XL approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses 18-layer Transformer-XL approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses 18-layer Transformer-XL approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling. sent: Article 1 uses Transformer-XL - 18 layers approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses Transformer-XL - 18 layers approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses Transformer-XL - 18 layers approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling. sent: Article 1 uses 12-layer Transformer-XL approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses 12-layer Transformer-XL approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses 12-layer Transformer-XL approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling. sent: Article 1 uses Transformer-XL Standard approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses Transformer-XL Standard approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses Transformer-XL Standard approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling. sent: Article 1 uses 24-layer Transformer-XL approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses 24-layer Transformer-XL approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses 24-layer Transformer-XL approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling. sent: Article 1 uses Transformer-XL Large approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses Transformer-XL Large approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses Transformer-XL Large approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling. sent: Article 1 uses Transformer-XL Base approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses Transformer-XL Base approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses Transformer-XL Base approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling. sent: Article 1 uses Transformer-XL - 12 layers approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses Transformer-XL - 12 layers approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses Transformer-XL - 12 layers approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling."
1553,1553,59_paper_16,59_paper_57,601,602,1903.04167,1901.0286,Language Modelling,"sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset. sent: Article 1 uses WikiText-2 dataset, while Article 2 uses multiple datasets:  Hutter Prize,  WikiText-103,  Text8,  One Billion Word, and  enwiki8.","sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses Transformer-XL - 24 layers for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses Transformer-XL for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses Transformer-XL - 18 layers for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses 12-layer Transformer-XL for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses Transformer-XL Standard for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses Transformer-XL - 12 layers for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses 24-layer Transformer-XL for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses Transformer-XL Large for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses Transformer-XL Base for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses 18-layer Transformer-XL for Language Modelling."
1554,1554,59_paper_16,59_paper_38,601,604,1903.04167,1711.03953,Language Modelling,sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset.sent: Article 1 and Article 2 use WikiText-2 dataset.,"sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling."
1555,1555,59_paper_16,59_paper_52,601,605,1903.04167,1611.01462,Language Modelling,"sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset. sent: Article 1 uses WikiText-2 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses Tied Variational LSTM   augmented loss for Language Modelling."
1556,1556,59_paper_16,59_paper_35,601,6010,1903.04167,1609.07959,Language Modelling,"sent: Article 1 uses multiple datasets:  Penn Treebank  Word Level , and  WikiText-2, while Article 2 uses multiple datasets:  enwiki8,  Text8, and  Hutter Prize.","sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling."
1557,1557,59_paper_16,59_paper_14,601,6020,1903.04167,1609.01704,Language Modelling,"sent: Article 1 uses multiple datasets:  Penn Treebank  Word Level , and  WikiText-2, while Article 2 uses multiple datasets:  enwiki8, and  Text8.","sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses LayerNorm HM-LSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses LN HM-LSTM for Language Modelling."
1558,1558,59_paper_16,59_paper_10,601,6023,1903.04167,1808.10143,Language Modelling,"sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset.sent: Article 1 and Article 2 use WikiText-2 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Penn Treebank dataset.","sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   Partial Shuffle approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling."
1559,1559,59_paper_38,59_paper_52,604,605,1711.03953,1611.01462,Language Modelling,"sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset. sent: Article 1 uses WikiText-2 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses Tied Variational LSTM   augmented loss for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses Tied Variational LSTM   augmented loss for Language Modelling."
1560,1560,59_paper_38,59_paper_64,604,606,1711.03953,1901.0286,Language Modelling,"sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset. sent: Article 1 uses WikiText-2 dataset, while Article 2 uses multiple datasets:  Hutter Prize,  WikiText-103,  Text8,  One Billion Word, and  enwiki8.","sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses Transformer-XL - 24 layers for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses Transformer-XL for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses 18-layer Transformer-XL for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses Transformer-XL - 18 layers for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses 12-layer Transformer-XL for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses Transformer-XL Standard for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses 24-layer Transformer-XL for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses Transformer-XL Large for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses Transformer-XL Base for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses Transformer-XL - 12 layers for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses Transformer-XL - 24 layers for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses Transformer-XL for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses 18-layer Transformer-XL for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses Transformer-XL - 18 layers for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses 12-layer Transformer-XL for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses Transformer-XL Standard for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses 24-layer Transformer-XL for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses Transformer-XL Large for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses Transformer-XL Base for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses Transformer-XL - 12 layers for Language Modelling."
1561,1561,59_paper_38,59_paper_20,604,609,1711.03953,1903.04167,Language Modelling,sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset.sent: Article 1 and Article 2 use WikiText-2 dataset.,"sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling."
1562,1562,59_paper_38,59_paper_35,604,6010,1711.03953,1609.07959,Language Modelling,"sent: Article 1 uses multiple datasets:  Penn Treebank  Word Level , and  WikiText-2, while Article 2 uses multiple datasets:  enwiki8,  Text8, and  Hutter Prize.","sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling."
1563,1563,59_paper_38,59_paper_14,604,6020,1711.03953,1609.01704,Language Modelling,"sent: Article 1 uses multiple datasets:  Penn Treebank  Word Level , and  WikiText-2, while Article 2 uses multiple datasets:  enwiki8, and  Text8.","sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses LayerNorm HM-LSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses LN HM-LSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses LayerNorm HM-LSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses LN HM-LSTM for Language Modelling."
1564,1564,59_paper_38,59_paper_10,604,6023,1711.03953,1808.10143,Language Modelling,"sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset.sent: Article 1 and Article 2 use WikiText-2 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Penn Treebank dataset.","sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling."
1565,1565,59_paper_52,59_paper_64,605,606,1611.01462,1901.0286,Language Modelling,"sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Hutter Prize,  WikiText-103,  Text8,  One Billion Word, and  enwiki8.","sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses Transformer-XL - 24 layers for Language Modelling. sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses Transformer-XL for Language Modelling. sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses Transformer-XL - 18 layers for Language Modelling. sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses 12-layer Transformer-XL for Language Modelling. sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses Transformer-XL Standard for Language Modelling. sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses Transformer-XL - 12 layers for Language Modelling. sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses 24-layer Transformer-XL for Language Modelling. sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses Transformer-XL Large for Language Modelling. sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses Transformer-XL Base for Language Modelling. sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses 18-layer Transformer-XL for Language Modelling."
1566,1566,59_paper_52,59_paper_20,605,609,1611.01462,1903.04167,Language Modelling,"sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses WikiText-2 dataset.","sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling."
1567,1567,59_paper_52,59_paper_35,605,6010,1611.01462,1609.07959,Language Modelling,"sent: Article 1 uses Penn Treebank  Word Level  dataset, while Article 2 uses multiple datasets:  enwiki8,  Text8, and  Hutter Prize.","sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling."
1568,1568,59_paper_52,59_paper_40,605,6019,1611.01462,1711.03953,Language Modelling,"sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses WikiText-2 dataset.","sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling."
1569,1569,59_paper_52,59_paper_14,605,6020,1611.01462,1609.01704,Language Modelling,"sent: Article 1 uses Penn Treebank  Word Level  dataset, while Article 2 uses multiple datasets:  enwiki8, and  Text8.","sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses LayerNorm HM-LSTM for Language Modelling. sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses LN HM-LSTM for Language Modelling."
1570,1570,59_paper_52,59_paper_10,605,6023,1611.01462,1808.10143,Language Modelling,"sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WikiText-2, and  Penn Treebank.","sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses Tied Variational LSTM   augmented loss approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling."
1571,1571,59_paper_35,59_paper_73,6010,6011,1609.07959,1901.0286,Language Modelling,"sent: Article 1 and Article 2 use enwiki8 dataset.sent: Article 1 and Article 2 use Text8 dataset.sent: Article 1 and Article 2 use Hutter Prize dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WikiText-103,  Penn Treebank  Word Level , and  One Billion Word.","sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses Transformer-XL - 24 layers for Language Modelling. sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses Transformer-XL for Language Modelling. sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses 18-layer Transformer-XL for Language Modelling. sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses Transformer-XL - 18 layers for Language Modelling. sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses 12-layer Transformer-XL for Language Modelling. sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses Transformer-XL Standard for Language Modelling. sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses 24-layer Transformer-XL for Language Modelling. sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses Transformer-XL Large for Language Modelling. sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses Transformer-XL Base for Language Modelling. sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses Transformer-XL - 12 layers for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses Transformer-XL - 24 layers for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses Transformer-XL for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses 18-layer Transformer-XL for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses Transformer-XL - 18 layers for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses 12-layer Transformer-XL for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses Transformer-XL Standard for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses 24-layer Transformer-XL for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses Transformer-XL Large for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses Transformer-XL Base for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses Transformer-XL - 12 layers for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses Transformer-XL - 24 layers for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses Transformer-XL for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses 18-layer Transformer-XL for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses Transformer-XL - 18 layers for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses 12-layer Transformer-XL for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses Transformer-XL Standard for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses 24-layer Transformer-XL for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses Transformer-XL Large for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses Transformer-XL Base for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses Transformer-XL - 12 layers for Language Modelling."
1572,1572,59_paper_35,59_paper_18,6010,6012,1609.07959,1903.04167,Language Modelling,"sent: Article 1 uses multiple datasets:  enwiki8,  Text8, and  Hutter Prize, while Article 2 uses multiple datasets:  Penn Treebank  Word Level , and  WikiText-2.","sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling."
1573,1573,59_paper_35,59_paper_51,6010,6013,1609.07959,1611.01462,Language Modelling,"sent: Article 1 uses multiple datasets:  enwiki8,  Text8, and  Hutter Prize, while Article 2 uses Penn Treebank  Word Level  dataset.","sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses Tied Variational LSTM   augmented loss for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses Tied Variational LSTM   augmented loss for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses Tied Variational LSTM   augmented loss for Language Modelling."
1574,1574,59_paper_35,59_paper_40,6010,6019,1609.07959,1711.03953,Language Modelling,"sent: Article 1 uses multiple datasets:  enwiki8,  Text8, and  Hutter Prize, while Article 2 uses multiple datasets:  Penn Treebank  Word Level , and  WikiText-2.","sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling."
1575,1575,59_paper_35,59_paper_14,6010,6020,1609.07959,1609.01704,Language Modelling,"sent: Article 1 and Article 2 use enwiki8 dataset.sent: Article 1 and Article 2 use Text8 dataset. sent: Article 1 uses Hutter Prize dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses LayerNorm HM-LSTM for Language Modelling. sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses LN HM-LSTM for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses LayerNorm HM-LSTM for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses LN HM-LSTM for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses LayerNorm HM-LSTM for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses LN HM-LSTM for Language Modelling."
1576,1576,59_paper_35,59_paper_10,6010,6023,1609.07959,1808.10143,Language Modelling,"sent: Article 1 uses multiple datasets:  enwiki8,  Text8, and  Hutter Prize, while Article 2 uses multiple datasets:  Penn Treebank  Word Level ,  WikiText-2, and  Penn Treebank.","sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling."
1577,1577,59_paper_14,59_paper_44,6020,6021,1609.01704,1711.03953,Language Modelling,"sent: Article 1 uses multiple datasets:  enwiki8, and  Text8, while Article 2 uses multiple datasets:  Penn Treebank  Word Level , and  WikiText-2.","sent: Article 1 uses LayerNorm HM-LSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses LayerNorm HM-LSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling. sent: Article 1 uses LN HM-LSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses LN HM-LSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling."
1578,1578,59_paper_14,59_paper_10,6020,6023,1609.01704,1808.10143,Language Modelling,"sent: Article 1 uses multiple datasets:  enwiki8, and  Text8, while Article 2 uses multiple datasets:  Penn Treebank  Word Level ,  WikiText-2, and  Penn Treebank.","sent: Article 1 uses LayerNorm HM-LSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses LayerNorm HM-LSTM approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses LayerNorm HM-LSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling. sent: Article 1 uses LN HM-LSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses LN HM-LSTM approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses LN HM-LSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling."
1579,1579,59_paper_14,59_paper_19,6020,6025,1609.01704,1903.04167,Language Modelling,"sent: Article 1 uses multiple datasets:  enwiki8, and  Text8, while Article 2 uses multiple datasets:  Penn Treebank  Word Level , and  WikiText-2.","sent: Article 1 uses LayerNorm HM-LSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling. sent: Article 1 uses LN HM-LSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling."
1580,1580,59_paper_14,59_paper_71,6020,6029,1609.01704,1901.0286,Language Modelling,"sent: Article 1 and Article 2 use enwiki8 dataset.sent: Article 1 and Article 2 use Text8 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WikiText-103,  Penn Treebank  Word Level ,  One Billion Word, and  Hutter Prize.","sent: Article 1 uses LayerNorm HM-LSTM approach for Language Modelling, while Article 2 uses Transformer-XL - 24 layers for Language Modelling. sent: Article 1 uses LayerNorm HM-LSTM approach for Language Modelling, while Article 2 uses Transformer-XL for Language Modelling. sent: Article 1 uses LayerNorm HM-LSTM approach for Language Modelling, while Article 2 uses 18-layer Transformer-XL for Language Modelling. sent: Article 1 uses LayerNorm HM-LSTM approach for Language Modelling, while Article 2 uses Transformer-XL - 18 layers for Language Modelling. sent: Article 1 uses LayerNorm HM-LSTM approach for Language Modelling, while Article 2 uses 12-layer Transformer-XL for Language Modelling. sent: Article 1 uses LayerNorm HM-LSTM approach for Language Modelling, while Article 2 uses Transformer-XL Standard for Language Modelling. sent: Article 1 uses LayerNorm HM-LSTM approach for Language Modelling, while Article 2 uses 24-layer Transformer-XL for Language Modelling. sent: Article 1 uses LayerNorm HM-LSTM approach for Language Modelling, while Article 2 uses Transformer-XL Large for Language Modelling. sent: Article 1 uses LayerNorm HM-LSTM approach for Language Modelling, while Article 2 uses Transformer-XL Base for Language Modelling. sent: Article 1 uses LayerNorm HM-LSTM approach for Language Modelling, while Article 2 uses Transformer-XL - 12 layers for Language Modelling. sent: Article 1 uses LN HM-LSTM approach for Language Modelling, while Article 2 uses Transformer-XL - 24 layers for Language Modelling. sent: Article 1 uses LN HM-LSTM approach for Language Modelling, while Article 2 uses Transformer-XL for Language Modelling. sent: Article 1 uses LN HM-LSTM approach for Language Modelling, while Article 2 uses 18-layer Transformer-XL for Language Modelling. sent: Article 1 uses LN HM-LSTM approach for Language Modelling, while Article 2 uses Transformer-XL - 18 layers for Language Modelling. sent: Article 1 uses LN HM-LSTM approach for Language Modelling, while Article 2 uses 12-layer Transformer-XL for Language Modelling. sent: Article 1 uses LN HM-LSTM approach for Language Modelling, while Article 2 uses Transformer-XL Standard for Language Modelling. sent: Article 1 uses LN HM-LSTM approach for Language Modelling, while Article 2 uses 24-layer Transformer-XL for Language Modelling. sent: Article 1 uses LN HM-LSTM approach for Language Modelling, while Article 2 uses Transformer-XL Large for Language Modelling. sent: Article 1 uses LN HM-LSTM approach for Language Modelling, while Article 2 uses Transformer-XL Base for Language Modelling. sent: Article 1 uses LN HM-LSTM approach for Language Modelling, while Article 2 uses Transformer-XL - 12 layers for Language Modelling."
1581,1581,59_paper_14,59_paper_30,6020,6031,1609.01704,1609.07959,Language Modelling,"sent: Article 1 and Article 2 use enwiki8 dataset.sent: Article 1 and Article 2 use Text8 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Hutter Prize dataset.","sent: Article 1 uses LayerNorm HM-LSTM approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses LayerNorm HM-LSTM approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses LayerNorm HM-LSTM approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling. sent: Article 1 uses LN HM-LSTM approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses LN HM-LSTM approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses LN HM-LSTM approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling."
1582,1582,59_paper_10,59_paper_19,6023,6025,1808.10143,1903.04167,Language Modelling,"sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset.sent: Article 1 and Article 2 use WikiText-2 dataset. sent: Article 1 uses Penn Treebank dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   Partial Shuffle for Language Modelling."
1583,1583,59_paper_10,59_paper_71,6023,6029,1808.10143,1901.0286,Language Modelling,"sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset. sent: Article 1 uses multiple datasets:  WikiText-2, and  Penn Treebank, while Article 2 uses multiple datasets:  Hutter Prize,  WikiText-103,  Text8,  One Billion Word, and  enwiki8.","sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses Transformer-XL - 24 layers for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses Transformer-XL for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses 18-layer Transformer-XL for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses Transformer-XL - 18 layers for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses 12-layer Transformer-XL for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses Transformer-XL Standard for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses 24-layer Transformer-XL for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses Transformer-XL Large for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses Transformer-XL Base for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses Transformer-XL - 12 layers for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses Transformer-XL - 24 layers for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses Transformer-XL for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses 18-layer Transformer-XL for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses Transformer-XL - 18 layers for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses 12-layer Transformer-XL for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses Transformer-XL Standard for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses 24-layer Transformer-XL for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses Transformer-XL Large for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses Transformer-XL Base for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses Transformer-XL - 12 layers for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses Transformer-XL - 24 layers for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses Transformer-XL for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses 18-layer Transformer-XL for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses Transformer-XL - 18 layers for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses 12-layer Transformer-XL for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses Transformer-XL Standard for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses 24-layer Transformer-XL for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses Transformer-XL Large for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses Transformer-XL Base for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses Transformer-XL - 12 layers for Language Modelling."
1584,1584,59_paper_10,59_paper_30,6023,6031,1808.10143,1609.07959,Language Modelling,"sent: Article 1 uses multiple datasets:  Penn Treebank  Word Level ,  WikiText-2, and  Penn Treebank, while Article 2 uses multiple datasets:  enwiki8,  Text8, and  Hutter Prize.","sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling."
1585,1585,59_paper_10,59_paper_45,6023,6033,1808.10143,1711.03953,Language Modelling,"sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset.sent: Article 1 and Article 2 use WikiText-2 dataset. sent: Article 1 uses Penn Treebank dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling."
1586,1586,59_paper_10,59_paper_15,6023,6035,1808.10143,1609.01704,Language Modelling,"sent: Article 1 uses multiple datasets:  Penn Treebank  Word Level ,  WikiText-2, and  Penn Treebank, while Article 2 uses multiple datasets:  enwiki8, and  Text8.","sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses LayerNorm HM-LSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses LN HM-LSTM for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses LayerNorm HM-LSTM for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses LN HM-LSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses LayerNorm HM-LSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses LN HM-LSTM for Language Modelling."
1587,1587,60_paper_3,60_paper_0,610,611,1707.09531,1802.02142,Face Detection,"sent: Article 1 uses Annotated Faces in the Wild dataset, while Article 2 uses multiple datasets:  WIDER Face  Hard ,  WIDER Face  Easy , and  WIDER Face  Medium .","sent: Article 1 uses LRN   RSA approach for Face Detection, while Article 2 uses FDNet for Face Detection."
1588,1588,61_paper_19,61_paper_30,620,623,1711.10295,1808.06281,Person Re-Identification,sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.,"sent: Article 1 uses IDE    CamStyle   Random Erasing approach for Person Re-Identification, while Article 2 uses Incremental Learning for Person Re-Identification. sent: Article 1 uses IDE    CamStyle approach for Person Re-Identification, while Article 2 uses Incremental Learning for Person Re-Identification. sent: Article 1 uses IDE  approach for Person Re-Identification, while Article 2 uses Incremental Learning for Person Re-Identification."
1589,1589,61_paper_19,61_paper_2,620,624,1711.10295,1703.05693,Person Re-Identification,sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.,"sent: Article 1 uses IDE    CamStyle   Random Erasing approach for Person Re-Identification, while Article 2 uses SVDNet for Person Re-Identification. sent: Article 1 uses IDE    CamStyle approach for Person Re-Identification, while Article 2 uses SVDNet for Person Re-Identification. sent: Article 1 uses IDE  approach for Person Re-Identification, while Article 2 uses SVDNet for Person Re-Identification."
1590,1590,61_paper_19,61_paper_27,620,625,1711.10295,1708.04896,Person Re-Identification,"sent: Article 1 and Article 2 use DukeMTMC-reID dataset. sent: Article 1 uses Market-1501 dataset, while Article 2 uses multiple datasets:  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses IDE    CamStyle   Random Erasing approach for Person Re-Identification, while Article 2 uses TriNet   Random Erasing for Person Re-Identification. sent: Article 1 uses IDE    CamStyle   Random Erasing approach for Person Re-Identification, while Article 2 uses SVDNet   Random Erasing for Person Re-Identification. sent: Article 1 uses IDE    CamStyle   Random Erasing approach for Person Re-Identification, while Article 2 uses I ORE for Person Re-Identification. sent: Article 1 uses IDE    CamStyle   Random Erasing approach for Person Re-Identification, while Article 2 uses Random Erasing for Person Re-Identification. sent: Article 1 uses IDE    CamStyle approach for Person Re-Identification, while Article 2 uses TriNet   Random Erasing for Person Re-Identification. sent: Article 1 uses IDE    CamStyle approach for Person Re-Identification, while Article 2 uses SVDNet   Random Erasing for Person Re-Identification. sent: Article 1 uses IDE    CamStyle approach for Person Re-Identification, while Article 2 uses I ORE for Person Re-Identification. sent: Article 1 uses IDE    CamStyle approach for Person Re-Identification, while Article 2 uses Random Erasing for Person Re-Identification. sent: Article 1 uses IDE  approach for Person Re-Identification, while Article 2 uses TriNet   Random Erasing for Person Re-Identification. sent: Article 1 uses IDE  approach for Person Re-Identification, while Article 2 uses SVDNet   Random Erasing for Person Re-Identification. sent: Article 1 uses IDE  approach for Person Re-Identification, while Article 2 uses I ORE for Person Re-Identification. sent: Article 1 uses IDE  approach for Person Re-Identification, while Article 2 uses Random Erasing for Person Re-Identification."
1591,1591,61_paper_19,61_paper_9,620,626,1711.10295,1701.07717,Person Re-Identification,sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.,"sent: Article 1 uses IDE    CamStyle   Random Erasing approach for Person Re-Identification, while Article 2 uses GAN for Person Re-Identification. sent: Article 1 uses IDE    CamStyle approach for Person Re-Identification, while Article 2 uses GAN for Person Re-Identification. sent: Article 1 uses IDE  approach for Person Re-Identification, while Article 2 uses GAN for Person Re-Identification."
1592,1592,61_paper_19,61_paper_1,620,6210,1711.10295,1604.0185,Person Re-Identification,"sent: Article 1 and Article 2 use DukeMTMC-reID dataset. sent: Article 1 uses Market-1501 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses IDE    CamStyle   Random Erasing approach for Person Re-Identification, while Article 2 uses OIM for Person Re-Identification. sent: Article 1 uses IDE    CamStyle approach for Person Re-Identification, while Article 2 uses OIM for Person Re-Identification. sent: Article 1 uses IDE  approach for Person Re-Identification, while Article 2 uses OIM for Person Re-Identification."
1593,1593,61_paper_19,61_paper_23,620,6215,1711.10295,1603.02139,Person Re-Identification,"sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 uses DukeMTMC-reID dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses IDE    CamStyle   Random Erasing approach for Person Re-Identification, while Article 2 uses DNS for Person Re-Identification. sent: Article 1 uses IDE    CamStyle approach for Person Re-Identification, while Article 2 uses DNS for Person Re-Identification. sent: Article 1 uses IDE  approach for Person Re-Identification, while Article 2 uses DNS for Person Re-Identification."
1594,1594,61_paper_19,61_paper_32,620,6218,1711.10295,1710.06555,Person Re-Identification,"sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 uses DukeMTMC-reID dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses IDE    CamStyle   Random Erasing approach for Person Re-Identification, while Article 2 uses MSCAN for Person Re-Identification. sent: Article 1 uses IDE    CamStyle approach for Person Re-Identification, while Article 2 uses MSCAN for Person Re-Identification. sent: Article 1 uses IDE  approach for Person Re-Identification, while Article 2 uses MSCAN for Person Re-Identification."
1595,1595,61_paper_30,61_paper_2,623,624,1808.06281,1703.05693,Person Re-Identification,sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.,"sent: Article 1 uses Incremental Learning approach for Person Re-Identification, while Article 2 uses SVDNet for Person Re-Identification."
1596,1596,61_paper_30,61_paper_27,623,625,1808.06281,1708.04896,Person Re-Identification,"sent: Article 1 and Article 2 use DukeMTMC-reID dataset. sent: Article 1 uses Market-1501 dataset, while Article 2 uses multiple datasets:  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses Incremental Learning approach for Person Re-Identification, while Article 2 uses TriNet   Random Erasing for Person Re-Identification. sent: Article 1 uses Incremental Learning approach for Person Re-Identification, while Article 2 uses SVDNet   Random Erasing for Person Re-Identification. sent: Article 1 uses Incremental Learning approach for Person Re-Identification, while Article 2 uses I ORE for Person Re-Identification. sent: Article 1 uses Incremental Learning approach for Person Re-Identification, while Article 2 uses Random Erasing for Person Re-Identification."
1597,1597,61_paper_30,61_paper_9,623,626,1808.06281,1701.07717,Person Re-Identification,sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.,"sent: Article 1 uses Incremental Learning approach for Person Re-Identification, while Article 2 uses GAN for Person Re-Identification."
1598,1598,61_paper_30,61_paper_17,623,627,1808.06281,1711.10295,Person Re-Identification,sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.,"sent: Article 1 uses Incremental Learning approach for Person Re-Identification, while Article 2 uses IDE    CamStyle   Random Erasing for Person Re-Identification. sent: Article 1 uses Incremental Learning approach for Person Re-Identification, while Article 2 uses IDE    CamStyle for Person Re-Identification. sent: Article 1 uses Incremental Learning approach for Person Re-Identification, while Article 2 uses IDE  for Person Re-Identification."
1599,1599,61_paper_30,61_paper_1,623,6210,1808.06281,1604.0185,Person Re-Identification,"sent: Article 1 and Article 2 use DukeMTMC-reID dataset. sent: Article 1 uses Market-1501 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Incremental Learning approach for Person Re-Identification, while Article 2 uses OIM for Person Re-Identification."
1600,1600,61_paper_30,61_paper_23,623,6215,1808.06281,1603.02139,Person Re-Identification,"sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 uses DukeMTMC-reID dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Incremental Learning approach for Person Re-Identification, while Article 2 uses DNS for Person Re-Identification."
1601,1601,61_paper_30,61_paper_32,623,6218,1808.06281,1710.06555,Person Re-Identification,"sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 uses DukeMTMC-reID dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Incremental Learning approach for Person Re-Identification, while Article 2 uses MSCAN for Person Re-Identification."
1602,1602,61_paper_2,61_paper_27,624,625,1703.05693,1708.04896,Person Re-Identification,"sent: Article 1 and Article 2 use DukeMTMC-reID dataset. sent: Article 1 uses Market-1501 dataset, while Article 2 uses multiple datasets:  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses SVDNet approach for Person Re-Identification, while Article 2 uses TriNet   Random Erasing for Person Re-Identification. sent: Article 1 uses SVDNet approach for Person Re-Identification, while Article 2 uses SVDNet   Random Erasing for Person Re-Identification. sent: Article 1 uses SVDNet approach for Person Re-Identification, while Article 2 uses I ORE for Person Re-Identification. sent: Article 1 uses SVDNet approach for Person Re-Identification, while Article 2 uses Random Erasing for Person Re-Identification."
1603,1603,61_paper_2,61_paper_9,624,626,1703.05693,1701.07717,Person Re-Identification,sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.,"sent: Article 1 uses SVDNet approach for Person Re-Identification, while Article 2 uses GAN for Person Re-Identification."
1604,1604,61_paper_2,61_paper_17,624,627,1703.05693,1711.10295,Person Re-Identification,sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.,"sent: Article 1 uses SVDNet approach for Person Re-Identification, while Article 2 uses IDE    CamStyle   Random Erasing for Person Re-Identification. sent: Article 1 uses SVDNet approach for Person Re-Identification, while Article 2 uses IDE    CamStyle for Person Re-Identification. sent: Article 1 uses SVDNet approach for Person Re-Identification, while Article 2 uses IDE  for Person Re-Identification."
1605,1605,61_paper_2,61_paper_1,624,6210,1703.05693,1604.0185,Person Re-Identification,"sent: Article 1 and Article 2 use DukeMTMC-reID dataset. sent: Article 1 uses Market-1501 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SVDNet approach for Person Re-Identification, while Article 2 uses OIM for Person Re-Identification."
1606,1606,61_paper_2,61_paper_28,624,6212,1703.05693,1808.06281,Person Re-Identification,sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.,"sent: Article 1 uses SVDNet approach for Person Re-Identification, while Article 2 uses Incremental Learning for Person Re-Identification."
1607,1607,61_paper_2,61_paper_23,624,6215,1703.05693,1603.02139,Person Re-Identification,"sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 uses DukeMTMC-reID dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SVDNet approach for Person Re-Identification, while Article 2 uses DNS for Person Re-Identification."
1608,1608,61_paper_2,61_paper_32,624,6218,1703.05693,1710.06555,Person Re-Identification,"sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 uses DukeMTMC-reID dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SVDNet approach for Person Re-Identification, while Article 2 uses MSCAN for Person Re-Identification."
1609,1609,61_paper_27,61_paper_9,625,626,1708.04896,1701.07717,Person Re-Identification,"sent: Article 1 and Article 2 use DukeMTMC-reID dataset. sent: Article 1 uses multiple datasets:  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses Market-1501 dataset.","sent: Article 1 uses TriNet   Random Erasing approach for Person Re-Identification, while Article 2 uses GAN for Person Re-Identification. sent: Article 1 uses SVDNet   Random Erasing approach for Person Re-Identification, while Article 2 uses GAN for Person Re-Identification. sent: Article 1 uses I ORE approach for Person Re-Identification, while Article 2 uses GAN for Person Re-Identification. sent: Article 1 uses Random Erasing approach for Person Re-Identification, while Article 2 uses GAN for Person Re-Identification."
1610,1610,61_paper_27,61_paper_17,625,627,1708.04896,1711.10295,Person Re-Identification,"sent: Article 1 and Article 2 use DukeMTMC-reID dataset. sent: Article 1 uses multiple datasets:  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses Market-1501 dataset.","sent: Article 1 uses TriNet   Random Erasing approach for Person Re-Identification, while Article 2 uses IDE    CamStyle   Random Erasing for Person Re-Identification. sent: Article 1 uses TriNet   Random Erasing approach for Person Re-Identification, while Article 2 uses IDE    CamStyle for Person Re-Identification. sent: Article 1 uses TriNet   Random Erasing approach for Person Re-Identification, while Article 2 uses IDE  for Person Re-Identification. sent: Article 1 uses SVDNet   Random Erasing approach for Person Re-Identification, while Article 2 uses IDE    CamStyle   Random Erasing for Person Re-Identification. sent: Article 1 uses SVDNet   Random Erasing approach for Person Re-Identification, while Article 2 uses IDE    CamStyle for Person Re-Identification. sent: Article 1 uses SVDNet   Random Erasing approach for Person Re-Identification, while Article 2 uses IDE  for Person Re-Identification. sent: Article 1 uses I ORE approach for Person Re-Identification, while Article 2 uses IDE    CamStyle   Random Erasing for Person Re-Identification. sent: Article 1 uses I ORE approach for Person Re-Identification, while Article 2 uses IDE    CamStyle for Person Re-Identification. sent: Article 1 uses I ORE approach for Person Re-Identification, while Article 2 uses IDE  for Person Re-Identification. sent: Article 1 uses Random Erasing approach for Person Re-Identification, while Article 2 uses IDE    CamStyle   Random Erasing for Person Re-Identification. sent: Article 1 uses Random Erasing approach for Person Re-Identification, while Article 2 uses IDE    CamStyle for Person Re-Identification. sent: Article 1 uses Random Erasing approach for Person Re-Identification, while Article 2 uses IDE  for Person Re-Identification."
1611,1611,61_paper_27,61_paper_1,625,6210,1708.04896,1604.0185,Person Re-Identification,"sent: Article 1 and Article 2 use DukeMTMC-reID dataset. sent: Article 1 uses multiple datasets:  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses TriNet   Random Erasing approach for Person Re-Identification, while Article 2 uses OIM for Person Re-Identification. sent: Article 1 uses SVDNet   Random Erasing approach for Person Re-Identification, while Article 2 uses OIM for Person Re-Identification. sent: Article 1 uses I ORE approach for Person Re-Identification, while Article 2 uses OIM for Person Re-Identification. sent: Article 1 uses Random Erasing approach for Person Re-Identification, while Article 2 uses OIM for Person Re-Identification."
1612,1612,61_paper_27,61_paper_28,625,6212,1708.04896,1808.06281,Person Re-Identification,"sent: Article 1 and Article 2 use DukeMTMC-reID dataset. sent: Article 1 uses multiple datasets:  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses Market-1501 dataset.","sent: Article 1 uses TriNet   Random Erasing approach for Person Re-Identification, while Article 2 uses Incremental Learning for Person Re-Identification. sent: Article 1 uses SVDNet   Random Erasing approach for Person Re-Identification, while Article 2 uses Incremental Learning for Person Re-Identification. sent: Article 1 uses I ORE approach for Person Re-Identification, while Article 2 uses Incremental Learning for Person Re-Identification. sent: Article 1 uses Random Erasing approach for Person Re-Identification, while Article 2 uses Incremental Learning for Person Re-Identification."
1613,1613,61_paper_27,61_paper_4,625,6213,1708.04896,1703.05693,Person Re-Identification,"sent: Article 1 and Article 2 use DukeMTMC-reID dataset. sent: Article 1 uses multiple datasets:  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses Market-1501 dataset.","sent: Article 1 uses TriNet   Random Erasing approach for Person Re-Identification, while Article 2 uses SVDNet for Person Re-Identification. sent: Article 1 uses SVDNet   Random Erasing approach for Person Re-Identification, while Article 2 uses SVDNet for Person Re-Identification. sent: Article 1 uses I ORE approach for Person Re-Identification, while Article 2 uses SVDNet for Person Re-Identification. sent: Article 1 uses Random Erasing approach for Person Re-Identification, while Article 2 uses SVDNet for Person Re-Identification."
1614,1614,61_paper_27,61_paper_23,625,6215,1708.04896,1603.02139,Person Re-Identification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses Market-1501 dataset.","sent: Article 1 uses TriNet   Random Erasing approach for Person Re-Identification, while Article 2 uses DNS for Person Re-Identification. sent: Article 1 uses SVDNet   Random Erasing approach for Person Re-Identification, while Article 2 uses DNS for Person Re-Identification. sent: Article 1 uses I ORE approach for Person Re-Identification, while Article 2 uses DNS for Person Re-Identification. sent: Article 1 uses Random Erasing approach for Person Re-Identification, while Article 2 uses DNS for Person Re-Identification."
1615,1615,61_paper_27,61_paper_32,625,6218,1708.04896,1710.06555,Person Re-Identification,"sent: Article 1 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST, while Article 2 uses Market-1501 dataset.","sent: Article 1 uses TriNet   Random Erasing approach for Person Re-Identification, while Article 2 uses MSCAN for Person Re-Identification. sent: Article 1 uses SVDNet   Random Erasing approach for Person Re-Identification, while Article 2 uses MSCAN for Person Re-Identification. sent: Article 1 uses I ORE approach for Person Re-Identification, while Article 2 uses MSCAN for Person Re-Identification. sent: Article 1 uses Random Erasing approach for Person Re-Identification, while Article 2 uses MSCAN for Person Re-Identification."
1616,1616,61_paper_9,61_paper_17,626,627,1701.07717,1711.10295,Person Re-Identification,sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.,"sent: Article 1 uses GAN approach for Person Re-Identification, while Article 2 uses IDE    CamStyle   Random Erasing for Person Re-Identification. sent: Article 1 uses GAN approach for Person Re-Identification, while Article 2 uses IDE    CamStyle for Person Re-Identification. sent: Article 1 uses GAN approach for Person Re-Identification, while Article 2 uses IDE  for Person Re-Identification."
1617,1617,61_paper_9,61_paper_1,626,6210,1701.07717,1604.0185,Person Re-Identification,"sent: Article 1 and Article 2 use DukeMTMC-reID dataset. sent: Article 1 uses Market-1501 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses GAN approach for Person Re-Identification, while Article 2 uses OIM for Person Re-Identification."
1618,1618,61_paper_9,61_paper_28,626,6212,1701.07717,1808.06281,Person Re-Identification,sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.,"sent: Article 1 uses GAN approach for Person Re-Identification, while Article 2 uses Incremental Learning for Person Re-Identification."
1619,1619,61_paper_9,61_paper_4,626,6213,1701.07717,1703.05693,Person Re-Identification,sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.,"sent: Article 1 uses GAN approach for Person Re-Identification, while Article 2 uses SVDNet for Person Re-Identification."
1620,1620,61_paper_9,61_paper_23,626,6215,1701.07717,1603.02139,Person Re-Identification,"sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 uses DukeMTMC-reID dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses GAN approach for Person Re-Identification, while Article 2 uses DNS for Person Re-Identification."
1621,1621,61_paper_9,61_paper_26,626,6217,1701.07717,1708.04896,Person Re-Identification,"sent: Article 1 and Article 2 use DukeMTMC-reID dataset. sent: Article 1 uses Market-1501 dataset, while Article 2 uses multiple datasets:  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses GAN approach for Person Re-Identification, while Article 2 uses TriNet   Random Erasing for Person Re-Identification. sent: Article 1 uses GAN approach for Person Re-Identification, while Article 2 uses SVDNet   Random Erasing for Person Re-Identification. sent: Article 1 uses GAN approach for Person Re-Identification, while Article 2 uses I ORE for Person Re-Identification. sent: Article 1 uses GAN approach for Person Re-Identification, while Article 2 uses Random Erasing for Person Re-Identification."
1622,1622,61_paper_9,61_paper_32,626,6218,1701.07717,1710.06555,Person Re-Identification,"sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 uses DukeMTMC-reID dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses GAN approach for Person Re-Identification, while Article 2 uses MSCAN for Person Re-Identification."
1623,1623,61_paper_1,61_paper_8,6210,6211,1604.0185,1701.07717,Person Re-Identification,"sent: Article 1 and Article 2 use DukeMTMC-reID dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Market-1501 dataset.","sent: Article 1 uses OIM approach for Person Re-Identification, while Article 2 uses GAN for Person Re-Identification."
1624,1624,61_paper_1,61_paper_28,6210,6212,1604.0185,1808.06281,Person Re-Identification,"sent: Article 1 and Article 2 use DukeMTMC-reID dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Market-1501 dataset.","sent: Article 1 uses OIM approach for Person Re-Identification, while Article 2 uses Incremental Learning for Person Re-Identification."
1625,1625,61_paper_1,61_paper_4,6210,6213,1604.0185,1703.05693,Person Re-Identification,"sent: Article 1 and Article 2 use DukeMTMC-reID dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Market-1501 dataset.","sent: Article 1 uses OIM approach for Person Re-Identification, while Article 2 uses SVDNet for Person Re-Identification."
1626,1626,61_paper_1,61_paper_20,6210,6214,1604.0185,1711.10295,Person Re-Identification,"sent: Article 1 and Article 2 use DukeMTMC-reID dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Market-1501 dataset.","sent: Article 1 uses OIM approach for Person Re-Identification, while Article 2 uses IDE    CamStyle   Random Erasing for Person Re-Identification. sent: Article 1 uses OIM approach for Person Re-Identification, while Article 2 uses IDE    CamStyle for Person Re-Identification. sent: Article 1 uses OIM approach for Person Re-Identification, while Article 2 uses IDE  for Person Re-Identification."
1627,1627,61_paper_1,61_paper_23,6210,6215,1604.0185,1603.02139,Person Re-Identification,"sent: Article 1 uses DukeMTMC-reID dataset, while Article 2 uses Market-1501 dataset.","sent: Article 1 uses OIM approach for Person Re-Identification, while Article 2 uses DNS for Person Re-Identification."
1628,1628,61_paper_1,61_paper_26,6210,6217,1604.0185,1708.04896,Person Re-Identification,"sent: Article 1 and Article 2 use DukeMTMC-reID dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses OIM approach for Person Re-Identification, while Article 2 uses TriNet   Random Erasing for Person Re-Identification. sent: Article 1 uses OIM approach for Person Re-Identification, while Article 2 uses SVDNet   Random Erasing for Person Re-Identification. sent: Article 1 uses OIM approach for Person Re-Identification, while Article 2 uses I ORE for Person Re-Identification. sent: Article 1 uses OIM approach for Person Re-Identification, while Article 2 uses Random Erasing for Person Re-Identification."
1629,1629,61_paper_1,61_paper_32,6210,6218,1604.0185,1710.06555,Person Re-Identification,"sent: Article 1 uses DukeMTMC-reID dataset, while Article 2 uses Market-1501 dataset.","sent: Article 1 uses OIM approach for Person Re-Identification, while Article 2 uses MSCAN for Person Re-Identification."
1630,1630,61_paper_23,61_paper_13,6215,6216,1603.02139,1711.10295,Person Re-Identification,"sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses DukeMTMC-reID dataset.","sent: Article 1 uses DNS approach for Person Re-Identification, while Article 2 uses IDE    CamStyle   Random Erasing for Person Re-Identification. sent: Article 1 uses DNS approach for Person Re-Identification, while Article 2 uses IDE    CamStyle for Person Re-Identification. sent: Article 1 uses DNS approach for Person Re-Identification, while Article 2 uses IDE  for Person Re-Identification."
1631,1631,61_paper_23,61_paper_26,6215,6217,1603.02139,1708.04896,Person Re-Identification,"sent: Article 1 uses Market-1501 dataset, while Article 2 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses DNS approach for Person Re-Identification, while Article 2 uses TriNet   Random Erasing for Person Re-Identification. sent: Article 1 uses DNS approach for Person Re-Identification, while Article 2 uses SVDNet   Random Erasing for Person Re-Identification. sent: Article 1 uses DNS approach for Person Re-Identification, while Article 2 uses I ORE for Person Re-Identification. sent: Article 1 uses DNS approach for Person Re-Identification, while Article 2 uses Random Erasing for Person Re-Identification."
1632,1632,61_paper_23,61_paper_32,6215,6218,1603.02139,1710.06555,Person Re-Identification,sent: Article 1 and Article 2 use Market-1501 dataset.,"sent: Article 1 uses DNS approach for Person Re-Identification, while Article 2 uses MSCAN for Person Re-Identification."
1633,1633,61_paper_23,61_paper_5,6215,6219,1603.02139,1703.05693,Person Re-Identification,"sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses DukeMTMC-reID dataset.","sent: Article 1 uses DNS approach for Person Re-Identification, while Article 2 uses SVDNet for Person Re-Identification."
1634,1634,61_paper_23,61_paper_0,6215,6223,1603.02139,1604.0185,Person Re-Identification,"sent: Article 1 uses Market-1501 dataset, while Article 2 uses DukeMTMC-reID dataset.","sent: Article 1 uses DNS approach for Person Re-Identification, while Article 2 uses OIM for Person Re-Identification."
1635,1635,61_paper_23,61_paper_29,6215,6225,1603.02139,1808.06281,Person Re-Identification,"sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses DukeMTMC-reID dataset.","sent: Article 1 uses DNS approach for Person Re-Identification, while Article 2 uses Incremental Learning for Person Re-Identification."
1636,1636,61_paper_32,61_paper_5,6218,6219,1710.06555,1703.05693,Person Re-Identification,"sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses DukeMTMC-reID dataset.","sent: Article 1 uses MSCAN approach for Person Re-Identification, while Article 2 uses SVDNet for Person Re-Identification."
1637,1637,61_paper_32,61_paper_16,6218,6220,1710.06555,1711.10295,Person Re-Identification,"sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses DukeMTMC-reID dataset.","sent: Article 1 uses MSCAN approach for Person Re-Identification, while Article 2 uses IDE    CamStyle   Random Erasing for Person Re-Identification. sent: Article 1 uses MSCAN approach for Person Re-Identification, while Article 2 uses IDE    CamStyle for Person Re-Identification. sent: Article 1 uses MSCAN approach for Person Re-Identification, while Article 2 uses IDE  for Person Re-Identification."
1638,1638,61_paper_32,61_paper_0,6218,6223,1710.06555,1604.0185,Person Re-Identification,"sent: Article 1 uses Market-1501 dataset, while Article 2 uses DukeMTMC-reID dataset.","sent: Article 1 uses MSCAN approach for Person Re-Identification, while Article 2 uses OIM for Person Re-Identification."
1639,1639,61_paper_32,61_paper_29,6218,6225,1710.06555,1808.06281,Person Re-Identification,"sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses DukeMTMC-reID dataset.","sent: Article 1 uses MSCAN approach for Person Re-Identification, while Article 2 uses Incremental Learning for Person Re-Identification."
1640,1640,61_paper_32,61_paper_22,6218,6229,1710.06555,1603.02139,Person Re-Identification,sent: Article 1 and Article 2 use Market-1501 dataset.,"sent: Article 1 uses MSCAN approach for Person Re-Identification, while Article 2 uses DNS for Person Re-Identification."
1641,1641,61_paper_32,61_paper_24,6218,6230,1710.06555,1708.04896,Person Re-Identification,"sent: Article 1 uses Market-1501 dataset, while Article 2 uses multiple datasets:  DukeMTMC-reID,  PASCAL VOC 2007, and  Fashion-MNIST.","sent: Article 1 uses MSCAN approach for Person Re-Identification, while Article 2 uses TriNet   Random Erasing for Person Re-Identification. sent: Article 1 uses MSCAN approach for Person Re-Identification, while Article 2 uses SVDNet   Random Erasing for Person Re-Identification. sent: Article 1 uses MSCAN approach for Person Re-Identification, while Article 2 uses I ORE for Person Re-Identification. sent: Article 1 uses MSCAN approach for Person Re-Identification, while Article 2 uses Random Erasing for Person Re-Identification."
1642,1642,62_paper_0,62_paper_1,630,631,1704.01212,1511.05493,Drug Discovery,"sent: Article 1 and Article 2 use QM9 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses WikiSQL dataset.","sent: Article 1 uses MPNNs approach for Drug Discovery, while Article 2 uses Gated Graph Sequence NN for Drug Discovery. sent: Article 1 uses MPNNs approach for Drug Discovery, while Article 2 uses GGS-NN for Drug Discovery."
1643,1643,63_paper_9,63_paper_2,640,641,1611.0085,1612.01925,Dense Pixel Correspondence Estimation,"sent: Article 1 and Article 2 use HPatches dataset. sent: Article 1 uses multiple datasets:  Sintel-final, and  Sintel-clean, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SPyNet approach for Dense Pixel Correspondence Estimation, while Article 2 uses FlowNet2 for Dense Pixel Correspondence Estimation. sent: Article 1 uses Spynet approach for Dense Pixel Correspondence Estimation, while Article 2 uses FlowNet2 for Dense Pixel Correspondence Estimation."
1644,1644,63_paper_2,63_paper_8,641,642,1612.01925,1611.0085,Dense Pixel Correspondence Estimation,"sent: Article 1 and Article 2 use HPatches dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Sintel-final, and  Sintel-clean.","sent: Article 1 uses FlowNet2 approach for Dense Pixel Correspondence Estimation, while Article 2 uses SPyNet for Dense Pixel Correspondence Estimation. sent: Article 1 uses FlowNet2 approach for Dense Pixel Correspondence Estimation, while Article 2 uses Spynet for Dense Pixel Correspondence Estimation."
1645,1645,64_paper_3,64_paper_0,650,658,1802.05814,1408.1717,Collaborative Filtering,"sent: Article 1 uses multiple datasets:  Million Song Dataset,  Netflix, and  MovieLens 20M, while Article 2 uses MovieLens 100K dataset.","sent: Article 1 uses Mult-DAE approach for Collaborative Filtering, while Article 2 uses GMC for Collaborative Filtering. sent: Article 1 uses Mult-VAE PR approach for Collaborative Filtering, while Article 2 uses GMC for Collaborative Filtering."
1646,1646,64_paper_0,64_paper_5,658,659,1408.1717,1802.05814,Collaborative Filtering,"sent: Article 1 uses MovieLens 100K dataset, while Article 2 uses multiple datasets:  Million Song Dataset,  Netflix, and  MovieLens 20M.","sent: Article 1 uses GMC approach for Collaborative Filtering, while Article 2 uses Mult-DAE for Collaborative Filtering. sent: Article 1 uses GMC approach for Collaborative Filtering, while Article 2 uses Mult-VAE PR for Collaborative Filtering."
1647,1647,67_paper_0,67_paper_2,680,681,1805.04855,1701.01879,Facial Expression Recognition,"sent: Article 1 uses multiple datasets:   Static Facial Expressions in the Wild, and  Real-World Affective Faces, while Article 2 uses Cohn-Kanade dataset.","sent: Article 1 uses Covariance Pooling approach for Facial Expression Recognition, while Article 2 uses Sequential forward selection for Facial Expression Recognition."
1648,1648,67_paper_0,67_paper_3,680,682,1805.04855,1509.05371,Facial Expression Recognition,"sent: Article 1 uses multiple datasets:   Static Facial Expressions in the Wild, and  Real-World Affective Faces, while Article 2 uses MMI dataset.","sent: Article 1 uses Covariance Pooling approach for Facial Expression Recognition, while Article 2 uses DeXpression for Facial Expression Recognition."
1649,1649,67_paper_2,67_paper_3,681,682,1701.01879,1509.05371,Facial Expression Recognition,"sent: Article 1 uses Cohn-Kanade dataset, while Article 2 uses MMI dataset.","sent: Article 1 uses Sequential forward selection approach for Facial Expression Recognition, while Article 2 uses DeXpression for Facial Expression Recognition."
1650,1650,67_paper_2,67_paper_1,681,683,1701.01879,1805.04855,Facial Expression Recognition,"sent: Article 1 uses Cohn-Kanade dataset, while Article 2 uses multiple datasets:   Static Facial Expressions in the Wild, and  Real-World Affective Faces.","sent: Article 1 uses Sequential forward selection approach for Facial Expression Recognition, while Article 2 uses Covariance Pooling for Facial Expression Recognition."
1651,1651,67_paper_3,67_paper_1,682,683,1509.05371,1805.04855,Facial Expression Recognition,"sent: Article 1 uses MMI dataset, while Article 2 uses multiple datasets:   Static Facial Expressions in the Wild, and  Real-World Affective Faces.","sent: Article 1 uses DeXpression approach for Facial Expression Recognition, while Article 2 uses Covariance Pooling for Facial Expression Recognition."
1652,1652,68_paper_3,68_paper_0,690,691,1511.06645,1807.04067,Multi-Person Pose Estimation,"sent: Article 1 uses multiple datasets:  MPII Human Pose, and  WAF, while Article 2 uses COCO dataset.","sent: Article 1 uses DeepCut approach for Multi-Person Pose Estimation, while Article 2 uses Pose Residual Network for Multi-Person Pose Estimation."
1653,1653,68_paper_3,68_paper_2,690,692,1511.06645,1705.07422,Multi-Person Pose Estimation,"sent: Article 1 and Article 2 use WAF dataset. sent: Article 1 uses MPII Human Pose dataset, while Article 2 uses MPII Multi-Person dataset.","sent: Article 1 uses DeepCut approach for Multi-Person Pose Estimation, while Article 2 uses Generative Partition Networks for Multi-Person Pose Estimation."
1654,1654,68_paper_0,68_paper_2,691,692,1807.04067,1705.07422,Multi-Person Pose Estimation,"sent: Article 1 uses COCO dataset, while Article 2 uses multiple datasets:  MPII Multi-Person, and  WAF.","sent: Article 1 uses Pose Residual Network approach for Multi-Person Pose Estimation, while Article 2 uses Generative Partition Networks for Multi-Person Pose Estimation."
1655,1655,69_paper_0,69_paper_1,700,701,1807.04067,1703.0687,Keypoint Detection,"sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MHP v1 0, and  MHP v2 0.","sent: Article 1 uses Pose Residual Network approach for Keypoint Detection, while Article 2 uses Mask R-CNN for Keypoint Detection."
1656,1656,69_paper_0,69_paper_3,700,703,1807.04067,1411.1091,Keypoint Detection,"sent: Article 1 uses COCO dataset, while Article 2 uses  Pascal3D  dataset.","sent: Article 1 uses Pose Residual Network approach for Keypoint Detection, while Article 2 uses ConvNet for Keypoint Detection."
1657,1657,69_paper_1,69_paper_3,701,703,1703.0687,1411.1091,Keypoint Detection,"sent: Article 1 uses multiple datasets:  COCO,  MHP v1 0, and  MHP v2 0, while Article 2 uses  Pascal3D  dataset.","sent: Article 1 uses Mask R-CNN approach for Keypoint Detection, while Article 2 uses ConvNet for Keypoint Detection."
1658,1658,70_paper_0,70_paper_4,710,711,1603.06042,1603.03793,Dependency Parsing,sent: Article 1 and Article 2 use Penn Treebank dataset.,"sent: Article 1 uses Andor et al  approach for Dependency Parsing, while Article 2 uses Arc-hybrid for Dependency Parsing."
1659,1659,70_paper_4,70_paper_2,711,713,1603.03793,1603.06042,Dependency Parsing,sent: Article 1 and Article 2 use Penn Treebank dataset.,"sent: Article 1 uses Arc-hybrid approach for Dependency Parsing, while Article 2 uses Andor et al  for Dependency Parsing."
1660,1660,71_paper_2,71_paper_5,720,721,1604.02426,1902.05509,Image Retrieval,"sent: Article 1 uses multiple datasets:  Oxf5k,  Par106k,  Oxf105k, and  Par6k, while Article 2 uses multiple datasets:  ImageNet, and  INRIA Holidays.","sent: Article 1 uses siaMAC QE  approach for Image Retrieval, while Article 2 uses MultiGrain R50   500 for Image Retrieval. sent: Article 1 uses siaMAC QE  approach for Image Retrieval, while Article 2 uses MultiGrain R50-AA-500 for Image Retrieval. sent: Article 1 uses siaMAC QE  approach for Image Retrieval, while Article 2 uses MultiGrain R50   800 for Image Retrieval. sent: Article 1 uses siaMAC QE  approach for Image Retrieval, while Article 2 uses PNASNet-5-Large   MultiGrain p    500 for Image Retrieval. sent: Article 1 uses siaMAC QE  approach for Image Retrieval, while Article 2 uses SENet154   MultiGrain p    450 for Image Retrieval. sent: Article 1 uses siaMAC QE  approach for Image Retrieval, while Article 2 uses MultiGrain R50-AA-224 for Image Retrieval."
1661,1661,71_paper_5,71_paper_1,721,722,1902.05509,1604.02426,Image Retrieval,"sent: Article 1 uses multiple datasets:  ImageNet, and  INRIA Holidays, while Article 2 uses multiple datasets:  Oxf5k,  Par106k,  Oxf105k, and  Par6k.","sent: Article 1 uses MultiGrain R50   500 approach for Image Retrieval, while Article 2 uses siaMAC QE  for Image Retrieval. sent: Article 1 uses MultiGrain R50-AA-500 approach for Image Retrieval, while Article 2 uses siaMAC QE  for Image Retrieval. sent: Article 1 uses MultiGrain R50   800 approach for Image Retrieval, while Article 2 uses siaMAC QE  for Image Retrieval. sent: Article 1 uses PNASNet-5-Large   MultiGrain p    500 approach for Image Retrieval, while Article 2 uses siaMAC QE  for Image Retrieval. sent: Article 1 uses SENet154   MultiGrain p    450 approach for Image Retrieval, while Article 2 uses siaMAC QE  for Image Retrieval. sent: Article 1 uses MultiGrain R50-AA-224 approach for Image Retrieval, while Article 2 uses siaMAC QE  for Image Retrieval."
1662,1662,73_paper_10,73_paper_2,740,741,1712.01034,1506.08959,Fine-Grained Image Classification,"sent: Article 1 uses multiple datasets:  Stanford Cars,  FGVC Aircraft, and   CUB-200-2011, while Article 2 uses CompCars dataset.","sent: Article 1 uses MPN-COV approach for Fine-Grained Image Classification, while Article 2 uses AlexNet for Fine-Grained Image Classification. sent: Article 1 uses MPN-COV approach for Fine-Grained Image Classification, while Article 2 uses GoogLeNet for Fine-Grained Image Classification."
1663,1663,73_paper_10,73_paper_0,740,742,1712.01034,1407.3867,Fine-Grained Image Classification,"sent: Article 1 and Article 2 use  CUB-200-2011 dataset. sent: Article 1 uses multiple datasets:  Stanford Cars, and  FGVC Aircraft, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses MPN-COV approach for Fine-Grained Image Classification, while Article 2 uses Part RCNN for Fine-Grained Image Classification."
1664,1664,73_paper_2,73_paper_0,741,742,1506.08959,1407.3867,Fine-Grained Image Classification,"sent: Article 1 uses CompCars dataset, while Article 2 uses  CUB-200-2011 dataset.","sent: Article 1 uses AlexNet approach for Fine-Grained Image Classification, while Article 2 uses Part RCNN for Fine-Grained Image Classification. sent: Article 1 uses GoogLeNet approach for Fine-Grained Image Classification, while Article 2 uses Part RCNN for Fine-Grained Image Classification."
1665,1665,73_paper_2,73_paper_9,741,743,1506.08959,1712.01034,Fine-Grained Image Classification,"sent: Article 1 uses CompCars dataset, while Article 2 uses multiple datasets:  Stanford Cars,  FGVC Aircraft, and   CUB-200-2011.","sent: Article 1 uses AlexNet approach for Fine-Grained Image Classification, while Article 2 uses MPN-COV for Fine-Grained Image Classification. sent: Article 1 uses GoogLeNet approach for Fine-Grained Image Classification, while Article 2 uses MPN-COV for Fine-Grained Image Classification."
1666,1666,73_paper_0,73_paper_9,742,743,1407.3867,1712.01034,Fine-Grained Image Classification,"sent: Article 1 and Article 2 use  CUB-200-2011 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Stanford Cars, and  FGVC Aircraft.","sent: Article 1 uses Part RCNN approach for Fine-Grained Image Classification, while Article 2 uses MPN-COV for Fine-Grained Image Classification."
1667,1667,73_paper_0,73_paper_1,742,745,1407.3867,1506.08959,Fine-Grained Image Classification,"sent: Article 1 uses  CUB-200-2011 dataset, while Article 2 uses CompCars dataset.","sent: Article 1 uses Part RCNN approach for Fine-Grained Image Classification, while Article 2 uses AlexNet for Fine-Grained Image Classification. sent: Article 1 uses Part RCNN approach for Fine-Grained Image Classification, while Article 2 uses GoogLeNet for Fine-Grained Image Classification."
1668,1668,76_paper_11,76_paper_20,770,771,1706.06978,1611.00144,Click-Through Rate Prediction,"sent: Article 1 and Article 2 use Amazon dataset.sent: Article 1 and Article 2 use MovieLens 20M dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Company ,  Dianping,  iPinYou,  Criteo, and  Bing News.","sent: Article 1 uses DIN approach for Click-Through Rate Prediction, while Article 2 uses IPNN for Click-Through Rate Prediction. sent: Article 1 uses DIN approach for Click-Through Rate Prediction, while Article 2 uses PNN for Click-Through Rate Prediction. sent: Article 1 uses DIN approach for Click-Through Rate Prediction, while Article 2 uses PNN  for Click-Through Rate Prediction. sent: Article 1 uses DIN approach for Click-Through Rate Prediction, while Article 2 uses OPNN for Click-Through Rate Prediction. sent: Article 1 uses DIN   Dice Activation approach for Click-Through Rate Prediction, while Article 2 uses IPNN for Click-Through Rate Prediction. sent: Article 1 uses DIN   Dice Activation approach for Click-Through Rate Prediction, while Article 2 uses PNN for Click-Through Rate Prediction. sent: Article 1 uses DIN   Dice Activation approach for Click-Through Rate Prediction, while Article 2 uses PNN  for Click-Through Rate Prediction. sent: Article 1 uses DIN   Dice Activation approach for Click-Through Rate Prediction, while Article 2 uses OPNN for Click-Through Rate Prediction."
1669,1669,76_paper_11,76_paper_18,770,773,1706.06978,1601.02376,Click-Through Rate Prediction,"sent: Article 1 uses multiple datasets:  Amazon, and  MovieLens 20M, while Article 2 uses multiple datasets:  Company ,  iPinYou, and  Criteo.","sent: Article 1 uses DIN approach for Click-Through Rate Prediction, while Article 2 uses FNN for Click-Through Rate Prediction. sent: Article 1 uses DIN   Dice Activation approach for Click-Through Rate Prediction, while Article 2 uses FNN for Click-Through Rate Prediction."
1670,1670,76_paper_11,76_paper_1,770,774,1706.06978,1703.04247,Click-Through Rate Prediction,"sent: Article 1 and Article 2 use Amazon dataset.sent: Article 1 and Article 2 use MovieLens 20M dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Company ,  Bing News,  Dianping, and  Criteo.","sent: Article 1 uses DIN approach for Click-Through Rate Prediction, while Article 2 uses DeepFM for Click-Through Rate Prediction. sent: Article 1 uses DIN   Dice Activation approach for Click-Through Rate Prediction, while Article 2 uses DeepFM for Click-Through Rate Prediction."
1671,1671,76_paper_20,76_paper_18,771,773,1611.00144,1601.02376,Click-Through Rate Prediction,"sent: Article 1 and Article 2 use Company  dataset.sent: Article 1 and Article 2 use iPinYou dataset.sent: Article 1 and Article 2 use Criteo dataset. sent: Article 1 uses multiple datasets:  Amazon,  Bing News,  Dianping, and  MovieLens 20M, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses IPNN approach for Click-Through Rate Prediction, while Article 2 uses FNN for Click-Through Rate Prediction. sent: Article 1 uses PNN approach for Click-Through Rate Prediction, while Article 2 uses FNN for Click-Through Rate Prediction. sent: Article 1 uses PNN  approach for Click-Through Rate Prediction, while Article 2 uses FNN for Click-Through Rate Prediction. sent: Article 1 uses OPNN approach for Click-Through Rate Prediction, while Article 2 uses FNN for Click-Through Rate Prediction."
1672,1672,76_paper_20,76_paper_1,771,774,1611.00144,1703.04247,Click-Through Rate Prediction,"sent: Article 1 and Article 2 use Company  dataset.sent: Article 1 and Article 2 use Dianping dataset.sent: Article 1 and Article 2 use Criteo dataset.sent: Article 1 and Article 2 use Bing News dataset.sent: Article 1 and Article 2 use Amazon dataset.sent: Article 1 and Article 2 use MovieLens 20M dataset. sent: Article 1 uses iPinYou dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses IPNN approach for Click-Through Rate Prediction, while Article 2 uses DeepFM for Click-Through Rate Prediction. sent: Article 1 uses PNN approach for Click-Through Rate Prediction, while Article 2 uses DeepFM for Click-Through Rate Prediction. sent: Article 1 uses PNN  approach for Click-Through Rate Prediction, while Article 2 uses DeepFM for Click-Through Rate Prediction. sent: Article 1 uses OPNN approach for Click-Through Rate Prediction, while Article 2 uses DeepFM for Click-Through Rate Prediction."
1673,1673,76_paper_20,76_paper_10,771,7711,1611.00144,1706.06978,Click-Through Rate Prediction,"sent: Article 1 and Article 2 use Amazon dataset.sent: Article 1 and Article 2 use MovieLens 20M dataset. sent: Article 1 uses multiple datasets:  Company ,  Dianping,  iPinYou,  Criteo, and  Bing News, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses IPNN approach for Click-Through Rate Prediction, while Article 2 uses DIN for Click-Through Rate Prediction. sent: Article 1 uses IPNN approach for Click-Through Rate Prediction, while Article 2 uses DIN   Dice Activation for Click-Through Rate Prediction. sent: Article 1 uses PNN approach for Click-Through Rate Prediction, while Article 2 uses DIN for Click-Through Rate Prediction. sent: Article 1 uses PNN approach for Click-Through Rate Prediction, while Article 2 uses DIN   Dice Activation for Click-Through Rate Prediction. sent: Article 1 uses PNN  approach for Click-Through Rate Prediction, while Article 2 uses DIN for Click-Through Rate Prediction. sent: Article 1 uses PNN  approach for Click-Through Rate Prediction, while Article 2 uses DIN   Dice Activation for Click-Through Rate Prediction. sent: Article 1 uses OPNN approach for Click-Through Rate Prediction, while Article 2 uses DIN for Click-Through Rate Prediction. sent: Article 1 uses OPNN approach for Click-Through Rate Prediction, while Article 2 uses DIN   Dice Activation for Click-Through Rate Prediction."
1674,1674,76_paper_18,76_paper_1,773,774,1601.02376,1703.04247,Click-Through Rate Prediction,"sent: Article 1 and Article 2 use Company  dataset.sent: Article 1 and Article 2 use Criteo dataset. sent: Article 1 uses iPinYou dataset, while Article 2 uses multiple datasets:  Amazon,  Bing News,  Dianping, and  MovieLens 20M.","sent: Article 1 uses FNN approach for Click-Through Rate Prediction, while Article 2 uses DeepFM for Click-Through Rate Prediction."
1675,1675,76_paper_18,76_paper_31,773,775,1601.02376,1611.00144,Click-Through Rate Prediction,"sent: Article 1 and Article 2 use Company  dataset.sent: Article 1 and Article 2 use iPinYou dataset.sent: Article 1 and Article 2 use Criteo dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Amazon,  Bing News,  Dianping, and  MovieLens 20M.","sent: Article 1 uses FNN approach for Click-Through Rate Prediction, while Article 2 uses IPNN for Click-Through Rate Prediction. sent: Article 1 uses FNN approach for Click-Through Rate Prediction, while Article 2 uses PNN for Click-Through Rate Prediction. sent: Article 1 uses FNN approach for Click-Through Rate Prediction, while Article 2 uses PNN  for Click-Through Rate Prediction. sent: Article 1 uses FNN approach for Click-Through Rate Prediction, while Article 2 uses OPNN for Click-Through Rate Prediction."
1676,1676,76_paper_18,76_paper_10,773,7711,1601.02376,1706.06978,Click-Through Rate Prediction,"sent: Article 1 uses multiple datasets:  Company ,  iPinYou, and  Criteo, while Article 2 uses multiple datasets:  Amazon, and  MovieLens 20M.","sent: Article 1 uses FNN approach for Click-Through Rate Prediction, while Article 2 uses DIN for Click-Through Rate Prediction. sent: Article 1 uses FNN approach for Click-Through Rate Prediction, while Article 2 uses DIN   Dice Activation for Click-Through Rate Prediction."
1677,1677,76_paper_1,76_paper_31,774,775,1703.04247,1611.00144,Click-Through Rate Prediction,"sent: Article 1 and Article 2 use Company  dataset.sent: Article 1 and Article 2 use Dianping dataset.sent: Article 1 and Article 2 use Criteo dataset.sent: Article 1 and Article 2 use Bing News dataset.sent: Article 1 and Article 2 use Amazon dataset.sent: Article 1 and Article 2 use MovieLens 20M dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses iPinYou dataset.","sent: Article 1 uses DeepFM approach for Click-Through Rate Prediction, while Article 2 uses IPNN for Click-Through Rate Prediction. sent: Article 1 uses DeepFM approach for Click-Through Rate Prediction, while Article 2 uses PNN for Click-Through Rate Prediction. sent: Article 1 uses DeepFM approach for Click-Through Rate Prediction, while Article 2 uses PNN  for Click-Through Rate Prediction. sent: Article 1 uses DeepFM approach for Click-Through Rate Prediction, while Article 2 uses OPNN for Click-Through Rate Prediction."
1678,1678,76_paper_1,76_paper_10,774,7711,1703.04247,1706.06978,Click-Through Rate Prediction,"sent: Article 1 and Article 2 use Amazon dataset.sent: Article 1 and Article 2 use MovieLens 20M dataset. sent: Article 1 uses multiple datasets:  Company ,  Bing News,  Dianping, and  Criteo, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DeepFM approach for Click-Through Rate Prediction, while Article 2 uses DIN for Click-Through Rate Prediction. sent: Article 1 uses DeepFM approach for Click-Through Rate Prediction, while Article 2 uses DIN   Dice Activation for Click-Through Rate Prediction."
1679,1679,76_paper_1,76_paper_15,774,7720,1703.04247,1601.02376,Click-Through Rate Prediction,"sent: Article 1 and Article 2 use Company  dataset.sent: Article 1 and Article 2 use Criteo dataset. sent: Article 1 uses multiple datasets:  Amazon,  Bing News,  Dianping, and  MovieLens 20M, while Article 2 uses iPinYou dataset.","sent: Article 1 uses DeepFM approach for Click-Through Rate Prediction, while Article 2 uses FNN for Click-Through Rate Prediction."
1680,1680,79_paper_1,79_paper_9,800,801,1610.05256,1509.08967,Speech Recognition,sent: Article 1 and Article 2 use Switchboard   Hub500 dataset.,"sent: Article 1 uses CNN-LSTM approach for Speech Recognition, while Article 2 uses Deep CNN  10 conv  4 FC layers   multi-scale feature maps for Speech Recognition. sent: Article 1 uses Microsoft 2016b approach for Speech Recognition, while Article 2 uses Deep CNN  10 conv  4 FC layers   multi-scale feature maps for Speech Recognition."
1681,1681,79_paper_1,79_paper_5,800,802,1610.05256,1412.5567,Speech Recognition,"sent: Article 1 and Article 2 use Switchboard   Hub500 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  VoxForge American-Canadian,  VoxForge Commonwealth,  swb hub 500 WER fullSWBCH,  CHiME real,  VoxForge Indian,  VoxForge European, and  CHiME clean.","sent: Article 1 uses CNN-LSTM approach for Speech Recognition, while Article 2 uses CNN   Bi-RNN   CTC  speech to letters  for Speech Recognition. sent: Article 1 uses CNN-LSTM approach for Speech Recognition, while Article 2 uses Deep Speech for Speech Recognition. sent: Article 1 uses CNN-LSTM approach for Speech Recognition, while Article 2 uses CNN   Bi-RNN   CTC  speech to letters   25 9  WER if trainedonlyon SWB for Speech Recognition. sent: Article 1 uses CNN-LSTM approach for Speech Recognition, while Article 2 uses Deep Speech   FSH for Speech Recognition. sent: Article 1 uses Microsoft 2016b approach for Speech Recognition, while Article 2 uses CNN   Bi-RNN   CTC  speech to letters  for Speech Recognition. sent: Article 1 uses Microsoft 2016b approach for Speech Recognition, while Article 2 uses Deep Speech for Speech Recognition. sent: Article 1 uses Microsoft 2016b approach for Speech Recognition, while Article 2 uses CNN   Bi-RNN   CTC  speech to letters   25 9  WER if trainedonlyon SWB for Speech Recognition. sent: Article 1 uses Microsoft 2016b approach for Speech Recognition, while Article 2 uses Deep Speech   FSH for Speech Recognition."
1682,1682,79_paper_1,79_paper_8,800,804,1610.05256,1803.10225,Speech Recognition,"sent: Article 1 uses Switchboard   Hub500 dataset, while Article 2 uses TIMIT dataset.","sent: Article 1 uses CNN-LSTM approach for Speech Recognition, while Article 2 uses Light Gated Recurrent Units for Speech Recognition. sent: Article 1 uses CNN-LSTM approach for Speech Recognition, while Article 2 uses Li-GRU   fMLLR features for Speech Recognition. sent: Article 1 uses Microsoft 2016b approach for Speech Recognition, while Article 2 uses Light Gated Recurrent Units for Speech Recognition. sent: Article 1 uses Microsoft 2016b approach for Speech Recognition, while Article 2 uses Li-GRU   fMLLR features for Speech Recognition."
1683,1683,79_paper_1,79_paper_6,800,805,1610.05256,1506.07503,Speech Recognition,"sent: Article 1 uses Switchboard   Hub500 dataset, while Article 2 uses TIMIT dataset.","sent: Article 1 uses CNN-LSTM approach for Speech Recognition, while Article 2 uses Bi-RNN   Attention for Speech Recognition. sent: Article 1 uses Microsoft 2016b approach for Speech Recognition, while Article 2 uses Bi-RNN   Attention for Speech Recognition."
1684,1684,79_paper_1,79_paper_10,800,808,1610.05256,1806.07789,Speech Recognition,"sent: Article 1 uses Switchboard   Hub500 dataset, while Article 2 uses TIMIT dataset.","sent: Article 1 uses CNN-LSTM approach for Speech Recognition, while Article 2 uses QCNN-10L-256FM for Speech Recognition. sent: Article 1 uses Microsoft 2016b approach for Speech Recognition, while Article 2 uses QCNN-10L-256FM for Speech Recognition."
1685,1685,79_paper_9,79_paper_5,801,802,1509.08967,1412.5567,Speech Recognition,"sent: Article 1 and Article 2 use Switchboard   Hub500 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  VoxForge American-Canadian,  VoxForge Commonwealth,  swb hub 500 WER fullSWBCH,  CHiME real,  VoxForge Indian,  VoxForge European, and  CHiME clean.","sent: Article 1 uses Deep CNN  10 conv  4 FC layers   multi-scale feature maps approach for Speech Recognition, while Article 2 uses CNN   Bi-RNN   CTC  speech to letters  for Speech Recognition. sent: Article 1 uses Deep CNN  10 conv  4 FC layers   multi-scale feature maps approach for Speech Recognition, while Article 2 uses Deep Speech for Speech Recognition. sent: Article 1 uses Deep CNN  10 conv  4 FC layers   multi-scale feature maps approach for Speech Recognition, while Article 2 uses CNN   Bi-RNN   CTC  speech to letters   25 9  WER if trainedonlyon SWB for Speech Recognition. sent: Article 1 uses Deep CNN  10 conv  4 FC layers   multi-scale feature maps approach for Speech Recognition, while Article 2 uses Deep Speech   FSH for Speech Recognition."
1686,1686,79_paper_9,79_paper_8,801,804,1509.08967,1803.10225,Speech Recognition,"sent: Article 1 uses Switchboard   Hub500 dataset, while Article 2 uses TIMIT dataset.","sent: Article 1 uses Deep CNN  10 conv  4 FC layers   multi-scale feature maps approach for Speech Recognition, while Article 2 uses Light Gated Recurrent Units for Speech Recognition. sent: Article 1 uses Deep CNN  10 conv  4 FC layers   multi-scale feature maps approach for Speech Recognition, while Article 2 uses Li-GRU   fMLLR features for Speech Recognition."
1687,1687,79_paper_9,79_paper_6,801,805,1509.08967,1506.07503,Speech Recognition,"sent: Article 1 uses Switchboard   Hub500 dataset, while Article 2 uses TIMIT dataset.","sent: Article 1 uses Deep CNN  10 conv  4 FC layers   multi-scale feature maps approach for Speech Recognition, while Article 2 uses Bi-RNN   Attention for Speech Recognition."
1688,1688,79_paper_9,79_paper_10,801,808,1509.08967,1806.07789,Speech Recognition,"sent: Article 1 uses Switchboard   Hub500 dataset, while Article 2 uses TIMIT dataset.","sent: Article 1 uses Deep CNN  10 conv  4 FC layers   multi-scale feature maps approach for Speech Recognition, while Article 2 uses QCNN-10L-256FM for Speech Recognition."
1689,1689,79_paper_9,79_paper_0,801,8010,1509.08967,1610.05256,Speech Recognition,sent: Article 1 and Article 2 use Switchboard   Hub500 dataset.,"sent: Article 1 uses Deep CNN  10 conv  4 FC layers   multi-scale feature maps approach for Speech Recognition, while Article 2 uses CNN-LSTM for Speech Recognition. sent: Article 1 uses Deep CNN  10 conv  4 FC layers   multi-scale feature maps approach for Speech Recognition, while Article 2 uses Microsoft 2016b for Speech Recognition."
1690,1690,79_paper_5,79_paper_8,802,804,1412.5567,1803.10225,Speech Recognition,"sent: Article 1 uses multiple datasets:  Switchboard   Hub500,  VoxForge American-Canadian,  VoxForge Commonwealth,  swb hub 500 WER fullSWBCH,  CHiME real,  VoxForge Indian,  VoxForge European, and  CHiME clean, while Article 2 uses TIMIT dataset.","sent: Article 1 uses CNN   Bi-RNN   CTC  speech to letters  approach for Speech Recognition, while Article 2 uses Light Gated Recurrent Units for Speech Recognition. sent: Article 1 uses CNN   Bi-RNN   CTC  speech to letters  approach for Speech Recognition, while Article 2 uses Li-GRU   fMLLR features for Speech Recognition. sent: Article 1 uses Deep Speech approach for Speech Recognition, while Article 2 uses Light Gated Recurrent Units for Speech Recognition. sent: Article 1 uses Deep Speech approach for Speech Recognition, while Article 2 uses Li-GRU   fMLLR features for Speech Recognition. sent: Article 1 uses CNN   Bi-RNN   CTC  speech to letters   25 9  WER if trainedonlyon SWB approach for Speech Recognition, while Article 2 uses Light Gated Recurrent Units for Speech Recognition. sent: Article 1 uses CNN   Bi-RNN   CTC  speech to letters   25 9  WER if trainedonlyon SWB approach for Speech Recognition, while Article 2 uses Li-GRU   fMLLR features for Speech Recognition. sent: Article 1 uses Deep Speech   FSH approach for Speech Recognition, while Article 2 uses Light Gated Recurrent Units for Speech Recognition. sent: Article 1 uses Deep Speech   FSH approach for Speech Recognition, while Article 2 uses Li-GRU   fMLLR features for Speech Recognition."
1691,1691,79_paper_5,79_paper_6,802,805,1412.5567,1506.07503,Speech Recognition,"sent: Article 1 uses multiple datasets:  Switchboard   Hub500,  VoxForge American-Canadian,  VoxForge Commonwealth,  swb hub 500 WER fullSWBCH,  CHiME real,  VoxForge Indian,  VoxForge European, and  CHiME clean, while Article 2 uses TIMIT dataset.","sent: Article 1 uses CNN   Bi-RNN   CTC  speech to letters  approach for Speech Recognition, while Article 2 uses Bi-RNN   Attention for Speech Recognition. sent: Article 1 uses Deep Speech approach for Speech Recognition, while Article 2 uses Bi-RNN   Attention for Speech Recognition. sent: Article 1 uses CNN   Bi-RNN   CTC  speech to letters   25 9  WER if trainedonlyon SWB approach for Speech Recognition, while Article 2 uses Bi-RNN   Attention for Speech Recognition. sent: Article 1 uses Deep Speech   FSH approach for Speech Recognition, while Article 2 uses Bi-RNN   Attention for Speech Recognition."
1692,1692,79_paper_5,79_paper_10,802,808,1412.5567,1806.07789,Speech Recognition,"sent: Article 1 uses multiple datasets:  Switchboard   Hub500,  VoxForge American-Canadian,  VoxForge Commonwealth,  swb hub 500 WER fullSWBCH,  CHiME real,  VoxForge Indian,  VoxForge European, and  CHiME clean, while Article 2 uses TIMIT dataset.","sent: Article 1 uses CNN   Bi-RNN   CTC  speech to letters  approach for Speech Recognition, while Article 2 uses QCNN-10L-256FM for Speech Recognition. sent: Article 1 uses Deep Speech approach for Speech Recognition, while Article 2 uses QCNN-10L-256FM for Speech Recognition. sent: Article 1 uses CNN   Bi-RNN   CTC  speech to letters   25 9  WER if trainedonlyon SWB approach for Speech Recognition, while Article 2 uses QCNN-10L-256FM for Speech Recognition. sent: Article 1 uses Deep Speech   FSH approach for Speech Recognition, while Article 2 uses QCNN-10L-256FM for Speech Recognition."
1693,1693,79_paper_5,79_paper_0,802,8010,1412.5567,1610.05256,Speech Recognition,"sent: Article 1 and Article 2 use Switchboard   Hub500 dataset. sent: Article 1 uses multiple datasets:  VoxForge American-Canadian,  VoxForge Commonwealth,  swb hub 500 WER fullSWBCH,  CHiME real,  VoxForge Indian,  VoxForge European, and  CHiME clean, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses CNN   Bi-RNN   CTC  speech to letters  approach for Speech Recognition, while Article 2 uses CNN-LSTM for Speech Recognition. sent: Article 1 uses CNN   Bi-RNN   CTC  speech to letters  approach for Speech Recognition, while Article 2 uses Microsoft 2016b for Speech Recognition. sent: Article 1 uses Deep Speech approach for Speech Recognition, while Article 2 uses CNN-LSTM for Speech Recognition. sent: Article 1 uses Deep Speech approach for Speech Recognition, while Article 2 uses Microsoft 2016b for Speech Recognition. sent: Article 1 uses CNN   Bi-RNN   CTC  speech to letters   25 9  WER if trainedonlyon SWB approach for Speech Recognition, while Article 2 uses CNN-LSTM for Speech Recognition. sent: Article 1 uses CNN   Bi-RNN   CTC  speech to letters   25 9  WER if trainedonlyon SWB approach for Speech Recognition, while Article 2 uses Microsoft 2016b for Speech Recognition. sent: Article 1 uses Deep Speech   FSH approach for Speech Recognition, while Article 2 uses CNN-LSTM for Speech Recognition. sent: Article 1 uses Deep Speech   FSH approach for Speech Recognition, while Article 2 uses Microsoft 2016b for Speech Recognition."
1694,1694,79_paper_8,79_paper_6,804,805,1803.10225,1506.07503,Speech Recognition,sent: Article 1 and Article 2 use TIMIT dataset.,"sent: Article 1 uses Light Gated Recurrent Units approach for Speech Recognition, while Article 2 uses Bi-RNN   Attention for Speech Recognition. sent: Article 1 uses Li-GRU   fMLLR features approach for Speech Recognition, while Article 2 uses Bi-RNN   Attention for Speech Recognition."
1695,1695,79_paper_8,79_paper_2,804,807,1803.10225,1412.5567,Speech Recognition,"sent: Article 1 uses TIMIT dataset, while Article 2 uses multiple datasets:  Switchboard   Hub500,  VoxForge American-Canadian,  VoxForge Commonwealth,  swb hub 500 WER fullSWBCH,  CHiME real,  VoxForge Indian,  VoxForge European, and  CHiME clean.","sent: Article 1 uses Light Gated Recurrent Units approach for Speech Recognition, while Article 2 uses CNN   Bi-RNN   CTC  speech to letters  for Speech Recognition. sent: Article 1 uses Light Gated Recurrent Units approach for Speech Recognition, while Article 2 uses Deep Speech for Speech Recognition. sent: Article 1 uses Light Gated Recurrent Units approach for Speech Recognition, while Article 2 uses CNN   Bi-RNN   CTC  speech to letters   25 9  WER if trainedonlyon SWB for Speech Recognition. sent: Article 1 uses Light Gated Recurrent Units approach for Speech Recognition, while Article 2 uses Deep Speech   FSH for Speech Recognition. sent: Article 1 uses Li-GRU   fMLLR features approach for Speech Recognition, while Article 2 uses CNN   Bi-RNN   CTC  speech to letters  for Speech Recognition. sent: Article 1 uses Li-GRU   fMLLR features approach for Speech Recognition, while Article 2 uses Deep Speech for Speech Recognition. sent: Article 1 uses Li-GRU   fMLLR features approach for Speech Recognition, while Article 2 uses CNN   Bi-RNN   CTC  speech to letters   25 9  WER if trainedonlyon SWB for Speech Recognition. sent: Article 1 uses Li-GRU   fMLLR features approach for Speech Recognition, while Article 2 uses Deep Speech   FSH for Speech Recognition."
1696,1696,79_paper_8,79_paper_10,804,808,1803.10225,1806.07789,Speech Recognition,sent: Article 1 and Article 2 use TIMIT dataset.,"sent: Article 1 uses Light Gated Recurrent Units approach for Speech Recognition, while Article 2 uses QCNN-10L-256FM for Speech Recognition. sent: Article 1 uses Li-GRU   fMLLR features approach for Speech Recognition, while Article 2 uses QCNN-10L-256FM for Speech Recognition."
1697,1697,79_paper_8,79_paper_0,804,8010,1803.10225,1610.05256,Speech Recognition,"sent: Article 1 uses TIMIT dataset, while Article 2 uses Switchboard   Hub500 dataset.","sent: Article 1 uses Light Gated Recurrent Units approach for Speech Recognition, while Article 2 uses CNN-LSTM for Speech Recognition. sent: Article 1 uses Light Gated Recurrent Units approach for Speech Recognition, while Article 2 uses Microsoft 2016b for Speech Recognition. sent: Article 1 uses Li-GRU   fMLLR features approach for Speech Recognition, while Article 2 uses CNN-LSTM for Speech Recognition. sent: Article 1 uses Li-GRU   fMLLR features approach for Speech Recognition, while Article 2 uses Microsoft 2016b for Speech Recognition."
1698,1698,79_paper_6,79_paper_7,805,806,1506.07503,1803.10225,Speech Recognition,sent: Article 1 and Article 2 use TIMIT dataset.,"sent: Article 1 uses Bi-RNN   Attention approach for Speech Recognition, while Article 2 uses Light Gated Recurrent Units for Speech Recognition. sent: Article 1 uses Bi-RNN   Attention approach for Speech Recognition, while Article 2 uses Li-GRU   fMLLR features for Speech Recognition."
1699,1699,79_paper_6,79_paper_2,805,807,1506.07503,1412.5567,Speech Recognition,"sent: Article 1 uses TIMIT dataset, while Article 2 uses multiple datasets:  Switchboard   Hub500,  VoxForge American-Canadian,  VoxForge Commonwealth,  swb hub 500 WER fullSWBCH,  CHiME real,  VoxForge Indian,  VoxForge European, and  CHiME clean.","sent: Article 1 uses Bi-RNN   Attention approach for Speech Recognition, while Article 2 uses CNN   Bi-RNN   CTC  speech to letters  for Speech Recognition. sent: Article 1 uses Bi-RNN   Attention approach for Speech Recognition, while Article 2 uses Deep Speech for Speech Recognition. sent: Article 1 uses Bi-RNN   Attention approach for Speech Recognition, while Article 2 uses CNN   Bi-RNN   CTC  speech to letters   25 9  WER if trainedonlyon SWB for Speech Recognition. sent: Article 1 uses Bi-RNN   Attention approach for Speech Recognition, while Article 2 uses Deep Speech   FSH for Speech Recognition."
1700,1700,79_paper_6,79_paper_10,805,808,1506.07503,1806.07789,Speech Recognition,sent: Article 1 and Article 2 use TIMIT dataset.,"sent: Article 1 uses Bi-RNN   Attention approach for Speech Recognition, while Article 2 uses QCNN-10L-256FM for Speech Recognition."
1701,1701,79_paper_6,79_paper_0,805,8010,1506.07503,1610.05256,Speech Recognition,"sent: Article 1 uses TIMIT dataset, while Article 2 uses Switchboard   Hub500 dataset.","sent: Article 1 uses Bi-RNN   Attention approach for Speech Recognition, while Article 2 uses CNN-LSTM for Speech Recognition. sent: Article 1 uses Bi-RNN   Attention approach for Speech Recognition, while Article 2 uses Microsoft 2016b for Speech Recognition."
1702,1702,79_paper_10,79_paper_4,808,809,1806.07789,1412.5567,Speech Recognition,"sent: Article 1 uses TIMIT dataset, while Article 2 uses multiple datasets:  Switchboard   Hub500,  VoxForge American-Canadian,  VoxForge Commonwealth,  swb hub 500 WER fullSWBCH,  CHiME real,  VoxForge Indian,  VoxForge European, and  CHiME clean.","sent: Article 1 uses QCNN-10L-256FM approach for Speech Recognition, while Article 2 uses CNN   Bi-RNN   CTC  speech to letters  for Speech Recognition. sent: Article 1 uses QCNN-10L-256FM approach for Speech Recognition, while Article 2 uses Deep Speech for Speech Recognition. sent: Article 1 uses QCNN-10L-256FM approach for Speech Recognition, while Article 2 uses CNN   Bi-RNN   CTC  speech to letters   25 9  WER if trainedonlyon SWB for Speech Recognition. sent: Article 1 uses QCNN-10L-256FM approach for Speech Recognition, while Article 2 uses Deep Speech   FSH for Speech Recognition."
1703,1703,79_paper_10,79_paper_0,808,8010,1806.07789,1610.05256,Speech Recognition,"sent: Article 1 uses TIMIT dataset, while Article 2 uses Switchboard   Hub500 dataset.","sent: Article 1 uses QCNN-10L-256FM approach for Speech Recognition, while Article 2 uses CNN-LSTM for Speech Recognition. sent: Article 1 uses QCNN-10L-256FM approach for Speech Recognition, while Article 2 uses Microsoft 2016b for Speech Recognition."
1704,1704,80_paper_2,80_paper_1,810,811,1806.02559,1703.0652,Scene Text Detection,"sent: Article 1 and Article 2 use SCUT-CTW1500 dataset.sent: Article 1 and Article 2 use IC15 dataset. sent: Article 1 uses IC17-MLT dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses PSENet-1s approach for Curved Text Detection, while Article 2 uses SegLink for Curved Text Detection. sent: Article 1 uses PSENet-1s approach for Scene Text Detection, while Article 2 uses SegLink for Scene Text Detection."
1705,1705,80_paper_2,80_paper_0,810,812,1806.02559,1706.09579,Scene Text Detection,"sent: Article 1 and Article 2 use IC15 dataset. sent: Article 1 uses multiple datasets:  SCUT-CTW1500, and  IC17-MLT, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses PSENet-1s approach for Scene Text Detection, while Article 2 uses R2CNN for Scene Text Detection."
1706,1706,80_paper_1,80_paper_0,811,812,1703.0652,1706.09579,Scene Text Detection,"sent: Article 1 and Article 2 use IC15 dataset. sent: Article 1 uses SCUT-CTW1500 dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses SegLink approach for Scene Text Detection, while Article 2 uses R2CNN for Scene Text Detection."
1707,1707,80_paper_1,80_paper_3,811,813,1703.0652,1806.02559,Scene Text Detection,"sent: Article 1 and Article 2 use SCUT-CTW1500 dataset.sent: Article 1 and Article 2 use IC15 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses IC17-MLT dataset.","sent: Article 1 uses SegLink approach for Curved Text Detection, while Article 2 uses PSENet-1s for Curved Text Detection. sent: Article 1 uses SegLink approach for Scene Text Detection, while Article 2 uses PSENet-1s for Scene Text Detection."
1708,1708,80_paper_0,80_paper_3,812,813,1706.09579,1806.02559,Scene Text Detection,"sent: Article 1 and Article 2 use IC15 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SCUT-CTW1500, and  IC17-MLT.","sent: Article 1 uses R2CNN approach for Scene Text Detection, while Article 2 uses PSENet-1s for Scene Text Detection."
1709,1709,81_paper_0,81_paper_4,820,821,1711.04903,1604.05529,Part-Of-Speech Tagging,sent: Article 1 and Article 2 use UD dataset.sent: Article 1 and Article 2 use Penn Treebank dataset.,"sent: Article 1 uses Adversarial Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses Bi-LSTM for Part-Of-Speech Tagging."
1710,1710,81_paper_0,81_paper_2,820,823,1711.04903,1805.08237,Part-Of-Speech Tagging,"sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 uses UD dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Adversarial Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses Meta BiLSTM for Part-Of-Speech Tagging."
1711,1711,81_paper_0,81_paper_5,820,825,1711.04903,1805.02474,Part-Of-Speech Tagging,"sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 uses UD dataset, while Article 2 uses multiple datasets:  MR,  IMDb, and  CoNLL 2003  English .","sent: Article 1 uses Adversarial Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses S-LSTM for Part-Of-Speech Tagging."
1712,1712,81_paper_4,81_paper_1,821,822,1604.05529,1711.04903,Part-Of-Speech Tagging,sent: Article 1 and Article 2 use UD dataset.sent: Article 1 and Article 2 use Penn Treebank dataset.,"sent: Article 1 uses Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses Adversarial Bi-LSTM for Part-Of-Speech Tagging."
1713,1713,81_paper_4,81_paper_2,821,823,1604.05529,1805.08237,Part-Of-Speech Tagging,"sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 uses UD dataset, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses Meta BiLSTM for Part-Of-Speech Tagging."
1714,1714,81_paper_4,81_paper_5,821,825,1604.05529,1805.02474,Part-Of-Speech Tagging,"sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 uses UD dataset, while Article 2 uses multiple datasets:  MR,  IMDb, and  CoNLL 2003  English .","sent: Article 1 uses Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses S-LSTM for Part-Of-Speech Tagging."
1715,1715,81_paper_2,81_paper_3,823,824,1805.08237,1604.05529,Part-Of-Speech Tagging,"sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses UD dataset.","sent: Article 1 uses Meta BiLSTM approach for Part-Of-Speech Tagging, while Article 2 uses Bi-LSTM for Part-Of-Speech Tagging."
1716,1716,81_paper_2,81_paper_5,823,825,1805.08237,1805.02474,Part-Of-Speech Tagging,"sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MR,  IMDb, and  CoNLL 2003  English .","sent: Article 1 uses Meta BiLSTM approach for Part-Of-Speech Tagging, while Article 2 uses S-LSTM for Part-Of-Speech Tagging."
1717,1717,84_paper_2,84_paper_1,850,851,1512.04412,1703.0687,Multi-Human Parsing,"sent: Article 1 uses PASCAL-Person-Part dataset, while Article 2 uses multiple datasets:  COCO,  MHP v1 0, and  MHP v2 0.","sent: Article 1 uses MNC approach for Multi-Human Parsing, while Article 2 uses Mask R-CNN for Multi-Human Parsing."
1718,1718,86_paper_2,86_paper_11,870,875,1403.6652,1811.02798,Node Classification,"sent: Article 1 and Article 2 use Citeseer dataset.sent: Article 1 and Article 2 use Pubmed dataset.sent: Article 1 and Article 2 use Cora dataset. sent: Article 1 uses multiple datasets:  BlogCatalog,  NELL, and  Wikipedia, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DeepWalk approach for Node Classification, while Article 2 uses MTGAE for Node Classification."
1719,1719,86_paper_2,86_paper_8,870,8710,1403.6652,1802.08352,Node Classification,"sent: Article 1 and Article 2 use Citeseer dataset.sent: Article 1 and Article 2 use Pubmed dataset.sent: Article 1 and Article 2 use Cora dataset. sent: Article 1 uses multiple datasets:  BlogCatalog,  NELL, and  Wikipedia, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses DeepWalk approach for Node Classification, while Article 2 uses alpha-LoNGAE for Node Classification."
1720,1720,86_paper_11,86_paper_3,875,877,1811.02798,1403.6652,Node Classification,"sent: Article 1 and Article 2 use Citeseer dataset.sent: Article 1 and Article 2 use Pubmed dataset.sent: Article 1 and Article 2 use Cora dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  BlogCatalog,  NELL, and  Wikipedia.","sent: Article 1 uses MTGAE approach for Node Classification, while Article 2 uses DeepWalk for Node Classification."
1721,1721,86_paper_11,86_paper_8,875,8710,1811.02798,1802.08352,Node Classification,sent: Article 1 and Article 2 use Citeseer dataset.sent: Article 1 and Article 2 use Pubmed dataset.sent: Article 1 and Article 2 use Cora dataset.,"sent: Article 1 uses MTGAE approach for Node Classification, while Article 2 uses alpha-LoNGAE for Node Classification."
1722,1722,86_paper_8,86_paper_0,8710,8713,1802.08352,1403.6652,Node Classification,"sent: Article 1 and Article 2 use Citeseer dataset.sent: Article 1 and Article 2 use Pubmed dataset.sent: Article 1 and Article 2 use Cora dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  BlogCatalog,  NELL, and  Wikipedia.","sent: Article 1 uses alpha-LoNGAE approach for Node Classification, while Article 2 uses DeepWalk for Node Classification."
1723,1723,87_paper_1,87_paper_0,880,881,1710.10903,1403.6652,Document Classification,"sent: Article 1 and Article 2 use Cora dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  BlogCatalog,  Citeseer,  Pubmed,  NELL, and  Wikipedia.","sent: Article 1 uses GAT approach for Document Classification, while Article 2 uses DeepWalk for Document Classification."
1724,1724,94_paper_1,94_paper_2,950,951,1710.00925,1804.01005,Head Pose Estimation,"sent: Article 1 and Article 2 use AFLW2000 dataset.sent: Article 1 and Article 2 use BIWI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  AFLW2000-3D, and  Florence.","sent: Article 1 uses Multi-Loss ResNet50 approach for Head Pose Estimation, while Article 2 uses 3DDFA for Head Pose Estimation. sent: Article 1 uses Multi-Loss ResNet50 approach for Head Pose Estimation, while Article 2 uses 3DDFA   SDM for Head Pose Estimation."
1725,1725,94_paper_2,94_paper_0,951,952,1804.01005,1710.00925,Head Pose Estimation,"sent: Article 1 and Article 2 use AFLW2000 dataset.sent: Article 1 and Article 2 use BIWI dataset. sent: Article 1 uses multiple datasets:  AFLW2000-3D, and  Florence, while Article 2 does not use specific datasets for experiments.","sent: Article 1 uses 3DDFA approach for Head Pose Estimation, while Article 2 uses Multi-Loss ResNet50 for Head Pose Estimation. sent: Article 1 uses 3DDFA   SDM approach for Head Pose Estimation, while Article 2 uses Multi-Loss ResNet50 for Head Pose Estimation."
1726,1726,95_paper_5,95_paper_11,960,963,1606.08921,1710.04026,Image Denoising,"sent: Article 1 uses multiple datasets:  BSD200 sigma50,  BSD200 sigma10,  Urban100 sigma50,  Urban100 sigma70,  BSD200 sigma30, and  BSD200 sigma70, while Article 2 uses multiple datasets:  BSD68 sigma15,  BSD68 sigma50, and  BSD68 sigma25.","sent: Article 1 uses RED30 approach for Image Denoising, while Article 2 uses FFDNet for Image Denoising."
1727,1727,95_paper_11,95_paper_0,963,964,1710.04026,1606.08921,Image Denoising,"sent: Article 1 uses multiple datasets:  BSD68 sigma15,  BSD68 sigma50, and  BSD68 sigma25, while Article 2 uses multiple datasets:  BSD200 sigma50,  BSD200 sigma10,  Urban100 sigma50,  Urban100 sigma70,  BSD200 sigma30, and  BSD200 sigma70.","sent: Article 1 uses FFDNet approach for Image Denoising, while Article 2 uses RED30 for Image Denoising."
1728,1728,117_paper_2,117_paper_0,1181,1182,1506.01497,1904.089,Real-Time Object Detection,"sent: Article 1 uses PASCAL VOC 2007 dataset, while Article 2 uses COCO dataset.","sent: Article 1 uses Faster R-CNN approach for Real-Time Object Detection, while Article 2 uses CornerNet-Saccade for Real-Time Object Detection. sent: Article 1 uses Faster R-CNN approach for Real-Time Object Detection, while Article 2 uses CornerNet-Squeeze for Real-Time Object Detection. sent: Article 1 uses Faster R-CNN approach for Object Detection, while Article 2 uses CornerNet-Saccade for Object Detection. sent: Article 1 uses Faster R-CNN approach for Object Detection, while Article 2 uses CornerNet-Squeeze for Object Detection."
