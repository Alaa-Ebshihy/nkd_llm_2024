,Unnamed: 0,paper_1,paper_2,paper_1_id,paper_2_id,datasets_diff,approach_diff,similar_pairs_arxiv_ids,syn_datasets_diff,syn_approach_diff
0,0,06_paper_01,06_paper_02,61,62,sent: Article 1 and Article 2 use SciFact sourced from S2ORC dataset. sent: Article 1 and Article 2 use dataset from diverse topics in biomedicine domain. sent: Article 1 and Article 2 use dataset of 1409 claims against 5183 abstracts.,"sent: Article 1 uses TF-IDF approach for document retrieval, while Article 2 uses BioSentVec approach for document retrieval. sent: Article 1 uses BERT approach for rationale selection, while Article 2 uses BERT combined with MLP (multi-layer perceptron) and  BERT combined with KGAT (Kernel Graph Attention Network) approach for rationale selection. sent: Article 1 uses BERT approach for verdict prediction, while Article 2 uses BERT combined with MLP (multi-layer perceptron) approach for stance prediction.","[['1606.01549', '1710.10723'], ['1710.10723', '1606.01549'], ['1603.06147', '1606.02891'], ['1603.06147', '1711.02281'], ['1603.06147', '1711.01068'], ['1606.02891', '1606.07947'], ['1606.02891', '1511.06732'], ['1606.02891', '1603.06147'], ['1711.02281', '1606.07947'], ['1711.02281', '1511.06732'], ['1711.02281', '1603.06147'], ['1606.07947', '1711.02281'], ['1606.07947', '1606.02891'], ['1606.07947', '1711.01068'], ['1511.06732', '1606.02891'], ['1511.06732', '1711.01068'], ['1511.06732', '1711.02281'], ['1403.6652', '1811.02798'], ['1811.02798', '1403.6652']]","['sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.', 'sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.', 'sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.', 'sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian.', 'sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses IWSLT2015 German-English dataset.', 'sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English.', 'sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses IWSLT2015 German-English dataset.', 'sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses WMT2015 English-German dataset.', 'sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses multiple datasets:  WMT2016 Romanian-English,  WMT2016 English-Romanian,  IWSLT2015 English-German, and  WMT2014 German-English, while Article 2 uses IWSLT2015 Thai-English dataset.', 'sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian, while Article 2 uses IWSLT2015 German-English dataset.', 'sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian, while Article 2 uses WMT2015 English-German dataset.', 'sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses IWSLT2015 Thai-English dataset, while Article 2 uses multiple datasets:  WMT2016 Romanian-English,  WMT2016 English-Romanian,  IWSLT2015 English-German, and  WMT2014 German-English.', 'sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.', 'sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English, while Article 2 uses IWSLT2015 German-English dataset.', 'sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.', 'sent: Article 1 and Article 2 use IWSLT2015 German-English dataset.', 'sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian.', 'sent: Article 1 and Article 2 use Citeseer dataset.sent: Article 1 and Article 2 use Pubmed dataset.sent: Article 1 and Article 2 use Cora dataset. sent: Article 1 uses multiple datasets:  BlogCatalog,  NELL, and  Wikipedia, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Citeseer dataset.sent: Article 1 and Article 2 use Pubmed dataset.sent: Article 1 and Article 2 use Cora dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  BlogCatalog,  NELL, and  Wikipedia.']","['sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses S-Norm for Question Answering.', 'sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses GA for Question Answering.', 'sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation.', 'sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation.', 'sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses DCCL for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses DCCL for Machine Translation.', 'sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation.', 'sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation.', 'sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation.', 'sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation.', 'sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation.', 'sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation.', 'sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation.', 'sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation.', 'sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses DCCL for Machine Translation.', 'sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation.', 'sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses DCCL for Machine Translation.', 'sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation.', 'sent: Article 1 uses DeepWalk approach for Node Classification, while Article 2 uses MTGAE for Node Classification.', 'sent: Article 1 uses MTGAE approach for Node Classification, while Article 2 uses DeepWalk for Node Classification.']"
1,1,06_paper_01,06_paper_03,61,63,sent: Article 1 and Article 2 use SciFact sourced from S2ORC dataset. sent: Article 1 and Article 2 use dataset from diverse topics in biomedicine domain. sent: Article 1 and Article 2 use dataset of 1409 claims against 5183 abstracts.,"sent: Article 1 uses TF-IDF approach for document retrieval, while Article 2 uses BM25 combined with T5 as reranking approach for document retrieval. sent: Article 1 uses BERT approach for rationale selection, while Article 2 uses T5 trained on MS MARCO approach for rationale selection. sent: Article 1 uses BERT approach for verdict prediction, while Article 2 uses uses T5 without fine tuining  approach for verdict prediction.","[['1603.01547', '1606.01549'], ['1606.01549', '1603.01547'], ['1509.06461', '1507.04296'], ['1507.04296', '1509.06461'], ['1409.3215', '1606.02891'], ['1409.3215', '1711.02281'], ['1409.3215', '1711.01068'], ['1606.02891', '1409.3215'], ['1606.02891', '1511.06732'], ['1711.02281', '1409.3215'], ['1711.02281', '1511.06732'], ['1511.06732', '1606.02891'], ['1511.06732', '1711.01068'], ['1511.06732', '1711.02281'], ['1403.6652', '1802.08352'], ['1802.08352', '1403.6652']]","['sent: Article 1 and Article 2 use CNN   Daily Mail dataset.sent: Article 1 and Article 2 use Children s Book Test dataset. sent: Article 1 uses SearchQA dataset, while Article 2 uses Quasar dataset.', 'sent: Article 1 and Article 2 use CNN   Daily Mail dataset.sent: Article 1 and Article 2 use Children s Book Test dataset. sent: Article 1 uses Quasar dataset, while Article 2 uses SearchQA dataset.', 'sent: Article 1 and Article 2 use Atari 2600 Fishing Derby dataset.sent: Article 1 and Article 2 use Atari 2600 Atlantis dataset.sent: Article 1 and Article 2 use Atari 2600 Tennis dataset.sent: Article 1 and Article 2 use Atari 2600 Video Pinball dataset.sent: Article 1 and Article 2 use Atari 2600 Up and Down dataset.sent: Article 1 and Article 2 use Atari 2600 Gopher dataset.sent: Article 1 and Article 2 use Atari 2600 Double Dunk dataset.sent: Article 1 and Article 2 use Atari 2600 Assault dataset.sent: Article 1 and Article 2 use Atari 2600 Road Runner dataset.sent: Article 1 and Article 2 use Atari 2600 Private Eye dataset.sent: Article 1 and Article 2 use Atari 2600 Gravitar dataset.sent: Article 1 and Article 2 use Atari 2600 Breakout dataset.sent: Article 1 and Article 2 use Atari 2600 Krull dataset.sent: Article 1 and Article 2 use Atari 2600 HERO dataset.sent: Article 1 and Article 2 use Atari 2600 Ice Hockey dataset.sent: Article 1 and Article 2 use Atari 2600 Asterix dataset.sent: Article 1 and Article 2 use Atari 2600 Time Pilot dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Seaquest dataset.sent: Article 1 and Article 2 use Atari 2600 Name This Game dataset.sent: Article 1 and Article 2 use Atari 2600 Chopper Command dataset.sent: Article 1 and Article 2 use Atari 2600 Kangaroo dataset.sent: Article 1 and Article 2 use Atari 2600 Zaxxon dataset.sent: Article 1 and Article 2 use Atari 2600 Bowling dataset.sent: Article 1 and Article 2 use Atari 2600 Enduro dataset.sent: Article 1 and Article 2 use Atari 2600 Wizard of Wor dataset.sent: Article 1 and Article 2 use Atari 2600 Ms  Pacman dataset.sent: Article 1 and Article 2 use Atari 2600 Pong dataset.sent: Article 1 and Article 2 use Atari 2600 Star Gunner dataset.sent: Article 1 and Article 2 use Atari 2600 Space Invaders dataset.sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Bank Heist dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Alien dataset.sent: Article 1 and Article 2 use Atari 2600 Amidar dataset.sent: Article 1 and Article 2 use Atari 2600 Centipede dataset.sent: Article 1 and Article 2 use Atari 2600 Beam Rider dataset.sent: Article 1 and Article 2 use Atari 2600 Demon Attack dataset.sent: Article 1 and Article 2 use Atari 2600 Crazy Climber dataset.sent: Article 1 and Article 2 use Atari 2600 James Bond dataset.sent: Article 1 and Article 2 use Atari 2600 Kung-Fu Master dataset.sent: Article 1 and Article 2 use Atari 2600 Asteroids dataset.sent: Article 1 and Article 2 use Atari 2600 Battle Zone dataset.sent: Article 1 and Article 2 use Atari 2600 Tutankham dataset.sent: Article 1 and Article 2 use Atari 2600 Robotank dataset.sent: Article 1 and Article 2 use Atari 2600 Boxing dataset.sent: Article 1 and Article 2 use Atari 2600 River Raid dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 uses Atari 2600 Berzerk dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Atari 2600 Fishing Derby dataset.sent: Article 1 and Article 2 use Atari 2600 Atlantis dataset.sent: Article 1 and Article 2 use Atari 2600 Tennis dataset.sent: Article 1 and Article 2 use Atari 2600 Video Pinball dataset.sent: Article 1 and Article 2 use Atari 2600 Up and Down dataset.sent: Article 1 and Article 2 use Atari 2600 Gopher dataset.sent: Article 1 and Article 2 use Atari 2600 Double Dunk dataset.sent: Article 1 and Article 2 use Atari 2600 Assault dataset.sent: Article 1 and Article 2 use Atari 2600 Road Runner dataset.sent: Article 1 and Article 2 use Atari 2600 Private Eye dataset.sent: Article 1 and Article 2 use Atari 2600 Gravitar dataset.sent: Article 1 and Article 2 use Atari 2600 Breakout dataset.sent: Article 1 and Article 2 use Atari 2600 Krull dataset.sent: Article 1 and Article 2 use Atari 2600 HERO dataset.sent: Article 1 and Article 2 use Atari 2600 Ice Hockey dataset.sent: Article 1 and Article 2 use Atari 2600 Asterix dataset.sent: Article 1 and Article 2 use Atari 2600 Time Pilot dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Seaquest dataset.sent: Article 1 and Article 2 use Atari 2600 Name This Game dataset.sent: Article 1 and Article 2 use Atari 2600 Chopper Command dataset.sent: Article 1 and Article 2 use Atari 2600 Kangaroo dataset.sent: Article 1 and Article 2 use Atari 2600 Zaxxon dataset.sent: Article 1 and Article 2 use Atari 2600 Bowling dataset.sent: Article 1 and Article 2 use Atari 2600 Enduro dataset.sent: Article 1 and Article 2 use Atari 2600 Wizard of Wor dataset.sent: Article 1 and Article 2 use Atari 2600 Ms  Pacman dataset.sent: Article 1 and Article 2 use Atari 2600 Pong dataset.sent: Article 1 and Article 2 use Atari 2600 Star Gunner dataset.sent: Article 1 and Article 2 use Atari 2600 Space Invaders dataset.sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Bank Heist dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Alien dataset.sent: Article 1 and Article 2 use Atari 2600 Amidar dataset.sent: Article 1 and Article 2 use Atari 2600 Centipede dataset.sent: Article 1 and Article 2 use Atari 2600 Beam Rider dataset.sent: Article 1 and Article 2 use Atari 2600 Demon Attack dataset.sent: Article 1 and Article 2 use Atari 2600 Crazy Climber dataset.sent: Article 1 and Article 2 use Atari 2600 James Bond dataset.sent: Article 1 and Article 2 use Atari 2600 Kung-Fu Master dataset.sent: Article 1 and Article 2 use Atari 2600 Asteroids dataset.sent: Article 1 and Article 2 use Atari 2600 Battle Zone dataset.sent: Article 1 and Article 2 use Atari 2600 Tutankham dataset.sent: Article 1 and Article 2 use Atari 2600 Robotank dataset.sent: Article 1 and Article 2 use Atari 2600 Boxing dataset.sent: Article 1 and Article 2 use Atari 2600 River Raid dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Atari 2600 Berzerk dataset.', 'sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.', 'sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian.', 'sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses IWSLT2015 German-English dataset.', 'sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses WMT2014 English-French dataset.', 'sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses IWSLT2015 German-English dataset.', 'sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian, while Article 2 uses WMT2014 English-French dataset.', 'sent: Article 1 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian, while Article 2 uses IWSLT2015 German-English dataset.', 'sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.', 'sent: Article 1 and Article 2 use IWSLT2015 German-English dataset.', 'sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  IWSLT2015 English-German,  WMT2014 German-English,  WMT2016 Romanian-English, and  WMT2016 English-Romanian.', 'sent: Article 1 and Article 2 use Citeseer dataset.sent: Article 1 and Article 2 use Pubmed dataset.sent: Article 1 and Article 2 use Cora dataset. sent: Article 1 uses multiple datasets:  BlogCatalog,  NELL, and  Wikipedia, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Citeseer dataset.sent: Article 1 and Article 2 use Pubmed dataset.sent: Article 1 and Article 2 use Cora dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  BlogCatalog,  NELL, and  Wikipedia.']","['sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses ASR approach for Open-Domain Question Answering, while Article 2 uses GA for Open-Domain Question Answering.', 'sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses GA approach for Open-Domain Question Answering, while Article 2 uses ASR for Open-Domain Question Answering.', 'sent: Article 1 uses DDQN  tuned  hs approach for Atari Games, while Article 2 uses Gorila for Atari Games. sent: Article 1 uses Prior Duel hs approach for Atari Games, while Article 2 uses Gorila for Atari Games. sent: Article 1 uses DQN hs approach for Atari Games, while Article 2 uses Gorila for Atari Games. sent: Article 1 uses DQN noop approach for Atari Games, while Article 2 uses Gorila for Atari Games.', 'sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses DDQN  tuned  hs for Atari Games. sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses Prior Duel hs for Atari Games. sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses DQN hs for Atari Games. sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses DQN noop for Atari Games.', 'sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation.', 'sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation.', 'sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses DCCL for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses DCCL for Machine Translation.', 'sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation.', 'sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation.', 'sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation.', 'sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation.', 'sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation.', 'sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses DCCL for Machine Translation.', 'sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation.', 'sent: Article 1 uses DeepWalk approach for Node Classification, while Article 2 uses alpha-LoNGAE for Node Classification.', 'sent: Article 1 uses alpha-LoNGAE approach for Node Classification, while Article 2 uses DeepWalk for Node Classification.']"
2,2,06_paper_01,06_paper_04,61,64,sent: Article 1 and Article 2 use SciFact sourced from S2ORC dataset. sent: Article 1 and Article 2 use dataset from diverse topics in biomedicine domain. sent: Article 1 and Article 2 use dataset of 1409 claims against 5183 abstracts.,"sent: Article 1 uses TF-IDF approach for document retrieval, while Article 2 uses BioSentVec approach for document retrieval. sent: Article 1 uses BERT approach for rationale selection, while Article 2 uses BioBERT combined with MLP (multi-layer perceptron) approach for rationale selection. sent: Article 1 uses BERT approach for verdict prediction, while Article 2 uses BioBERT combined with MLP (multi-layer perceptron) approach for stance prediction.","[['1610.05256', '1506.07503'], ['1506.07503', '1610.05256'], ['1711.04903', '1805.08237']]","['sent: Article 1 uses Switchboard   Hub500 dataset, while Article 2 uses TIMIT dataset.', 'sent: Article 1 uses TIMIT dataset, while Article 2 uses Switchboard   Hub500 dataset.', 'sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 uses UD dataset, while Article 2 does not use specific datasets for experiments.']","['sent: Article 1 uses CNN-LSTM approach for Speech Recognition, while Article 2 uses Bi-RNN   Attention for Speech Recognition. sent: Article 1 uses Microsoft 2016b approach for Speech Recognition, while Article 2 uses Bi-RNN   Attention for Speech Recognition.', 'sent: Article 1 uses Bi-RNN   Attention approach for Speech Recognition, while Article 2 uses CNN-LSTM for Speech Recognition. sent: Article 1 uses Bi-RNN   Attention approach for Speech Recognition, while Article 2 uses Microsoft 2016b for Speech Recognition.', 'sent: Article 1 uses Adversarial Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses Meta BiLSTM for Part-Of-Speech Tagging.']"
3,3,06_paper_01,06_paper_05,61,65,sent: Article 1 and Article 2 use SciFact sourced from S2ORC dataset. sent: Article 1 and Article 2 use dataset from diverse topics in biomedicine domain. sent: Article 1 and Article 2 use dataset of 1409 claims against 5183 abstracts.,"sent: Article 1 uses TF-IDF approach for document retrieval, while Article 2 uses BM25 combined with T5 as reranking approach for document retrieval. sent: Article 1 uses BERT approach for rationale selection, while Article 2 uses longformer encoder approach for rationale selection. sent: Article 1 uses BERT approach for verdict prediction, while Article 2 uses longformer encoder approach for verdict prediction.","[['1506.04579', '1511.07122'], ['1506.04579', '1611.08323'], ['1511.07122', '1504.01013'], ['1511.07122', '1506.04579'], ['1504.01013', '1611.08323'], ['1504.01013', '1511.07122'], ['1611.08323', '1506.04579'], ['1606.01549', '1610.09027'], ['1610.09027', '1606.01549'], ['1512.02167', '1603.02814'], ['1711.10295', '1710.06555'], ['1701.07717', '1710.06555'], ['1710.06555', '1711.10295'], ['1711.04903', '1604.05529'], ['1711.04903', '1805.08237'], ['1604.05529', '1711.04903']]","['sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset. sent: Article 1 uses PASCAL Context dataset, while Article 2 uses multiple datasets:  CamVid,  ADE20K, and  Cityscapes.', 'sent: Article 1 uses multiple datasets:  PASCAL VOC 2012, and  PASCAL Context, while Article 2 uses Cityscapes dataset.', 'sent: Article 1 uses multiple datasets:  CamVid,  PASCAL VOC 2012,  ADE20K, and  Cityscapes, while Article 2 uses PASCAL Context dataset.', 'sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset. sent: Article 1 uses multiple datasets:  CamVid,  ADE20K, and  Cityscapes, while Article 2 uses PASCAL Context dataset.', 'sent: Article 1 uses PASCAL Context dataset, while Article 2 uses Cityscapes dataset.', 'sent: Article 1 uses PASCAL Context dataset, while Article 2 uses multiple datasets:  CamVid,  PASCAL VOC 2012,  ADE20K, and  Cityscapes.', 'sent: Article 1 uses Cityscapes dataset, while Article 2 uses multiple datasets:  PASCAL VOC 2012, and  PASCAL Context.', 'sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses bAbi dataset.', 'sent: Article 1 uses bAbi dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.', 'sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset. sent: Article 1 uses COCO Visual Question Answering  VQA  real images 1 0 multiple choice dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 uses DukeMTMC-reID dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 uses DukeMTMC-reID dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses DukeMTMC-reID dataset.', 'sent: Article 1 and Article 2 use UD dataset.sent: Article 1 and Article 2 use Penn Treebank dataset.', 'sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 uses UD dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use UD dataset.sent: Article 1 and Article 2 use Penn Treebank dataset.']","['sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses Dilation10 for Semantic Segmentation. sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses Dilated Convolutions for Semantic Segmentation. sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses DilatedNet for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses Dilation10 for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses Dilated Convolutions for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses DilatedNet for Semantic Segmentation.', 'sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses FRRN for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses FRRN for Semantic Segmentation.', 'sent: Article 1 uses Dilation10 approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation.', 'sent: Article 1 uses Dilation10 approach for Semantic Segmentation, while Article 2 uses ParseNet  for Semantic Segmentation. sent: Article 1 uses Dilation10 approach for Semantic Segmentation, while Article 2 uses ParseNet for Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Semantic Segmentation, while Article 2 uses ParseNet  for Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Semantic Segmentation, while Article 2 uses ParseNet for Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Semantic Segmentation, while Article 2 uses ParseNet  for Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Semantic Segmentation, while Article 2 uses ParseNet for Semantic Segmentation.', 'sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses FRRN for Semantic Segmentation.', 'sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses Dilation10 for Semantic Segmentation. sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses Dilated Convolutions for Semantic Segmentation. sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses DilatedNet for Semantic Segmentation.', 'sent: Article 1 uses FRRN approach for Semantic Segmentation, while Article 2 uses ParseNet  for Semantic Segmentation. sent: Article 1 uses FRRN approach for Semantic Segmentation, while Article 2 uses ParseNet for Semantic Segmentation.', 'sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses SDNC for Question Answering.', 'sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses GA for Question Answering.', 'sent: Article 1 uses iBOWIMG baseline approach for Visual Question Answering, while Article 2 uses CNN-RNN for Visual Question Answering.', 'sent: Article 1 uses IDE    CamStyle   Random Erasing approach for Person Re-Identification, while Article 2 uses MSCAN for Person Re-Identification. sent: Article 1 uses IDE    CamStyle approach for Person Re-Identification, while Article 2 uses MSCAN for Person Re-Identification. sent: Article 1 uses IDE  approach for Person Re-Identification, while Article 2 uses MSCAN for Person Re-Identification.', 'sent: Article 1 uses GAN approach for Person Re-Identification, while Article 2 uses MSCAN for Person Re-Identification.', 'sent: Article 1 uses MSCAN approach for Person Re-Identification, while Article 2 uses IDE    CamStyle   Random Erasing for Person Re-Identification. sent: Article 1 uses MSCAN approach for Person Re-Identification, while Article 2 uses IDE    CamStyle for Person Re-Identification. sent: Article 1 uses MSCAN approach for Person Re-Identification, while Article 2 uses IDE  for Person Re-Identification.', 'sent: Article 1 uses Adversarial Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses Bi-LSTM for Part-Of-Speech Tagging.', 'sent: Article 1 uses Adversarial Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses Meta BiLSTM for Part-Of-Speech Tagging.', 'sent: Article 1 uses Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses Adversarial Bi-LSTM for Part-Of-Speech Tagging.']"
4,4,06_paper_01,06_paper_06,61,66,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses CoVERT sourced from fact-checked tweets and evidence from online resources from medical domain of 300 tweets.","sent: Article 1 uses TF-IDF approach for document retrieval, while Article 2 uses BM25 combined with T5 as reranking approach for document retrieval. sent: Article 1 uses BERT approach for rationale selection, while Article 2 uses longformer encoder approach for rationale selection. sent: Article 1 uses BERT approach for verdict prediction, while Article 2 uses Longformer (ternary head) encoder approach for verdict prediction.","[['1506.04579', '1712.02616'], ['1506.04579', '1707.02968'], ['1712.02616', '1504.01013'], ['1712.02616', '1506.04579'], ['1504.01013', '1707.02968'], ['1706.02596', '1606.01549'], ['1606.01549', '1706.02596'], ['1606.02891', '1610.10099'], ['1711.02281', '1610.10099'], ['1610.10099', '1606.02891'], ['1610.10099', '1711.01068'], ['1610.10099', '1711.02281'], ['1711.01068', '1610.10099'], ['1711.10295', '1808.06281'], ['1808.06281', '1701.07717'], ['1808.06281', '1711.10295'], ['1701.07717', '1808.06281'], ['1706.06978', '1611.00144'], ['1611.00144', '1706.06978'], ['1610.05256', '1506.07503'], ['1506.07503', '1610.05256'], ['1711.04903', '1805.02474']]","['sent: Article 1 uses multiple datasets:  PASCAL VOC 2012, and  PASCAL Context, while Article 2 uses Cityscapes dataset.', 'sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset. sent: Article 1 uses PASCAL Context dataset, while Article 2 uses ImageNet dataset.', 'sent: Article 1 uses Cityscapes dataset, while Article 2 uses PASCAL Context dataset.', 'sent: Article 1 uses Cityscapes dataset, while Article 2 uses multiple datasets:  PASCAL VOC 2012, and  PASCAL Context.', 'sent: Article 1 uses PASCAL Context dataset, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.', 'sent: Article 1 uses TriviaQA dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.', 'sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses TriviaQA dataset.', 'sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  WMT2015 English-German.', 'sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses multiple datasets:  WMT2016 Romanian-English,  WMT2016 English-Romanian,  IWSLT2015 English-German, and  WMT2014 German-English, while Article 2 uses multiple datasets:  WMT2014 English-French, and  WMT2015 English-German.', 'sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  WMT2015 English-German, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.', 'sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  WMT2015 English-German, while Article 2 uses IWSLT2015 German-English dataset.', 'sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-French, and  WMT2015 English-German, while Article 2 uses multiple datasets:  WMT2016 Romanian-English,  WMT2016 English-Romanian,  IWSLT2015 English-German, and  WMT2014 German-English.', 'sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  WMT2015 English-German.', 'sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.', 'sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.', 'sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.', 'sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.', 'sent: Article 1 and Article 2 use Amazon dataset.sent: Article 1 and Article 2 use MovieLens 20M dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Company ,  Dianping,  iPinYou,  Criteo, and  Bing News.', 'sent: Article 1 and Article 2 use Amazon dataset.sent: Article 1 and Article 2 use MovieLens 20M dataset. sent: Article 1 uses multiple datasets:  Company ,  Dianping,  iPinYou,  Criteo, and  Bing News, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses Switchboard   Hub500 dataset, while Article 2 uses TIMIT dataset.', 'sent: Article 1 uses TIMIT dataset, while Article 2 uses Switchboard   Hub500 dataset.', 'sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 uses UD dataset, while Article 2 uses multiple datasets:  MR,  IMDb, and  CoNLL 2003  English .']","['sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses Mapillary for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses Mapillary for Semantic Segmentation.', 'sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation.', 'sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation.', 'sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses ParseNet  for Semantic Segmentation. sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses ParseNet for Semantic Segmentation.', 'sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation.', 'sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses GA for Question Answering.', 'sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering.', 'sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation.', 'sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation.', 'sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation.', 'sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses DCCL for Machine Translation.', 'sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation.', 'sent: Article 1 uses DCCL approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation.', 'sent: Article 1 uses IDE    CamStyle   Random Erasing approach for Person Re-Identification, while Article 2 uses Incremental Learning for Person Re-Identification. sent: Article 1 uses IDE    CamStyle approach for Person Re-Identification, while Article 2 uses Incremental Learning for Person Re-Identification. sent: Article 1 uses IDE  approach for Person Re-Identification, while Article 2 uses Incremental Learning for Person Re-Identification.', 'sent: Article 1 uses Incremental Learning approach for Person Re-Identification, while Article 2 uses GAN for Person Re-Identification.', 'sent: Article 1 uses Incremental Learning approach for Person Re-Identification, while Article 2 uses IDE    CamStyle   Random Erasing for Person Re-Identification. sent: Article 1 uses Incremental Learning approach for Person Re-Identification, while Article 2 uses IDE    CamStyle for Person Re-Identification. sent: Article 1 uses Incremental Learning approach for Person Re-Identification, while Article 2 uses IDE  for Person Re-Identification.', 'sent: Article 1 uses GAN approach for Person Re-Identification, while Article 2 uses Incremental Learning for Person Re-Identification.', 'sent: Article 1 uses DIN approach for Click-Through Rate Prediction, while Article 2 uses IPNN for Click-Through Rate Prediction. sent: Article 1 uses DIN approach for Click-Through Rate Prediction, while Article 2 uses PNN for Click-Through Rate Prediction. sent: Article 1 uses DIN approach for Click-Through Rate Prediction, while Article 2 uses PNN  for Click-Through Rate Prediction. sent: Article 1 uses DIN approach for Click-Through Rate Prediction, while Article 2 uses OPNN for Click-Through Rate Prediction. sent: Article 1 uses DIN   Dice Activation approach for Click-Through Rate Prediction, while Article 2 uses IPNN for Click-Through Rate Prediction. sent: Article 1 uses DIN   Dice Activation approach for Click-Through Rate Prediction, while Article 2 uses PNN for Click-Through Rate Prediction. sent: Article 1 uses DIN   Dice Activation approach for Click-Through Rate Prediction, while Article 2 uses PNN  for Click-Through Rate Prediction. sent: Article 1 uses DIN   Dice Activation approach for Click-Through Rate Prediction, while Article 2 uses OPNN for Click-Through Rate Prediction.', 'sent: Article 1 uses IPNN approach for Click-Through Rate Prediction, while Article 2 uses DIN for Click-Through Rate Prediction. sent: Article 1 uses IPNN approach for Click-Through Rate Prediction, while Article 2 uses DIN   Dice Activation for Click-Through Rate Prediction. sent: Article 1 uses PNN approach for Click-Through Rate Prediction, while Article 2 uses DIN for Click-Through Rate Prediction. sent: Article 1 uses PNN approach for Click-Through Rate Prediction, while Article 2 uses DIN   Dice Activation for Click-Through Rate Prediction. sent: Article 1 uses PNN  approach for Click-Through Rate Prediction, while Article 2 uses DIN for Click-Through Rate Prediction. sent: Article 1 uses PNN  approach for Click-Through Rate Prediction, while Article 2 uses DIN   Dice Activation for Click-Through Rate Prediction. sent: Article 1 uses OPNN approach for Click-Through Rate Prediction, while Article 2 uses DIN for Click-Through Rate Prediction. sent: Article 1 uses OPNN approach for Click-Through Rate Prediction, while Article 2 uses DIN   Dice Activation for Click-Through Rate Prediction.', 'sent: Article 1 uses CNN-LSTM approach for Speech Recognition, while Article 2 uses Bi-RNN   Attention for Speech Recognition. sent: Article 1 uses Microsoft 2016b approach for Speech Recognition, while Article 2 uses Bi-RNN   Attention for Speech Recognition.', 'sent: Article 1 uses Bi-RNN   Attention approach for Speech Recognition, while Article 2 uses CNN-LSTM for Speech Recognition. sent: Article 1 uses Bi-RNN   Attention approach for Speech Recognition, while Article 2 uses Microsoft 2016b for Speech Recognition.', 'sent: Article 1 uses Adversarial Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses S-LSTM for Part-Of-Speech Tagging.']"
5,5,06_paper_01,06_paper_07,61,67,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses PubHealth sourced from websites and news/news reviews websites from health topics and biomedicine domain of 11832 claims.","sent: Article 1 uses TF-IDF approach for document retrieval. sent: Article 1 uses BERT approach for rationale selection, while Article 2 uses Sentence-BERT approach for rationale selection. sent: Article 1 uses BERT approach for verdict prediction, while Article 2 uses SciBERT approach for verdict prediction.","[['1804.09337', '1506.04579'], ['1804.09337', '1504.01013'], ['1506.04579', '1804.09337'], ['1504.01013', '1804.09337'], ['1511.06038', '1606.01549'], ['1606.01549', '1511.06038'], ['1512.02167', '1511.02274'], ['1511.06581', '1507.04296'], ['1507.04296', '1511.06581'], ['1606.02891', '1711.02132'], ['1711.02281', '1711.02132'], ['1711.02132', '1606.02891'], ['1711.02132', '1711.01068'], ['1711.02132', '1711.02281'], ['1711.10295', '1703.05693'], ['1703.05693', '1701.07717'], ['1703.05693', '1711.10295'], ['1701.07717', '1703.05693'], ['1610.05256', '1506.07503'], ['1506.07503', '1610.05256']]","['sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset. sent: Article 1 uses Cityscapes dataset, while Article 2 uses PASCAL Context dataset.', 'sent: Article 1 uses multiple datasets:  PASCAL VOC 2012, and  Cityscapes, while Article 2 uses PASCAL Context dataset.', 'sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset. sent: Article 1 uses PASCAL Context dataset, while Article 2 uses Cityscapes dataset.', 'sent: Article 1 uses PASCAL Context dataset, while Article 2 uses multiple datasets:  PASCAL VOC 2012, and  Cityscapes.', 'sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.', 'sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses multiple datasets:  WikiQA, and  QASent.', 'sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset. sent: Article 1 uses COCO Visual Question Answering  VQA  real images 1 0 multiple choice dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Atari 2600 Fishing Derby dataset.sent: Article 1 and Article 2 use Atari 2600 Atlantis dataset.sent: Article 1 and Article 2 use Atari 2600 Tennis dataset.sent: Article 1 and Article 2 use Atari 2600 Video Pinball dataset.sent: Article 1 and Article 2 use Atari 2600 Up and Down dataset.sent: Article 1 and Article 2 use Atari 2600 Gopher dataset.sent: Article 1 and Article 2 use Atari 2600 Double Dunk dataset.sent: Article 1 and Article 2 use Atari 2600 Assault dataset.sent: Article 1 and Article 2 use Atari 2600 Road Runner dataset.sent: Article 1 and Article 2 use Atari 2600 Private Eye dataset.sent: Article 1 and Article 2 use Atari 2600 Gravitar dataset.sent: Article 1 and Article 2 use Atari 2600 Breakout dataset.sent: Article 1 and Article 2 use Atari 2600 Krull dataset.sent: Article 1 and Article 2 use Atari 2600 HERO dataset.sent: Article 1 and Article 2 use Atari 2600 Ice Hockey dataset.sent: Article 1 and Article 2 use Atari 2600 Asterix dataset.sent: Article 1 and Article 2 use Atari 2600 Time Pilot dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Seaquest dataset.sent: Article 1 and Article 2 use Atari 2600 Name This Game dataset.sent: Article 1 and Article 2 use Atari 2600 Chopper Command dataset.sent: Article 1 and Article 2 use Atari 2600 Kangaroo dataset.sent: Article 1 and Article 2 use Atari 2600 Zaxxon dataset.sent: Article 1 and Article 2 use Atari 2600 Bowling dataset.sent: Article 1 and Article 2 use Atari 2600 Enduro dataset.sent: Article 1 and Article 2 use Atari 2600 Wizard of Wor dataset.sent: Article 1 and Article 2 use Atari 2600 Ms  Pacman dataset.sent: Article 1 and Article 2 use Atari 2600 Pong dataset.sent: Article 1 and Article 2 use Atari 2600 Star Gunner dataset.sent: Article 1 and Article 2 use Atari 2600 Space Invaders dataset.sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Bank Heist dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Alien dataset.sent: Article 1 and Article 2 use Atari 2600 Amidar dataset.sent: Article 1 and Article 2 use Atari 2600 Centipede dataset.sent: Article 1 and Article 2 use Atari 2600 Beam Rider dataset.sent: Article 1 and Article 2 use Atari 2600 Demon Attack dataset.sent: Article 1 and Article 2 use Atari 2600 Crazy Climber dataset.sent: Article 1 and Article 2 use Atari 2600 James Bond dataset.sent: Article 1 and Article 2 use Atari 2600 Kung-Fu Master dataset.sent: Article 1 and Article 2 use Atari 2600 Asteroids dataset.sent: Article 1 and Article 2 use Atari 2600 Battle Zone dataset.sent: Article 1 and Article 2 use Atari 2600 Tutankham dataset.sent: Article 1 and Article 2 use Atari 2600 Robotank dataset.sent: Article 1 and Article 2 use Atari 2600 Boxing dataset.sent: Article 1 and Article 2 use Atari 2600 River Raid dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 uses multiple datasets:  Atari 2600 Berzerk,  Atari 2600 Defender, and  Atari 2600 Phoenix, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Atari 2600 Fishing Derby dataset.sent: Article 1 and Article 2 use Atari 2600 Atlantis dataset.sent: Article 1 and Article 2 use Atari 2600 Tennis dataset.sent: Article 1 and Article 2 use Atari 2600 Video Pinball dataset.sent: Article 1 and Article 2 use Atari 2600 Up and Down dataset.sent: Article 1 and Article 2 use Atari 2600 Gopher dataset.sent: Article 1 and Article 2 use Atari 2600 Double Dunk dataset.sent: Article 1 and Article 2 use Atari 2600 Assault dataset.sent: Article 1 and Article 2 use Atari 2600 Road Runner dataset.sent: Article 1 and Article 2 use Atari 2600 Private Eye dataset.sent: Article 1 and Article 2 use Atari 2600 Gravitar dataset.sent: Article 1 and Article 2 use Atari 2600 Breakout dataset.sent: Article 1 and Article 2 use Atari 2600 Krull dataset.sent: Article 1 and Article 2 use Atari 2600 HERO dataset.sent: Article 1 and Article 2 use Atari 2600 Ice Hockey dataset.sent: Article 1 and Article 2 use Atari 2600 Asterix dataset.sent: Article 1 and Article 2 use Atari 2600 Time Pilot dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Seaquest dataset.sent: Article 1 and Article 2 use Atari 2600 Name This Game dataset.sent: Article 1 and Article 2 use Atari 2600 Chopper Command dataset.sent: Article 1 and Article 2 use Atari 2600 Kangaroo dataset.sent: Article 1 and Article 2 use Atari 2600 Zaxxon dataset.sent: Article 1 and Article 2 use Atari 2600 Bowling dataset.sent: Article 1 and Article 2 use Atari 2600 Enduro dataset.sent: Article 1 and Article 2 use Atari 2600 Wizard of Wor dataset.sent: Article 1 and Article 2 use Atari 2600 Ms  Pacman dataset.sent: Article 1 and Article 2 use Atari 2600 Pong dataset.sent: Article 1 and Article 2 use Atari 2600 Star Gunner dataset.sent: Article 1 and Article 2 use Atari 2600 Space Invaders dataset.sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Bank Heist dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Alien dataset.sent: Article 1 and Article 2 use Atari 2600 Amidar dataset.sent: Article 1 and Article 2 use Atari 2600 Centipede dataset.sent: Article 1 and Article 2 use Atari 2600 Beam Rider dataset.sent: Article 1 and Article 2 use Atari 2600 Demon Attack dataset.sent: Article 1 and Article 2 use Atari 2600 Crazy Climber dataset.sent: Article 1 and Article 2 use Atari 2600 James Bond dataset.sent: Article 1 and Article 2 use Atari 2600 Kung-Fu Master dataset.sent: Article 1 and Article 2 use Atari 2600 Asteroids dataset.sent: Article 1 and Article 2 use Atari 2600 Battle Zone dataset.sent: Article 1 and Article 2 use Atari 2600 Tutankham dataset.sent: Article 1 and Article 2 use Atari 2600 Robotank dataset.sent: Article 1 and Article 2 use Atari 2600 Boxing dataset.sent: Article 1 and Article 2 use Atari 2600 River Raid dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Atari 2600 Berzerk,  Atari 2600 Defender, and  Atari 2600 Phoenix.', 'sent: Article 1 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian, while Article 2 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French.', 'sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses multiple datasets:  WMT2016 Romanian-English,  WMT2016 English-Romanian,  IWSLT2015 English-German, and  WMT2014 German-English, while Article 2 uses WMT2014 English-French dataset.', 'sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French, while Article 2 uses multiple datasets:  WMT2016 Czech-English,  WMT2016 English-Russian,  WMT2016 German-English,  WMT2016 Romanian-English,  WMT2016 Russian-English,  WMT2016 English-Czech,  WMT2016 English-German, and  WMT2016 English-Romanian.', 'sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French, while Article 2 uses IWSLT2015 German-English dataset.', 'sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses multiple datasets:  WMT2016 Romanian-English,  WMT2016 English-Romanian,  IWSLT2015 English-German, and  WMT2014 German-English.', 'sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.', 'sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.', 'sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.', 'sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.', 'sent: Article 1 uses Switchboard   Hub500 dataset, while Article 2 uses TIMIT dataset.', 'sent: Article 1 uses TIMIT dataset, while Article 2 uses Switchboard   Hub500 dataset.']","['sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses ParseNet  for Semantic Segmentation. sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses ParseNet for Semantic Segmentation.', 'sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation.', 'sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses Smooth Network with Channel Attention Block for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses Smooth Network with Channel Attention Block for Semantic Segmentation.', 'sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses Smooth Network with Channel Attention Block for Semantic Segmentation.', 'sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses GA for Question Answering.', 'sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering.', 'sent: Article 1 uses iBOWIMG baseline approach for Visual Question Answering, while Article 2 uses SAN for Visual Question Answering.', 'sent: Article 1 uses Prior Duel hs approach for Atari Games, while Article 2 uses Gorila for Atari Games. sent: Article 1 uses Prior Duel noop approach for Atari Games, while Article 2 uses Gorila for Atari Games. sent: Article 1 uses Duel noop approach for Atari Games, while Article 2 uses Gorila for Atari Games. sent: Article 1 uses Duel hs approach for Atari Games, while Article 2 uses Gorila for Atari Games. sent: Article 1 uses DDQN  tuned  noop approach for Atari Games, while Article 2 uses Gorila for Atari Games.', 'sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses Prior Duel hs for Atari Games. sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses Prior Duel noop for Atari Games. sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses Duel noop for Atari Games. sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses Duel hs for Atari Games. sent: Article 1 uses Gorila approach for Atari Games, while Article 2 uses DDQN  tuned  noop for Atari Games.', 'sent: Article 1 uses Attentional encoder-decoder   BPE approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation. sent: Article 1 uses BiGRU approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation.', 'sent: Article 1 uses NAT  FT   NPD approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation.', 'sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses Attentional encoder-decoder   BPE for Machine Translation. sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses BiGRU for Machine Translation.', 'sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses DCCL for Machine Translation.', 'sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses NAT  FT   NPD for Machine Translation.', 'sent: Article 1 uses IDE    CamStyle   Random Erasing approach for Person Re-Identification, while Article 2 uses SVDNet for Person Re-Identification. sent: Article 1 uses IDE    CamStyle approach for Person Re-Identification, while Article 2 uses SVDNet for Person Re-Identification. sent: Article 1 uses IDE  approach for Person Re-Identification, while Article 2 uses SVDNet for Person Re-Identification.', 'sent: Article 1 uses SVDNet approach for Person Re-Identification, while Article 2 uses GAN for Person Re-Identification.', 'sent: Article 1 uses SVDNet approach for Person Re-Identification, while Article 2 uses IDE    CamStyle   Random Erasing for Person Re-Identification. sent: Article 1 uses SVDNet approach for Person Re-Identification, while Article 2 uses IDE    CamStyle for Person Re-Identification. sent: Article 1 uses SVDNet approach for Person Re-Identification, while Article 2 uses IDE  for Person Re-Identification.', 'sent: Article 1 uses GAN approach for Person Re-Identification, while Article 2 uses SVDNet for Person Re-Identification.', 'sent: Article 1 uses CNN-LSTM approach for Speech Recognition, while Article 2 uses Bi-RNN   Attention for Speech Recognition. sent: Article 1 uses Microsoft 2016b approach for Speech Recognition, while Article 2 uses Bi-RNN   Attention for Speech Recognition.', 'sent: Article 1 uses Bi-RNN   Attention approach for Speech Recognition, while Article 2 uses CNN-LSTM for Speech Recognition. sent: Article 1 uses Bi-RNN   Attention approach for Speech Recognition, while Article 2 uses Microsoft 2016b for Speech Recognition.']"
6,6,06_paper_01,06_paper_08,61,68,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses ClimateFever from climate related topics domain of 1500 sentences.","sent: Article 1 uses TF-IDF approach for document retrieval. sent: Article 1 uses BERT approach for rationale selection. sent: Article 1 uses BERT approach for verdict prediction, while Article 2 uses ClimateBERT approach for verdict prediction.","[['1511.06038', '1606.01549'], ['1606.01549', '1511.06038'], ['1606.01549', '1503.03244'], ['1503.03244', '1606.01549'], ['1711.04903', '1805.02474']]","['sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.', 'sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses multiple datasets:  WikiQA, and  QASent.', 'sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses SemEvalCQA dataset.', 'sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.', 'sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 uses UD dataset, while Article 2 uses multiple datasets:  MR,  IMDb, and  CoNLL 2003  English .']","['sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses GA for Question Answering.', 'sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering.', 'sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses ARC-II for Question Answering.', 'sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses GA for Question Answering.', 'sent: Article 1 uses Adversarial Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses S-LSTM for Part-Of-Speech Tagging.']"
7,7,06_paper_01,06_paper_09,61,69,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses HealthVer from health related domain of 14330 evidence-claim pairs.","sent: Article 1 uses TF-IDF approach for document retrieval. sent: Article 1 uses BERT approach for rationale selection. sent: Article 1 uses BERT approach for verdict prediction, while Article 2 uses T5-base approach for verdict prediction.","[['1506.04579', '1504.01013'], ['1504.01013', '1506.04579'], ['1506.08959', '1407.3867'], ['1407.3867', '1506.08959'], ['1706.06978', '1611.00144'], ['1706.06978', '1703.04247'], ['1611.00144', '1706.06978'], ['1703.04247', '1706.06978'], ['1610.05256', '1506.07503'], ['1506.07503', '1610.05256']]","['sent: Article 1 and Article 2 use PASCAL Context dataset. sent: Article 1 uses PASCAL VOC 2012 dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use PASCAL Context dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses PASCAL VOC 2012 dataset.', 'sent: Article 1 uses CompCars dataset, while Article 2 uses  CUB-200-2011 dataset.', 'sent: Article 1 uses  CUB-200-2011 dataset, while Article 2 uses CompCars dataset.', 'sent: Article 1 and Article 2 use Amazon dataset.sent: Article 1 and Article 2 use MovieLens 20M dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Company ,  Dianping,  iPinYou,  Criteo, and  Bing News.', 'sent: Article 1 and Article 2 use Amazon dataset.sent: Article 1 and Article 2 use MovieLens 20M dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Company ,  Bing News,  Dianping, and  Criteo.', 'sent: Article 1 and Article 2 use Amazon dataset.sent: Article 1 and Article 2 use MovieLens 20M dataset. sent: Article 1 uses multiple datasets:  Company ,  Dianping,  iPinYou,  Criteo, and  Bing News, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Amazon dataset.sent: Article 1 and Article 2 use MovieLens 20M dataset. sent: Article 1 uses multiple datasets:  Company ,  Bing News,  Dianping, and  Criteo, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses Switchboard   Hub500 dataset, while Article 2 uses TIMIT dataset.', 'sent: Article 1 uses TIMIT dataset, while Article 2 uses Switchboard   Hub500 dataset.']","['sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation.', 'sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses ParseNet  for Semantic Segmentation. sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses ParseNet for Semantic Segmentation.', 'sent: Article 1 uses AlexNet approach for Fine-Grained Image Classification, while Article 2 uses Part RCNN for Fine-Grained Image Classification. sent: Article 1 uses GoogLeNet approach for Fine-Grained Image Classification, while Article 2 uses Part RCNN for Fine-Grained Image Classification.', 'sent: Article 1 uses Part RCNN approach for Fine-Grained Image Classification, while Article 2 uses AlexNet for Fine-Grained Image Classification. sent: Article 1 uses Part RCNN approach for Fine-Grained Image Classification, while Article 2 uses GoogLeNet for Fine-Grained Image Classification.', 'sent: Article 1 uses DIN approach for Click-Through Rate Prediction, while Article 2 uses IPNN for Click-Through Rate Prediction. sent: Article 1 uses DIN approach for Click-Through Rate Prediction, while Article 2 uses PNN for Click-Through Rate Prediction. sent: Article 1 uses DIN approach for Click-Through Rate Prediction, while Article 2 uses PNN  for Click-Through Rate Prediction. sent: Article 1 uses DIN approach for Click-Through Rate Prediction, while Article 2 uses OPNN for Click-Through Rate Prediction. sent: Article 1 uses DIN   Dice Activation approach for Click-Through Rate Prediction, while Article 2 uses IPNN for Click-Through Rate Prediction. sent: Article 1 uses DIN   Dice Activation approach for Click-Through Rate Prediction, while Article 2 uses PNN for Click-Through Rate Prediction. sent: Article 1 uses DIN   Dice Activation approach for Click-Through Rate Prediction, while Article 2 uses PNN  for Click-Through Rate Prediction. sent: Article 1 uses DIN   Dice Activation approach for Click-Through Rate Prediction, while Article 2 uses OPNN for Click-Through Rate Prediction.', 'sent: Article 1 uses DIN approach for Click-Through Rate Prediction, while Article 2 uses DeepFM for Click-Through Rate Prediction. sent: Article 1 uses DIN   Dice Activation approach for Click-Through Rate Prediction, while Article 2 uses DeepFM for Click-Through Rate Prediction.', 'sent: Article 1 uses IPNN approach for Click-Through Rate Prediction, while Article 2 uses DIN for Click-Through Rate Prediction. sent: Article 1 uses IPNN approach for Click-Through Rate Prediction, while Article 2 uses DIN   Dice Activation for Click-Through Rate Prediction. sent: Article 1 uses PNN approach for Click-Through Rate Prediction, while Article 2 uses DIN for Click-Through Rate Prediction. sent: Article 1 uses PNN approach for Click-Through Rate Prediction, while Article 2 uses DIN   Dice Activation for Click-Through Rate Prediction. sent: Article 1 uses PNN  approach for Click-Through Rate Prediction, while Article 2 uses DIN for Click-Through Rate Prediction. sent: Article 1 uses PNN  approach for Click-Through Rate Prediction, while Article 2 uses DIN   Dice Activation for Click-Through Rate Prediction. sent: Article 1 uses OPNN approach for Click-Through Rate Prediction, while Article 2 uses DIN for Click-Through Rate Prediction. sent: Article 1 uses OPNN approach for Click-Through Rate Prediction, while Article 2 uses DIN   Dice Activation for Click-Through Rate Prediction.', 'sent: Article 1 uses DeepFM approach for Click-Through Rate Prediction, while Article 2 uses DIN for Click-Through Rate Prediction. sent: Article 1 uses DeepFM approach for Click-Through Rate Prediction, while Article 2 uses DIN   Dice Activation for Click-Through Rate Prediction.', 'sent: Article 1 uses CNN-LSTM approach for Speech Recognition, while Article 2 uses Bi-RNN   Attention for Speech Recognition. sent: Article 1 uses Microsoft 2016b approach for Speech Recognition, while Article 2 uses Bi-RNN   Attention for Speech Recognition.', 'sent: Article 1 uses Bi-RNN   Attention approach for Speech Recognition, while Article 2 uses CNN-LSTM for Speech Recognition. sent: Article 1 uses Bi-RNN   Attention approach for Speech Recognition, while Article 2 uses Microsoft 2016b for Speech Recognition.']"
8,8,06_paper_01,06_paper_10,61,610,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses CovidFact sourced from Sub-Reddit from health (Covid related)  domain.","sent: Article 1 uses TF-IDF approach for document retrieval, while Article 2 uses Google Search for document retrieval. sent: Article 1 uses BERT approach for rationale selection, while Article 2 uses Sentence-BERT approach for rationale selection. sent: Article 1 uses BERT approach for verdict prediction, while Article 2 uses RoBERTa (finetuned on GLUE dataset) approach for verdict prediction.","[['1506.04579', '1704.08545'], ['1704.08545', '1504.01013'], ['1704.08545', '1506.04579'], ['1512.02167', '1603.02814'], ['1605.08803', '1812.09916'], ['1706.06978', '1601.02376'], ['1601.02376', '1706.06978'], ['1711.04903', '1604.05529'], ['1604.05529', '1711.04903'], ['1403.6652', '1802.08352'], ['1802.08352', '1403.6652']]","['sent: Article 1 uses multiple datasets:  PASCAL VOC 2012, and  PASCAL Context, while Article 2 uses multiple datasets:  CamVid, and  Cityscapes.', 'sent: Article 1 uses multiple datasets:  CamVid, and  Cityscapes, while Article 2 uses PASCAL Context dataset.', 'sent: Article 1 uses multiple datasets:  CamVid, and  Cityscapes, while Article 2 uses multiple datasets:  PASCAL VOC 2012, and  PASCAL Context.', 'sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset. sent: Article 1 uses COCO Visual Question Answering  VQA  real images 1 0 multiple choice dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset.', 'sent: Article 1 uses multiple datasets:  Amazon, and  MovieLens 20M, while Article 2 uses multiple datasets:  Company ,  iPinYou, and  Criteo.', 'sent: Article 1 uses multiple datasets:  Company ,  iPinYou, and  Criteo, while Article 2 uses multiple datasets:  Amazon, and  MovieLens 20M.', 'sent: Article 1 and Article 2 use UD dataset.sent: Article 1 and Article 2 use Penn Treebank dataset.', 'sent: Article 1 and Article 2 use UD dataset.sent: Article 1 and Article 2 use Penn Treebank dataset.', 'sent: Article 1 and Article 2 use Citeseer dataset.sent: Article 1 and Article 2 use Pubmed dataset.sent: Article 1 and Article 2 use Cora dataset. sent: Article 1 uses multiple datasets:  BlogCatalog,  NELL, and  Wikipedia, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Citeseer dataset.sent: Article 1 and Article 2 use Pubmed dataset.sent: Article 1 and Article 2 use Cora dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  BlogCatalog,  NELL, and  Wikipedia.']","['sent: Article 1 uses ParseNet  approach for Semantic Segmentation, while Article 2 uses ICNet for Semantic Segmentation. sent: Article 1 uses ParseNet approach for Semantic Segmentation, while Article 2 uses ICNet for Semantic Segmentation.', 'sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation.', 'sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses ParseNet  for Semantic Segmentation. sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses ParseNet for Semantic Segmentation.', 'sent: Article 1 uses iBOWIMG baseline approach for Visual Question Answering, while Article 2 uses CNN-RNN for Visual Question Answering.', 'sent: Article 1 uses Real NVP approach for Image Generation, while Article 2 uses MMD-GAN-rep for Image Generation. sent: Article 1 uses PixelRNN approach for Image Generation, while Article 2 uses MMD-GAN-rep for Image Generation.', 'sent: Article 1 uses DIN approach for Click-Through Rate Prediction, while Article 2 uses FNN for Click-Through Rate Prediction. sent: Article 1 uses DIN   Dice Activation approach for Click-Through Rate Prediction, while Article 2 uses FNN for Click-Through Rate Prediction.', 'sent: Article 1 uses FNN approach for Click-Through Rate Prediction, while Article 2 uses DIN for Click-Through Rate Prediction. sent: Article 1 uses FNN approach for Click-Through Rate Prediction, while Article 2 uses DIN   Dice Activation for Click-Through Rate Prediction.', 'sent: Article 1 uses Adversarial Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses Bi-LSTM for Part-Of-Speech Tagging.', 'sent: Article 1 uses Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses Adversarial Bi-LSTM for Part-Of-Speech Tagging.', 'sent: Article 1 uses DeepWalk approach for Node Classification, while Article 2 uses alpha-LoNGAE for Node Classification.', 'sent: Article 1 uses alpha-LoNGAE approach for Node Classification, while Article 2 uses DeepWalk for Node Classification.']"
9,9,06_paper_02,06_paper_03,62,63,sent: Article 1 and Article 2 use SciFact sourced from S2ORC dataset. sent: Article 1 and Article 2 use dataset from diverse topics in biomedicine domain. sent: Article 1 and Article 2 use dataset of 1409 claims against 5183 abstracts.,"sent: Article 1 uses BioSentVec approach for document retrieval, while Article 2 uses BM25 combined with T5 as reranking approach for document retrieval. sent: Article 1 uses BERT combined with MLP (multi-layer perceptron) and  BERT combined with KGAT (Kernel Graph Attention Network) approach for rationale selection, while Article 2 uses T5 trained on MS MARCO approach for rationale selection. sent: Article 1 uses BERT combined with MLP (multi-layer perceptron) approach for stance prediction, while Article 2 uses uses T5 without fine tuining  approach for verdict prediction.","[['1603.01547', '1710.10723'], ['1710.10723', '1603.01547'], ['1901.11504', '1509.06664'], ['1707.02786', '1509.06664'], ['1603.06021', '1509.06664'], ['1509.06664', '1707.02786'], ['1509.06664', '1603.06021'], ['1509.06664', '1901.11504'], ['1603.06147', '1409.3215'], ['1603.06147', '1511.06732'], ['1409.3215', '1606.07947'], ['1409.3215', '1511.06732'], ['1409.3215', '1603.06147'], ['1606.07947', '1409.3215'], ['1606.07947', '1511.06732'], ['1511.06732', '1603.06147'], ['1511.06732', '1606.07947'], ['1603.03958', '1406.4773'], ['1811.02798', '1802.08352']]","['sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.', 'sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.', 'sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses WMT2014 English-French dataset.', 'sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses IWSLT2015 German-English dataset.', 'sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English.', 'sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses IWSLT2015 German-English dataset.', 'sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses WMT2015 English-German dataset.', 'sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English, while Article 2 uses WMT2014 English-French dataset.', 'sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English, while Article 2 uses IWSLT2015 German-English dataset.', 'sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses WMT2015 English-German dataset.', 'sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German, and  IWSLT2015 Thai-English.', 'sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.', 'sent: Article 1 and Article 2 use Citeseer dataset.sent: Article 1 and Article 2 use Pubmed dataset.sent: Article 1 and Article 2 use Cora dataset.']","['sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses S-Norm for Question Answering.', 'sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering.', 'sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference.', 'sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference.', 'sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference.', 'sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference.', 'sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference.', 'sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference.', 'sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation.', 'sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation.', 'sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation.', 'sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation.', 'sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation.', 'sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation.', 'sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation.', 'sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation.', 'sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation.', 'sent: Article 1 uses Template adaptation approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.', 'sent: Article 1 uses MTGAE approach for Node Classification, while Article 2 uses alpha-LoNGAE for Node Classification.']"
10,10,06_paper_02,06_paper_04,62,64,sent: Article 1 and Article 2 use SciFact sourced from S2ORC dataset. sent: Article 1 and Article 2 use dataset from diverse topics in biomedicine domain. sent: Article 1 and Article 2 use dataset of 1409 claims against 5183 abstracts.,"sent: Article 1 uses BioSentVec approach for document retrieval, while Article 2 uses BioSentVec approach for document retrieval. sent: Article 1 uses BERT combined with MLP (multi-layer perceptron) and  BERT combined with KGAT (Kernel Graph Attention Network) approach for rationale selection, while Article 2 uses BioBERT combined with MLP (multi-layer perceptron) approach for rationale selection. sent: Article 1 uses BERT combined with MLP (multi-layer perceptron) approach for stance prediction, while Article 2 uses BioBERT combined with MLP (multi-layer perceptron) approach for stance prediction.","[['1901.11504', '1603.06021'], ['1707.02786', '1603.06021'], ['1603.06021', '1901.11504'], ['1603.06021', '1707.02786'], ['1509.01626', '1811.09386'], ['1811.09386', '1509.01626'], ['1901.11504', '1811.09386'], ['1603.03958', '1406.4773'], ['1903.09359', '1808.01558'], ['1903.09359', '1804.03786'], ['1808.01558', '1804.03786']]","['sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset. sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification, and  Yelp Binary classification, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers.', 'sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset. sent: Article 1 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers, while Article 2 uses multiple datasets:  Yelp Fine-grained classification, and  Yelp Binary classification.', 'sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers.', 'sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.', 'sent: Article 1 uses AFLW2000-3D dataset, while Article 2 uses AFLW2000 dataset.', 'sent: Article 1 uses AFLW2000-3D dataset, while Article 2 uses AFLW2000 dataset.', 'sent: Article 1 and Article 2 use AFLW2000 dataset.']","['sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference.', 'sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference.', 'sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference.', 'sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference.', 'sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Text Classification, while Article 2 uses EXAM for Text Classification.', 'sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis. sent: Article 1 uses EXAM approach for Text Classification, while Article 2 uses Char-level CNN for Text Classification.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis.', 'sent: Article 1 uses Template adaptation approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.', 'sent: Article 1 uses 2DASL approach for Face Alignment, while Article 2 uses MCL for Face Alignment.', 'sent: Article 1 uses 2DASL approach for Face Alignment, while Article 2 uses Nonlinear 3D Face Morphable Model for Face Alignment.', 'sent: Article 1 uses MCL approach for Face Alignment, while Article 2 uses Nonlinear 3D Face Morphable Model for Face Alignment.']"
11,11,06_paper_02,06_paper_05,62,65,sent: Article 1 and Article 2 use SciFact sourced from S2ORC dataset. sent: Article 1 and Article 2 use dataset from diverse topics in biomedicine domain. sent: Article 1 and Article 2 use dataset of 1409 claims against 5183 abstracts.,"sent: Article 1 uses BioSentVec approach for document retrieval, while Article 2 uses BM25 combined with T5 as reranking approach for document retrieval. sent: Article 1 uses BERT combined with MLP (multi-layer perceptron) and  BERT combined with KGAT (Kernel Graph Attention Network) approach for rationale selection, while Article 2 uses longformer encoder approach for rationale selection. sent: Article 1 uses BERT combined with MLP (multi-layer perceptron) approach for stance prediction, while Article 2 uses BioBERT longformer encoder approach for verdict prediction.","[['1610.09027', '1710.10723'], ['1710.10723', '1610.09027'], ['1603.03958', '1406.4773'], ['1903.09359', '1808.01558'], ['1903.09359', '1804.03786'], ['1808.01558', '1804.01005'], ['1804.01005', '1804.03786']]","['sent: Article 1 uses bAbi dataset, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.', 'sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses bAbi dataset.', 'sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.', 'sent: Article 1 uses AFLW2000-3D dataset, while Article 2 uses AFLW2000 dataset.', 'sent: Article 1 uses AFLW2000-3D dataset, while Article 2 uses AFLW2000 dataset.', 'sent: Article 1 and Article 2 use AFLW2000 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  BIWI,  AFLW2000-3D, and  Florence.', 'sent: Article 1 and Article 2 use AFLW2000 dataset. sent: Article 1 uses multiple datasets:  BIWI,  AFLW2000-3D, and  Florence, while Article 2 does not use specific datasets for experiments.']","['sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses S-Norm for Question Answering.', 'sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses SDNC for Question Answering.', 'sent: Article 1 uses Template adaptation approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.', 'sent: Article 1 uses 2DASL approach for Face Alignment, while Article 2 uses MCL for Face Alignment.', 'sent: Article 1 uses 2DASL approach for Face Alignment, while Article 2 uses Nonlinear 3D Face Morphable Model for Face Alignment.', 'sent: Article 1 uses MCL approach for Face Alignment, while Article 2 uses 3DDFA for Face Alignment. sent: Article 1 uses MCL approach for Face Alignment, while Article 2 uses 3DDFA   SDM for Face Alignment.', 'sent: Article 1 uses 3DDFA approach for Face Alignment, while Article 2 uses Nonlinear 3D Face Morphable Model for Face Alignment. sent: Article 1 uses 3DDFA   SDM approach for Face Alignment, while Article 2 uses Nonlinear 3D Face Morphable Model for Face Alignment.']"
12,12,06_paper_02,06_paper_06,62,66,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses CoVERT sourced from fact-checked tweets and evidence from online resources from medical domain of 300 tweets.","sent: Article 1 uses BioSentVec approach for document retrieval, while Article 2 uses BM25 combined with T5 as reranking approach for document retrieval. sent: Article 1 uses BERT combined with MLP (multi-layer perceptron) and  BERT combined with KGAT (Kernel Graph Attention Network) approach for rationale selection, while Article 2 uses longformer encoder approach for rationale selection. sent:Article 1 uses BERT combined with MLP (multi-layer perceptron) approach for stance prediction, while Article 2 uses longformer encoder approach for verdict prediction.","[['1706.02596', '1710.10723'], ['1710.10723', '1706.02596'], ['1901.11504', '1805.02474'], ['1805.02474', '1509.01626'], ['1509.01626', '1805.02474'], ['1603.06147', '1610.10099'], ['1606.07947', '1610.10099'], ['1610.10099', '1511.06732'], ['1610.10099', '1603.06147'], ['1610.10099', '1606.07947'], ['1511.06732', '1610.10099'], ['1603.03958', '1406.4773']]","['sent: Article 1 and Article 2 use TriviaQA dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses SQuAD1 1 dataset.', 'sent: Article 1 and Article 2 use TriviaQA dataset. sent: Article 1 uses SQuAD1 1 dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.', 'sent: Article 1 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification.', 'sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.', 'sent: Article 1 and Article 2 use WMT2015 English-German dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French.', 'sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses IWSLT2015 Thai-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-French, and  WMT2015 English-German.', 'sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  WMT2015 English-German, while Article 2 uses IWSLT2015 German-English dataset.', 'sent: Article 1 and Article 2 use WMT2015 English-German dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-French, and  WMT2015 English-German, while Article 2 uses IWSLT2015 Thai-English dataset.', 'sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  WMT2015 English-German.', 'sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.']","['sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses S-Norm for Question Answering.', 'sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis.', 'sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis.', 'sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis.', 'sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation.', 'sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation.', 'sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation.', 'sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation.', 'sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation.', 'sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation.', 'sent: Article 1 uses Template adaptation approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.']"
13,13,06_paper_02,06_paper_07,62,67,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses PubHealth sourced from websites and news/news reviews websites from health topics and biomedicine domain of 11832 claims.","sent: Article 1 uses BioSentVec approach for document retrieval. sent: Article 1 uses BERT combined with MLP (multi-layer perceptron) and  BERT combined with KGAT (Kernel Graph Attention Network) approach for rationale selection, while Article 2 uses Sentence-BERT approach for rationale selection. sent: Article 1 uses BERT combined with MLP (multi-layer perceptron) approach for stance prediction, while Article 2 uses SciBERT approach for verdict prediction.","[['1511.06038', '1710.10723'], ['1606.01549', '1710.10723'], ['1710.10723', '1606.01549'], ['1710.10723', '1511.06038'], ['1901.11504', '1707.02786'], ['1901.11504', '1603.06021'], ['1707.02786', '1901.11504'], ['1603.06021', '1901.11504'], ['1901.11504', '1509.01626'], ['1603.06147', '1711.02132'], ['1606.07947', '1711.02132'], ['1711.02132', '1511.06732'], ['1711.02132', '1603.06147'], ['1711.02132', '1606.07947'], ['1511.06732', '1711.02132'], ['1603.05474', '1603.03958'], ['1603.05474', '1406.4773'], ['1808.01558', '1804.03786']]","['sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.', 'sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.', 'sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.', 'sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses multiple datasets:  WikiQA, and  QASent.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.', 'sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification.', 'sent: Article 1 uses WMT2015 English-German dataset, while Article 2 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French.', 'sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses IWSLT2015 Thai-English dataset, while Article 2 uses WMT2014 English-French dataset.', 'sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French, while Article 2 uses IWSLT2015 German-English dataset.', 'sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French, while Article 2 uses WMT2015 English-German dataset.', 'sent: Article 1 and Article 2 use WMT2014 English-German dataset. sent: Article 1 uses WMT2014 English-French dataset, while Article 2 uses IWSLT2015 Thai-English dataset.', 'sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French.', 'sent: Article 1 and Article 2 use IJB-A dataset.', 'sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.', 'sent: Article 1 and Article 2 use AFLW2000 dataset.']","['sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses S-Norm for Question Answering.', 'sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses S-Norm for Question Answering.', 'sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses GA for Question Answering.', 'sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering.', 'sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference.', 'sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference.', 'sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference.', 'sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis.', 'sent: Article 1 uses Enc-Dec Att  char  approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation. sent: Article 1 uses Enc-Dec Att  BPE  approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation.', 'sent: Article 1 uses Seq-KD   Seq-Inter   Word-KD approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation.', 'sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation.', 'sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses Enc-Dec Att  char  for Machine Translation. sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses Enc-Dec Att  BPE  for Machine Translation.', 'sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses Seq-KD   Seq-Inter   Word-KD for Machine Translation.', 'sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation.', 'sent: Article 1 uses NAN approach for Face Verification, while Article 2 uses Template adaptation for Face Verification.', 'sent: Article 1 uses NAN approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.', 'sent: Article 1 uses MCL approach for Face Alignment, while Article 2 uses Nonlinear 3D Face Morphable Model for Face Alignment.']"
14,14,06_paper_02,06_paper_08,62,68,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses ClimateFever from climate related topics domain of 1500 sentences.","sent: Article 1 uses BioSentVec approach for document retrieval. sent:Article 1 uses BERT combined with MLP (multi-layer perceptron) and  BERT combined with KGAT (Kernel Graph Attention Network) approach for rationale selection. sent:  Article 1 uses BERT combined with MLP (multi-layer perceptron) approach for stance prediction, while Article 2 uses ClimateBERT approach for verdict prediction.","[['1511.06038', '1710.10723'], ['1606.01549', '1710.10723'], ['1710.10723', '1503.03244'], ['1710.10723', '1606.01549'], ['1710.10723', '1511.06038'], ['1503.03244', '1710.10723'], ['1901.11504', '1707.02786'], ['1901.11504', '1802.05577'], ['1901.11504', '1603.06021'], ['1901.11504', '1508.05326'], ['1707.02786', '1802.05577'], ['1707.02786', '1603.06021'], ['1707.02786', '1901.11504'], ['1707.02786', '1508.05326'], ['1802.05577', '1603.06021'], ['1802.05577', '1901.11504'], ['1802.05577', '1707.02786'], ['1603.06021', '1901.11504'], ['1603.06021', '1508.05326'], ['1603.06021', '1707.02786'], ['1603.06021', '1802.05577'], ['1508.05326', '1707.02786'], ['1508.05326', '1603.06021'], ['1508.05326', '1901.11504'], ['1509.01626', '1811.09386'], ['1509.01626', '1607.01759'], ['1811.09386', '1509.01626'], ['1607.01759', '1509.01626'], ['1607.01759', '1901.11504'], ['1901.11504', '1805.02474'], ['1901.11504', '1811.09386'], ['1901.11504', '1509.01626'], ['1901.11504', '1412.1058'], ['1901.11504', '1607.01759'], ['1901.11504', '1812.01207'], ['1805.02474', '1509.01626'], ['1509.01626', '1412.1058'], ['1509.01626', '1805.02474'], ['1509.01626', '1812.01207'], ['1412.1058', '1509.01626'], ['1808.01558', '1804.03786']]","['sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.', 'sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.', 'sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses SemEvalCQA dataset.', 'sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.', 'sent: Article 1 uses multiple datasets:  TriviaQA, and  SQuAD1 1, while Article 2 uses multiple datasets:  WikiQA, and  QASent.', 'sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses multiple datasets:  TriviaQA, and  SQuAD1 1.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.', 'sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset. sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification, and  Yelp Binary classification, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers.', 'sent: Article 1 and Article 2 use Yelp Fine-grained classification dataset.sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset.sent: Article 1 and Article 2 use Yelp Binary classification dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Sogou News,  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers.', 'sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset. sent: Article 1 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers, while Article 2 uses multiple datasets:  Yelp Fine-grained classification, and  Yelp Binary classification.', 'sent: Article 1 and Article 2 use Yelp Fine-grained classification dataset.sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset.sent: Article 1 and Article 2 use Yelp Binary classification dataset. sent: Article 1 uses multiple datasets:  Sogou News,  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers, while Article 2 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification.', 'sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.', 'sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers.', 'sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification.', 'sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses IMDb dataset.', 'sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers.', 'sent: Article 1 and Article 2 use SST-2 Binary classification dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  Quora Question Pairs,  SciTail, and  SNLI, while Article 2 uses SemEval 2018 Task 1E-c dataset.', 'sent: Article 1 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification.', 'sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification, while Article 2 uses IMDb dataset.', 'sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.', 'sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification, while Article 2 uses multiple datasets:  SST-2 Binary classification, and  SemEval 2018 Task 1E-c.', 'sent: Article 1 uses IMDb dataset, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification.', 'sent: Article 1 and Article 2 use AFLW2000 dataset.']","['sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses S-Norm for Question Answering.', 'sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses S-Norm for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses S-Norm for Question Answering.', 'sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses ARC-II for Question Answering.', 'sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses GA for Question Answering.', 'sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses BiDAF   Self Attention  single model  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses S-Norm approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering.', 'sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses BiDAF   Self Attention  single model  for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses S-Norm for Question Answering.', 'sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference.', 'sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference.', 'sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference.', 'sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference.', 'sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference.', 'sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference.', 'sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference.', 'sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses 600D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses 300D Gumbel TreeLSTM encoders approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference.', 'sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference.', 'sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference.', 'sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference.', 'sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference.', 'sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference.', 'sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference.', 'sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference.', 'sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses 600D Gumbel TreeLSTM encoders for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses 300D Gumbel TreeLSTM encoders for Natural Language Inference.', 'sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference.', 'sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference.', 'sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Text Classification, while Article 2 uses EXAM for Text Classification.', 'sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Text Classification, while Article 2 uses FastText for Text Classification. sent: Article 1 uses Char-level CNN approach for Text Classification, while Article 2 uses fastText for Text Classification.', 'sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis. sent: Article 1 uses EXAM approach for Text Classification, while Article 2 uses Char-level CNN for Text Classification.', 'sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis. sent: Article 1 uses FastText approach for Text Classification, while Article 2 uses Char-level CNN for Text Classification. sent: Article 1 uses fastText approach for Text Classification, while Article 2 uses Char-level CNN for Text Classification.', 'sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses MT-DNN for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses MT-DNN for Sentiment Analysis.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses seq2-bown-CNN for Sentiment Analysis.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis.', 'sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis.', 'sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses seq2-bown-CNN for Sentiment Analysis.', 'sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis.', 'sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis.', 'sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis.', 'sent: Article 1 uses MCL approach for Face Alignment, while Article 2 uses Nonlinear 3D Face Morphable Model for Face Alignment.']"
15,15,06_paper_02,06_paper_09,62,69,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses HealthVer from health related domain of 14330 evidence-claim pairs.","sent: Article 1 uses BioSentVec approach for document retrieval. sent: Article 1 uses BERT combined with MLP (multi-layer perceptron) and  BERT combined with KGAT (Kernel Graph Attention Network) approach for rationale selection. sent: Article 1 uses BERT combined with MLP (multi-layer perceptron) approach for stance prediction, while Article 2 uses T5-base approach for verdict prediction.","[['1509.01626', '1607.01759'], ['1607.01759', '1509.01626'], ['1607.01759', '1901.11504'], ['1901.11504', '1607.01759'], ['1506.08959', '1407.3867'], ['1407.3867', '1506.08959'], ['1403.6652', '1811.02798'], ['1811.02798', '1403.6652']]","['sent: Article 1 and Article 2 use Yelp Fine-grained classification dataset.sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset.sent: Article 1 and Article 2 use Yelp Binary classification dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Sogou News,  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers.', 'sent: Article 1 and Article 2 use Yelp Fine-grained classification dataset.sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset.sent: Article 1 and Article 2 use Yelp Binary classification dataset. sent: Article 1 uses multiple datasets:  Sogou News,  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers, while Article 2 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification.', 'sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers.', 'sent: Article 1 uses CompCars dataset, while Article 2 uses  CUB-200-2011 dataset.', 'sent: Article 1 uses  CUB-200-2011 dataset, while Article 2 uses CompCars dataset.', 'sent: Article 1 and Article 2 use Citeseer dataset.sent: Article 1 and Article 2 use Pubmed dataset.sent: Article 1 and Article 2 use Cora dataset. sent: Article 1 uses multiple datasets:  BlogCatalog,  NELL, and  Wikipedia, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Citeseer dataset.sent: Article 1 and Article 2 use Pubmed dataset.sent: Article 1 and Article 2 use Cora dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  BlogCatalog,  NELL, and  Wikipedia.']","['sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Text Classification, while Article 2 uses FastText for Text Classification. sent: Article 1 uses Char-level CNN approach for Text Classification, while Article 2 uses fastText for Text Classification.', 'sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis. sent: Article 1 uses FastText approach for Text Classification, while Article 2 uses Char-level CNN for Text Classification. sent: Article 1 uses fastText approach for Text Classification, while Article 2 uses Char-level CNN for Text Classification.', 'sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses MT-DNN for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses MT-DNN for Sentiment Analysis.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis.', 'sent: Article 1 uses AlexNet approach for Fine-Grained Image Classification, while Article 2 uses Part RCNN for Fine-Grained Image Classification. sent: Article 1 uses GoogLeNet approach for Fine-Grained Image Classification, while Article 2 uses Part RCNN for Fine-Grained Image Classification.', 'sent: Article 1 uses Part RCNN approach for Fine-Grained Image Classification, while Article 2 uses AlexNet for Fine-Grained Image Classification. sent: Article 1 uses Part RCNN approach for Fine-Grained Image Classification, while Article 2 uses GoogLeNet for Fine-Grained Image Classification.', 'sent: Article 1 uses DeepWalk approach for Node Classification, while Article 2 uses MTGAE for Node Classification.', 'sent: Article 1 uses MTGAE approach for Node Classification, while Article 2 uses DeepWalk for Node Classification.']"
16,16,06_paper_02,06_paper_10,62,610,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses CovidFact sourced from Sub-Reddit from health (Covid related)  domain.","sent: Article 1 uses BioSentVec approach for document retrieval, while Article 2 uses Google Search for document retrieval. sent: Article 1 uses BERT combined with MLP (multi-layer perceptron) and  BERT combined with KGAT (Kernel Graph Attention Network) approach for rationale selection, while Article 2 uses Sentence-BERT approach for rationale selection. sent: Article 1 uses BERT combined with MLP (multi-layer perceptron) approach for stance prediction, while Article 2 uses RoBERTa (finetuned on GLUE dataset) approach for verdict prediction.","[['1603.03958', '1406.4773'], ['1811.02798', '1802.08352']]","['sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.', 'sent: Article 1 and Article 2 use Citeseer dataset.sent: Article 1 and Article 2 use Pubmed dataset.sent: Article 1 and Article 2 use Cora dataset.']","['sent: Article 1 uses Template adaptation approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.', 'sent: Article 1 uses MTGAE approach for Node Classification, while Article 2 uses alpha-LoNGAE for Node Classification.']"
17,17,06_paper_03,06_paper_04,63,64,sent: Article 1 and Article 2 use SciFact sourced from S2ORC dataset. sent: Article 1 and Article 2 use dataset from diverse topics in biomedicine domain. sent: Article 1 and Article 2 use dataset of 1409 claims against 5183 abstracts.,"sent: Article 1 uses BM25 combined with T5 as reranking approach for document retrieval, while Article 2 uses BioSentVec approach for document retrieval. sent: Article 1 uses T5 trained on MS MARCO approach for rationale selection, while Article 2 uses BioBERT combined with MLP (multi-layer perceptron) approach for rationale selection. sent: Article 1 uses uses T5 without fine tuining  approach for verdict prediction, while Article 2 uses BioBERT combined with MLP (multi-layer perceptron) approach for stance prediction.","[['1603.06021', '1509.06664'], ['1509.06664', '1603.06021'], ['1604.04112', '1506.03767'], ['1604.04112', '1512.03385'], ['1506.03767', '1604.04112'], ['1506.03767', '1503.04596'], ['1512.03385', '1604.04112'], ['1512.03385', '1503.04596'], ['1503.04596', '1512.03385'], ['1603.03958', '1406.4773'], ['1801.01641', '1406.3676'], ['1406.3676', '1801.01641']]","['sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  SVHN, and  MNIST.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses CIFAR-100 dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses multiple datasets:  SVHN, and  MNIST.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  MNIST, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.', 'sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.', 'sent: Article 1 uses TrecQA dataset, while Article 2 uses WebQuestions dataset.', 'sent: Article 1 uses WebQuestions dataset, while Article 2 uses TrecQA dataset.']","['sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference.', 'sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference.', 'sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses Spectral Representations for Convolutional Neural Networks for Image Classification.', 'sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification.', 'sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification.', 'sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses FLSCNN for Image Classification.', 'sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification.', 'sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses FLSCNN for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses FLSCNN for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses FLSCNN for Image Classification.', 'sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification.', 'sent: Article 1 uses Template adaptation approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.', 'sent: Article 1 uses aNMM approach for Question Answering, while Article 2 uses Subgraph embeddings for Question Answering.', 'sent: Article 1 uses Subgraph embeddings approach for Question Answering, while Article 2 uses aNMM for Question Answering.']"
18,18,06_paper_03,06_paper_05,63,65,sent: Article 1 and Article 2 use SciFact sourced from S2ORC dataset. sent: Article 1 and Article 2 use dataset from diverse topics in biomedicine domain. sent: Article 1 and Article 2 use dataset of 1409 claims against 5183 abstracts.,"sent: Article 1 and Article 2 use BM25 combined with T5 as reranking approach for document retrieval. sent: Article 1 uses T5 trained on MS MARCO approach for rationale selection, while Article 2 uses longformer encoder approach for rationale selection. sent: Article 1 uses T5 without fine tuining  approach for verdict prediction, while Article 2 uses BioBERT longformer encoder approach for verdict prediction. ","[['1603.01547', '1610.09027'], ['1610.09027', '1603.01547'], ['1603.03958', '1406.4773']]","['sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test, while Article 2 uses bAbi dataset.', 'sent: Article 1 uses bAbi dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test.', 'sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.']","['sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses SDNC for Question Answering.', 'sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering.', 'sent: Article 1 uses Template adaptation approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.']"
19,19,06_paper_03,06_paper_06,63,66,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses CoVERT sourced from fact-checked tweets and evidence from online resources from medical domain of 300 tweets.","sent: Article 1 uses BM25 combined with T5 as reranking approach for document retrieval, while Article 2 uses BM25 combined with T5 as reranking approach for document retrieval. sent: Article 1 uses T5 trained on MS MARCO approach for rationale selection, while Article 2 uses longformer encoder approach for rationale selection. sent: Article 1 uses T5 without fine tuining  approach for verdict prediction, while Article 2 uses longformer encoder approach for verdict prediction.","[['1603.01547', '1706.02596'], ['1706.02596', '1603.01547'], ['1604.04112', '1707.02968'], ['1604.04112', '1603.05027'], ['1707.02968', '1604.04112'], ['1707.02968', '1503.04596'], ['1603.05027', '1503.04596'], ['1503.04596', '1707.02968'], ['1503.04596', '1603.05027'], ['1409.3215', '1610.10099'], ['1610.10099', '1409.3215'], ['1610.10099', '1511.06732'], ['1511.06732', '1610.10099'], ['1603.03958', '1406.4773']]","['sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test, while Article 2 uses TriviaQA dataset.', 'sent: Article 1 uses TriviaQA dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test.', 'sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.', 'sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.', 'sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  MNIST.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  SVHN, and  MNIST.', 'sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN, and  MNIST, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  MNIST, while Article 2 uses CIFAR-100 dataset.', 'sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  WMT2014 English-German, and  WMT2015 English-German.', 'sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  WMT2015 English-German, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  WMT2015 English-German, while Article 2 uses IWSLT2015 German-English dataset.', 'sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German,  WMT2014 English-French, and  WMT2015 English-German.', 'sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.']","['sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering.', 'sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering.', 'sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification.', 'sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification.', 'sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification.', 'sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses FLSCNN for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses FLSCNN for Image Classification.', 'sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses FLSCNN for Image Classification.', 'sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification.', 'sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification.', 'sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation.', 'sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation.', 'sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation.', 'sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation.', 'sent: Article 1 uses Template adaptation approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.']"
20,20,06_paper_03,06_paper_07,63,67,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses PubHealth sourced from websites and news/news reviews websites from health topics and biomedicine domain of 11832 claims.","sent: Article 1 uses BM25 combined with T5 as reranking approach for document retrieval. sent: Article 1 uses T5 trained on MS MARCO approach for rationale selection, while Article 2 uses Sentence-BERT approach for rationale selection. sent: sent: Article 1 uses T5 without fine tuining  approach for verdict prediction, while Article 2 uses SciBERT approach for verdict prediction.","[['1511.06038', '1603.01547'], ['1603.01547', '1511.06038'], ['1603.01547', '1606.01549'], ['1606.01549', '1603.01547'], ['1901.11504', '1509.06664'], ['1509.06664', '1901.11504'], ['1604.04112', '1506.03767'], ['1604.04112', '1512.03385'], ['1604.04112', '1611.05431'], ['1506.03767', '1604.04112'], ['1506.03767', '1503.04596'], ['1512.03385', '1604.04112'], ['1512.03385', '1503.04596'], ['1503.04596', '1611.05431'], ['1503.04596', '1512.03385'], ['1611.05431', '1503.04596'], ['1509.06461', '1511.06581'], ['1511.06581', '1509.06461'], ['1409.3215', '1711.02132'], ['1711.02132', '1409.3215'], ['1711.02132', '1511.06732'], ['1511.06732', '1711.02132'], ['1603.05474', '1603.03958'], ['1603.05474', '1406.4773']]","['sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test.', 'sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test, while Article 2 uses multiple datasets:  WikiQA, and  QASent.', 'sent: Article 1 and Article 2 use CNN   Daily Mail dataset.sent: Article 1 and Article 2 use Children s Book Test dataset. sent: Article 1 uses SearchQA dataset, while Article 2 uses Quasar dataset.', 'sent: Article 1 and Article 2 use CNN   Daily Mail dataset.sent: Article 1 and Article 2 use Children s Book Test dataset. sent: Article 1 uses Quasar dataset, while Article 2 uses SearchQA dataset.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.', 'sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses ImageNet dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  SVHN, and  MNIST.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses CIFAR-100 dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses multiple datasets:  SVHN, and  MNIST.', 'sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN, and  MNIST, while Article 2 uses ImageNet dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  MNIST, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.', 'sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  MNIST.', 'sent: Article 1 and Article 2 use Atari 2600 Fishing Derby dataset.sent: Article 1 and Article 2 use Atari 2600 Atlantis dataset.sent: Article 1 and Article 2 use Atari 2600 Tennis dataset.sent: Article 1 and Article 2 use Atari 2600 Video Pinball dataset.sent: Article 1 and Article 2 use Atari 2600 Up and Down dataset.sent: Article 1 and Article 2 use Atari 2600 Gopher dataset.sent: Article 1 and Article 2 use Atari 2600 Double Dunk dataset.sent: Article 1 and Article 2 use Atari 2600 Assault dataset.sent: Article 1 and Article 2 use Atari 2600 Road Runner dataset.sent: Article 1 and Article 2 use Atari 2600 Berzerk dataset.sent: Article 1 and Article 2 use Atari 2600 Private Eye dataset.sent: Article 1 and Article 2 use Atari 2600 Gravitar dataset.sent: Article 1 and Article 2 use Atari 2600 Breakout dataset.sent: Article 1 and Article 2 use Atari 2600 Krull dataset.sent: Article 1 and Article 2 use Atari 2600 HERO dataset.sent: Article 1 and Article 2 use Atari 2600 Ice Hockey dataset.sent: Article 1 and Article 2 use Atari 2600 Asterix dataset.sent: Article 1 and Article 2 use Atari 2600 Time Pilot dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Seaquest dataset.sent: Article 1 and Article 2 use Atari 2600 Name This Game dataset.sent: Article 1 and Article 2 use Atari 2600 Chopper Command dataset.sent: Article 1 and Article 2 use Atari 2600 Kangaroo dataset.sent: Article 1 and Article 2 use Atari 2600 Zaxxon dataset.sent: Article 1 and Article 2 use Atari 2600 Bowling dataset.sent: Article 1 and Article 2 use Atari 2600 Enduro dataset.sent: Article 1 and Article 2 use Atari 2600 Wizard of Wor dataset.sent: Article 1 and Article 2 use Atari 2600 Ms  Pacman dataset.sent: Article 1 and Article 2 use Atari 2600 Pong dataset.sent: Article 1 and Article 2 use Atari 2600 Star Gunner dataset.sent: Article 1 and Article 2 use Atari 2600 Space Invaders dataset.sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Bank Heist dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Alien dataset.sent: Article 1 and Article 2 use Atari 2600 Amidar dataset.sent: Article 1 and Article 2 use Atari 2600 Centipede dataset.sent: Article 1 and Article 2 use Atari 2600 Beam Rider dataset.sent: Article 1 and Article 2 use Atari 2600 Demon Attack dataset.sent: Article 1 and Article 2 use Atari 2600 Crazy Climber dataset.sent: Article 1 and Article 2 use Atari 2600 James Bond dataset.sent: Article 1 and Article 2 use Atari 2600 Kung-Fu Master dataset.sent: Article 1 and Article 2 use Atari 2600 Asteroids dataset.sent: Article 1 and Article 2 use Atari 2600 Battle Zone dataset.sent: Article 1 and Article 2 use Atari 2600 Tutankham dataset.sent: Article 1 and Article 2 use Atari 2600 Robotank dataset.sent: Article 1 and Article 2 use Atari 2600 Boxing dataset.sent: Article 1 and Article 2 use Atari 2600 River Raid dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Atari 2600 Phoenix, and  Atari 2600 Defender.', 'sent: Article 1 and Article 2 use Atari 2600 Fishing Derby dataset.sent: Article 1 and Article 2 use Atari 2600 Atlantis dataset.sent: Article 1 and Article 2 use Atari 2600 Tennis dataset.sent: Article 1 and Article 2 use Atari 2600 Video Pinball dataset.sent: Article 1 and Article 2 use Atari 2600 Up and Down dataset.sent: Article 1 and Article 2 use Atari 2600 Gopher dataset.sent: Article 1 and Article 2 use Atari 2600 Double Dunk dataset.sent: Article 1 and Article 2 use Atari 2600 Assault dataset.sent: Article 1 and Article 2 use Atari 2600 Road Runner dataset.sent: Article 1 and Article 2 use Atari 2600 Berzerk dataset.sent: Article 1 and Article 2 use Atari 2600 Private Eye dataset.sent: Article 1 and Article 2 use Atari 2600 Gravitar dataset.sent: Article 1 and Article 2 use Atari 2600 Breakout dataset.sent: Article 1 and Article 2 use Atari 2600 Krull dataset.sent: Article 1 and Article 2 use Atari 2600 HERO dataset.sent: Article 1 and Article 2 use Atari 2600 Ice Hockey dataset.sent: Article 1 and Article 2 use Atari 2600 Asterix dataset.sent: Article 1 and Article 2 use Atari 2600 Time Pilot dataset.sent: Article 1 and Article 2 use Atari 2600 Freeway dataset.sent: Article 1 and Article 2 use Atari 2600 Seaquest dataset.sent: Article 1 and Article 2 use Atari 2600 Name This Game dataset.sent: Article 1 and Article 2 use Atari 2600 Chopper Command dataset.sent: Article 1 and Article 2 use Atari 2600 Kangaroo dataset.sent: Article 1 and Article 2 use Atari 2600 Zaxxon dataset.sent: Article 1 and Article 2 use Atari 2600 Bowling dataset.sent: Article 1 and Article 2 use Atari 2600 Enduro dataset.sent: Article 1 and Article 2 use Atari 2600 Wizard of Wor dataset.sent: Article 1 and Article 2 use Atari 2600 Ms  Pacman dataset.sent: Article 1 and Article 2 use Atari 2600 Pong dataset.sent: Article 1 and Article 2 use Atari 2600 Star Gunner dataset.sent: Article 1 and Article 2 use Atari 2600 Space Invaders dataset.sent: Article 1 and Article 2 use Atari 2600 Venture dataset.sent: Article 1 and Article 2 use Atari 2600 Q Bert dataset.sent: Article 1 and Article 2 use Atari 2600 Bank Heist dataset.sent: Article 1 and Article 2 use Atari 2600 Montezuma s Revenge dataset.sent: Article 1 and Article 2 use Atari 2600 Alien dataset.sent: Article 1 and Article 2 use Atari 2600 Amidar dataset.sent: Article 1 and Article 2 use Atari 2600 Centipede dataset.sent: Article 1 and Article 2 use Atari 2600 Beam Rider dataset.sent: Article 1 and Article 2 use Atari 2600 Demon Attack dataset.sent: Article 1 and Article 2 use Atari 2600 Crazy Climber dataset.sent: Article 1 and Article 2 use Atari 2600 James Bond dataset.sent: Article 1 and Article 2 use Atari 2600 Kung-Fu Master dataset.sent: Article 1 and Article 2 use Atari 2600 Asteroids dataset.sent: Article 1 and Article 2 use Atari 2600 Battle Zone dataset.sent: Article 1 and Article 2 use Atari 2600 Tutankham dataset.sent: Article 1 and Article 2 use Atari 2600 Robotank dataset.sent: Article 1 and Article 2 use Atari 2600 Boxing dataset.sent: Article 1 and Article 2 use Atari 2600 River Raid dataset.sent: Article 1 and Article 2 use Atari 2600 Frostbite dataset. sent: Article 1 uses multiple datasets:  Atari 2600 Phoenix, and  Atari 2600 Defender, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses WMT2014 English-German dataset.', 'sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses WMT2014 English-German dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French, while Article 2 uses IWSLT2015 German-English dataset.', 'sent: Article 1 uses IWSLT2015 German-English dataset, while Article 2 uses multiple datasets:  WMT2014 English-German, and  WMT2014 English-French.', 'sent: Article 1 and Article 2 use IJB-A dataset.', 'sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.']","['sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering.', 'sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering.', 'sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses ASR approach for Open-Domain Question Answering, while Article 2 uses GA for Open-Domain Question Answering.', 'sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses GA approach for Open-Domain Question Answering, while Article 2 uses ASR for Open-Domain Question Answering.', 'sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference.', 'sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference.', 'sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses Spectral Representations for Convolutional Neural Networks for Image Classification.', 'sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification.', 'sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification.', 'sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification.', 'sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses FLSCNN for Image Classification.', 'sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification.', 'sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses FLSCNN for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses FLSCNN for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses FLSCNN for Image Classification.', 'sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification.', 'sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification.', 'sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses FLSCNN for Image Classification.', 'sent: Article 1 and Article 2 use Prior Duel hsapproach for Atari Games. sent: Article 1 uses DDQN  tuned  hs approach for Atari Games, while Article 2 uses Duel hs for Atari Games. sent: Article 1 uses DDQN  tuned  hs approach for Atari Games, while Article 2 uses DDQN  tuned  noop for Atari Games. sent: Article 1 uses DDQN  tuned  hs approach for Atari Games, while Article 2 uses Prior Duel noop for Atari Games. sent: Article 1 uses DDQN  tuned  hs approach for Atari Games, while Article 2 uses Duel noop for Atari Games. sent: Article 1 uses DQN hs approach for Atari Games, while Article 2 uses Duel hs for Atari Games. sent: Article 1 uses DQN hs approach for Atari Games, while Article 2 uses DDQN  tuned  noop for Atari Games. sent: Article 1 uses DQN hs approach for Atari Games, while Article 2 uses Prior Duel noop for Atari Games. sent: Article 1 uses DQN hs approach for Atari Games, while Article 2 uses Duel noop for Atari Games. sent: Article 1 uses DQN noop approach for Atari Games, while Article 2 uses Duel hs for Atari Games. sent: Article 1 uses DQN noop approach for Atari Games, while Article 2 uses DDQN  tuned  noop for Atari Games. sent: Article 1 uses DQN noop approach for Atari Games, while Article 2 uses Prior Duel noop for Atari Games. sent: Article 1 uses DQN noop approach for Atari Games, while Article 2 uses Duel noop for Atari Games.', 'sent: Article 1 and Article 2 use Prior Duel hsapproach for Atari Games. sent: Article 1 uses Duel hs approach for Atari Games, while Article 2 uses DDQN  tuned  hs for Atari Games. sent: Article 1 uses Duel hs approach for Atari Games, while Article 2 uses DQN hs for Atari Games. sent: Article 1 uses Duel hs approach for Atari Games, while Article 2 uses DQN noop for Atari Games. sent: Article 1 uses DDQN  tuned  noop approach for Atari Games, while Article 2 uses DDQN  tuned  hs for Atari Games. sent: Article 1 uses DDQN  tuned  noop approach for Atari Games, while Article 2 uses DQN hs for Atari Games. sent: Article 1 uses DDQN  tuned  noop approach for Atari Games, while Article 2 uses DQN noop for Atari Games. sent: Article 1 uses Prior Duel noop approach for Atari Games, while Article 2 uses DDQN  tuned  hs for Atari Games. sent: Article 1 uses Prior Duel noop approach for Atari Games, while Article 2 uses DQN hs for Atari Games. sent: Article 1 uses Prior Duel noop approach for Atari Games, while Article 2 uses DQN noop for Atari Games. sent: Article 1 uses Duel noop approach for Atari Games, while Article 2 uses DDQN  tuned  hs for Atari Games. sent: Article 1 uses Duel noop approach for Atari Games, while Article 2 uses DQN hs for Atari Games. sent: Article 1 uses Duel noop approach for Atari Games, while Article 2 uses DQN noop for Atari Games.', 'sent: Article 1 uses LSTM approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation. sent: Article 1 uses SMT LSTM5 approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation.', 'sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses LSTM for Machine Translation. sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses SMT LSTM5 for Machine Translation.', 'sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses Word-level LSTM w attn for Machine Translation.', 'sent: Article 1 uses Word-level LSTM w attn approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation.', 'sent: Article 1 uses NAN approach for Face Verification, while Article 2 uses Template adaptation for Face Verification.', 'sent: Article 1 uses NAN approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.']"
21,21,06_paper_03,06_paper_08,63,68,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses ClimateFever from climate related topics domain of 1500 sentences.","sent: Article 1 uses BM25 combined with T5 as reranking approach for document retrieval.sent: Article 1 uses T5 trained on MS MARCO approach for rationale selection.sent: Article 1 uses T5 without fine tuining  approach for verdict prediction, while Article 2 uses ClimateBERT approach for verdict prediction.","[['1511.06038', '1603.01547'], ['1603.01547', '1511.06038'], ['1603.01547', '1606.01549'], ['1603.01547', '1503.03244'], ['1606.01549', '1603.01547'], ['1503.03244', '1603.01547'], ['1901.11504', '1509.06664'], ['1802.05577', '1509.06664'], ['1603.06021', '1509.06664'], ['1508.05326', '1509.06664'], ['1509.06664', '1802.05577'], ['1509.06664', '1603.06021'], ['1509.06664', '1508.05326'], ['1509.06664', '1901.11504']]","['sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test.', 'sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test, while Article 2 uses multiple datasets:  WikiQA, and  QASent.', 'sent: Article 1 and Article 2 use CNN   Daily Mail dataset.sent: Article 1 and Article 2 use Children s Book Test dataset. sent: Article 1 uses SearchQA dataset, while Article 2 uses Quasar dataset.', 'sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test, while Article 2 uses SemEvalCQA dataset.', 'sent: Article 1 and Article 2 use CNN   Daily Mail dataset.sent: Article 1 and Article 2 use Children s Book Test dataset. sent: Article 1 uses Quasar dataset, while Article 2 uses SearchQA dataset.', 'sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  SearchQA, and  Children s Book Test.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.']","['sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering.', 'sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering.', 'sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses ASR approach for Open-Domain Question Answering, while Article 2 uses GA for Open-Domain Question Answering.', 'sent: Article 1 uses ASR approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses AS reader  greedy  approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses AS Reader  single model  approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses AS reader  avg  approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses AS Reader  ensemble model  approach for Question Answering, while Article 2 uses ARC-II for Question Answering.', 'sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering. sent: Article 1 uses GA approach for Open-Domain Question Answering, while Article 2 uses ASR for Open-Domain Question Answering.', 'sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses ASR for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses AS reader  greedy  for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses AS Reader  single model  for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses AS reader  avg  for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses AS Reader  ensemble model  for Question Answering.', 'sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference.', 'sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference.', 'sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference.', 'sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses 100D LSTMs w  word-by-word attention for Natural Language Inference.', 'sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference.', 'sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference.', 'sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference.', 'sent: Article 1 uses 100D LSTMs w  word-by-word attention approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference.']"
22,22,06_paper_03,06_paper_09,63,69,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses HealthVer from health related domain of 14330 evidence-claim pairs.","sent: Article 1 uses BM25 combined with T5 as reranking approach for document retrieval. sent: Article 1 uses T5 trained on MS MARCO approach for rationale selection.sent: Article 1 uses T5 without fine tuining  approach for verdict prediction, while Article 2 uses T5-base approach for verdict prediction.","[['1604.04112', '1301.3557'], ['1604.04112', '1206.2944'], ['1301.3557', '1604.04112'], ['1301.3557', '1503.04596'], ['1503.04596', '1206.2944'], ['1503.04596', '1301.3557'], ['1206.2944', '1503.04596'], ['1811.11482', '1511.02228'], ['1511.02228', '1811.11482'], ['1506.08959', '1407.3867'], ['1407.3867', '1506.08959'], ['1403.6652', '1802.08352'], ['1802.08352', '1403.6652']]","['sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses SVHN dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses SVHN dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses MNIST dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  MNIST, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 uses MNIST dataset, while Article 2 uses CIFAR-100 dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  SVHN, and  MNIST.', 'sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses BSD100 - 4x upscaling dataset.', 'sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses BSD100 - 4x upscaling dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses CompCars dataset, while Article 2 uses  CUB-200-2011 dataset.', 'sent: Article 1 uses  CUB-200-2011 dataset, while Article 2 uses CompCars dataset.', 'sent: Article 1 and Article 2 use Citeseer dataset.sent: Article 1 and Article 2 use Pubmed dataset.sent: Article 1 and Article 2 use Cora dataset. sent: Article 1 uses multiple datasets:  BlogCatalog,  NELL, and  Wikipedia, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Citeseer dataset.sent: Article 1 and Article 2 use Pubmed dataset.sent: Article 1 and Article 2 use Cora dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  BlogCatalog,  NELL, and  Wikipedia.']","['sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification.', 'sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses GP EI for Image Classification.', 'sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification.', 'sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses FLSCNN for Image Classification.', 'sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses GP EI for Image Classification.', 'sent: Article 1 uses FLSCNN approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification.', 'sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses FLSCNN for Image Classification.', 'sent: Article 1 uses PFF approach for Image Super-Resolution, while Article 2 uses IA for Image Super-Resolution.', 'sent: Article 1 uses IA approach for Image Super-Resolution, while Article 2 uses PFF for Image Super-Resolution.', 'sent: Article 1 uses AlexNet approach for Fine-Grained Image Classification, while Article 2 uses Part RCNN for Fine-Grained Image Classification. sent: Article 1 uses GoogLeNet approach for Fine-Grained Image Classification, while Article 2 uses Part RCNN for Fine-Grained Image Classification.', 'sent: Article 1 uses Part RCNN approach for Fine-Grained Image Classification, while Article 2 uses AlexNet for Fine-Grained Image Classification. sent: Article 1 uses Part RCNN approach for Fine-Grained Image Classification, while Article 2 uses GoogLeNet for Fine-Grained Image Classification.', 'sent: Article 1 uses DeepWalk approach for Node Classification, while Article 2 uses alpha-LoNGAE for Node Classification.', 'sent: Article 1 uses alpha-LoNGAE approach for Node Classification, while Article 2 uses DeepWalk for Node Classification.']"
23,23,06_paper_03,06_paper_10,63,610,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses CovidFact sourced from Sub-Reddit from health (Covid related)  domain.","sent: Article 1 uses BM25 combined with T5 as reranking approach for document retrieval, while Article 2 uses Google Search for document retrieval. sent: Article 1 uses T5 trained on MS MARCO approach for rationale selection, while Article 2 uses Sentence-BERT approach for rationale selection. sent: Article 1 uses T5 without fine tuining  approach for verdict prediction, while Article 2 uses RoBERTa (finetuned on GLUE dataset) approach for verdict prediction.","[['1701.06264', '1604.04112'], ['1701.06264', '1503.04596'], ['1604.04112', '1701.06264'], ['1811.11482', '1609.05158'], ['1609.05158', '1811.11482'], ['1603.03958', '1406.4773']]","['sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses SVHN dataset, while Article 2 uses CIFAR-100 dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses MNIST dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses SVHN dataset.', 'sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Ultra Video Group HD - 4x upscaling,  BSD100 - 4x upscaling,  Vid4 - 4x upscaling, and  Xiph HD - 4x upscaling.', 'sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses multiple datasets:  Ultra Video Group HD - 4x upscaling,  BSD100 - 4x upscaling,  Vid4 - 4x upscaling, and  Xiph HD - 4x upscaling, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.']","['sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses ResNet ELU for Image Classification.', 'sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses FLSCNN for Image Classification.', 'sent: Article 1 uses ResNet ELU approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification.', 'sent: Article 1 uses PFF approach for Image Super-Resolution, while Article 2 uses ESPCN for Image Super-Resolution. sent: Article 1 uses PFF approach for Image Super-Resolution, while Article 2 uses bicubic for Image Super-Resolution.', 'sent: Article 1 uses ESPCN approach for Image Super-Resolution, while Article 2 uses PFF for Image Super-Resolution. sent: Article 1 uses bicubic approach for Image Super-Resolution, while Article 2 uses PFF for Image Super-Resolution.', 'sent: Article 1 uses Template adaptation approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.']"
24,24,06_paper_04,06_paper_05,64,65,sent: Article 1 and Article 2 use SciFact sourced from S2ORC dataset. sent: Article 1 and Article 2 use dataset from diverse topics in biomedicine domain. sent: Article 1 and Article 2 use dataset of 1409 claims against 5183 abstracts.,"sent: Article 1 uses BioSentVec approach for document retrieval, while Article 2 uses BM25 combined with T5 as reranking approach for document retrieval. sent: Article 1 uses BioBERT combined with MLP (multi-layer perceptron) approach for rationale selection, while Article 2 uses longformer encoder approach for rationale selection. sent: Article 1 uses BioBERT combined with MLP (multi-layer perceptron) approach for stance prediction, while Article 2 uses BioBERT longformer encoder approach for verdict prediction.","[['1603.03958', '1406.4773'], ['1903.09359', '1804.01005'], ['1903.09359', '1804.03786'], ['1804.01005', '1804.03786'], ['1604.05529', '1805.08237'], ['1805.08237', '1604.05529']]","['sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.', 'sent: Article 1 and Article 2 use AFLW2000-3D dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  AFLW2000,  BIWI, and  Florence.', 'sent: Article 1 uses AFLW2000-3D dataset, while Article 2 uses AFLW2000 dataset.', 'sent: Article 1 and Article 2 use AFLW2000 dataset. sent: Article 1 uses multiple datasets:  BIWI,  AFLW2000-3D, and  Florence, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 uses UD dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses UD dataset.']","['sent: Article 1 uses Template adaptation approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.', 'sent: Article 1 uses 2DASL approach for Face Alignment, while Article 2 uses 3DDFA for Face Alignment. sent: Article 1 uses 2DASL approach for Face Alignment, while Article 2 uses 3DDFA   SDM for Face Alignment.', 'sent: Article 1 uses 2DASL approach for Face Alignment, while Article 2 uses Nonlinear 3D Face Morphable Model for Face Alignment.', 'sent: Article 1 uses 3DDFA approach for Face Alignment, while Article 2 uses Nonlinear 3D Face Morphable Model for Face Alignment. sent: Article 1 uses 3DDFA   SDM approach for Face Alignment, while Article 2 uses Nonlinear 3D Face Morphable Model for Face Alignment.', 'sent: Article 1 uses Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses Meta BiLSTM for Part-Of-Speech Tagging.', 'sent: Article 1 uses Meta BiLSTM approach for Part-Of-Speech Tagging, while Article 2 uses Bi-LSTM for Part-Of-Speech Tagging.']"
25,25,06_paper_04,06_paper_06,64,66,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses CoVERT sourced from fact-checked tweets and evidence from online resources from medical domain of 300 tweets.","sent: Article 1 uses BioSentVec  approach for document retrieval, while Article 2 uses BM25 combined with T5 as reranking approach for document retrieval. sent: Article 1 uses BioBERT combined with MLP (multi-layer perceptron) approach for rationale selection, while Article 2 uses longformer encoder approach for rationale selection. sent: Article 1 uses BioBERT combined with MLP (multi-layer perceptron) approach for verdict prediction, while Article 2 uses longformer encoder approach for verdict prediction.","[['1506.03767', '1707.02968'], ['1506.03767', '1603.05027'], ['1512.03385', '1707.02968'], ['1512.03385', '1603.05027'], ['1707.02968', '1512.03385'], ['1603.05027', '1512.03385'], ['1805.02474', '1811.09386'], ['1811.09386', '1805.02474'], ['1603.03958', '1406.4773'], ['1805.08237', '1805.02474']]","['sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.', 'sent: Article 1 uses multiple datasets:  CIFAR-10,  COCO, and  PASCAL VOC 2007, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses CIFAR-100 dataset.', 'sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses multiple datasets:  CIFAR-10,  COCO, and  PASCAL VOC 2007.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.', 'sent: Article 1 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers.', 'sent: Article 1 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.', 'sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.', 'sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MR,  IMDb, and  CoNLL 2003  English .']","['sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification.', 'sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification.', 'sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification.', 'sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification.', 'sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification.', 'sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification.', 'sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis.', 'sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis.', 'sent: Article 1 uses Template adaptation approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.', 'sent: Article 1 uses Meta BiLSTM approach for Part-Of-Speech Tagging, while Article 2 uses S-LSTM for Part-Of-Speech Tagging.']"
26,26,06_paper_04,06_paper_07,64,67,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses PubHealth sourced from websites and news/news reviews websites from health topics and biomedicine domain of 11832 claims.","sent: Article 1 uses BioSentVec approach for document retrieval. sent: Article 1 uses BioBERT combined with MLP (multi-layer perceptron) approach for rationale selection, while Article 2 uses Sentence-BERT approach for rationale selection. sent: Article 1 uses BioBERT combined with MLP (multi-layer perceptron) approach for verdict prediction, while Article 2 uses SciBERT approach for verdict prediction.","[['1901.11504', '1603.06021'], ['1603.06021', '1901.11504'], ['1506.03767', '1512.03385'], ['1506.03767', '1611.05431'], ['1512.03385', '1611.05431'], ['1611.05431', '1512.03385'], ['1509.01626', '1811.09386'], ['1811.09386', '1509.01626'], ['1901.11504', '1811.09386'], ['1512.03385', '1612.03144'], ['1612.03144', '1512.03385'], ['1603.05474', '1603.03958'], ['1603.05474', '1406.4773'], ['1903.09359', '1808.01558'], ['1808.01558', '1804.03786'], ['1711.03953', '1609.07959'], ['1609.07959', '1711.03953'], ['1609.07959', '1808.10143'], ['1808.10143', '1609.07959']]","['sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.', 'sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses ImageNet dataset.', 'sent: Article 1 uses multiple datasets:  CIFAR-10,  COCO, and  PASCAL VOC 2007, while Article 2 uses ImageNet dataset.', 'sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10,  COCO, and  PASCAL VOC 2007.', 'sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset. sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification, and  Yelp Binary classification, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers.', 'sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset. sent: Article 1 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers, while Article 2 uses multiple datasets:  Yelp Fine-grained classification, and  Yelp Binary classification.', 'sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers.', 'sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 uses multiple datasets:  CIFAR-10, and  PASCAL VOC 2007, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use COCO dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  CIFAR-10, and  PASCAL VOC 2007.', 'sent: Article 1 and Article 2 use IJB-A dataset.', 'sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.', 'sent: Article 1 uses AFLW2000-3D dataset, while Article 2 uses AFLW2000 dataset.', 'sent: Article 1 and Article 2 use AFLW2000 dataset.', 'sent: Article 1 uses multiple datasets:  Penn Treebank  Word Level , and  WikiText-2, while Article 2 uses multiple datasets:  enwiki8,  Text8, and  Hutter Prize.', 'sent: Article 1 uses multiple datasets:  enwiki8,  Text8, and  Hutter Prize, while Article 2 uses multiple datasets:  Penn Treebank  Word Level , and  WikiText-2.', 'sent: Article 1 uses multiple datasets:  enwiki8,  Text8, and  Hutter Prize, while Article 2 uses multiple datasets:  Penn Treebank  Word Level ,  WikiText-2, and  Penn Treebank.', 'sent: Article 1 uses multiple datasets:  Penn Treebank  Word Level ,  WikiText-2, and  Penn Treebank, while Article 2 uses multiple datasets:  enwiki8,  Text8, and  Hutter Prize.']","['sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference.', 'sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference.', 'sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification.', 'sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification.', 'sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification.', 'sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification.', 'sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Text Classification, while Article 2 uses EXAM for Text Classification.', 'sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis. sent: Article 1 uses EXAM approach for Text Classification, while Article 2 uses Char-level CNN for Text Classification.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis.', 'sent: Article 1 uses ResNet-101 approach for Object Detection, while Article 2 uses Faster R-CNN   FPN for Object Detection. sent: Article 1 uses ResNet approach for Object Detection, while Article 2 uses Faster R-CNN   FPN for Object Detection. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Object Detection, while Article 2 uses Faster R-CNN   FPN for Object Detection.', 'sent: Article 1 uses Faster R-CNN   FPN approach for Object Detection, while Article 2 uses ResNet-101 for Object Detection. sent: Article 1 uses Faster R-CNN   FPN approach for Object Detection, while Article 2 uses ResNet for Object Detection. sent: Article 1 uses Faster R-CNN   FPN approach for Object Detection, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Object Detection.', 'sent: Article 1 uses NAN approach for Face Verification, while Article 2 uses Template adaptation for Face Verification.', 'sent: Article 1 uses NAN approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.', 'sent: Article 1 uses 2DASL approach for Face Alignment, while Article 2 uses MCL for Face Alignment.', 'sent: Article 1 uses MCL approach for Face Alignment, while Article 2 uses Nonlinear 3D Face Morphable Model for Face Alignment.', 'sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling.', 'sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling.', 'sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling.', 'sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling.']"
27,27,06_paper_04,06_paper_08,64,68,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses ClimateFever from climate related topics domain of 1500 sentences.","sent: Article 1 uses BioSentVec approach for document retrieval. sent: Article 1 uses BioBERT combined with MLP (multi-layer perceptron) approach for rationale selection. sent: Article 1 uses BioBERT combined with MLP (multi-layer perceptron) approach for verdict prediction, while Article 2 uses ClimateBERT approach for verdict prediction.","[['1901.11504', '1603.06021'], ['1802.05577', '1603.06021'], ['1603.06021', '1901.11504'], ['1603.06021', '1508.05326'], ['1603.06021', '1802.05577'], ['1508.05326', '1603.06021'], ['1509.01626', '1811.09386'], ['1811.09386', '1607.01759'], ['1811.09386', '1509.01626'], ['1607.01759', '1811.09386'], ['1901.11504', '1811.09386'], ['1805.02474', '1811.09386'], ['1811.09386', '1412.1058'], ['1811.09386', '1805.02474'], ['1811.09386', '1812.01207'], ['1412.1058', '1811.09386'], ['1903.09359', '1804.03786'], ['1711.03953', '1808.10143'], ['1808.10143', '1711.03953'], ['1805.08237', '1805.02474']]","['sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset. sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification, and  Yelp Binary classification, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers.', 'sent: Article 1 and Article 2 use Amazon Review Polarity dataset.sent: Article 1 and Article 2 use Amazon Review Full dataset.sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset.sent: Article 1 and Article 2 use Yahoo  Answers dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Sogou News,  Yelp Fine-grained classification, and  Yelp Binary classification.', 'sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset. sent: Article 1 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers, while Article 2 uses multiple datasets:  Yelp Fine-grained classification, and  Yelp Binary classification.', 'sent: Article 1 and Article 2 use Amazon Review Polarity dataset.sent: Article 1 and Article 2 use Amazon Review Full dataset.sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset.sent: Article 1 and Article 2 use Yahoo  Answers dataset. sent: Article 1 uses multiple datasets:  Sogou News,  Yelp Fine-grained classification, and  Yelp Binary classification, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers.', 'sent: Article 1 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers.', 'sent: Article 1 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers, while Article 2 uses IMDb dataset.', 'sent: Article 1 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.', 'sent: Article 1 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers, while Article 2 uses multiple datasets:  SST-2 Binary classification, and  SemEval 2018 Task 1E-c.', 'sent: Article 1 uses IMDb dataset, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers.', 'sent: Article 1 uses AFLW2000-3D dataset, while Article 2 uses AFLW2000 dataset.', 'sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset.sent: Article 1 and Article 2 use WikiText-2 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Penn Treebank dataset.', 'sent: Article 1 and Article 2 use Penn Treebank  Word Level  dataset.sent: Article 1 and Article 2 use WikiText-2 dataset. sent: Article 1 uses Penn Treebank dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MR,  IMDb, and  CoNLL 2003  English .']","['sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference.', 'sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference.', 'sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference.', 'sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference.', 'sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference.', 'sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference.', 'sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Text Classification, while Article 2 uses EXAM for Text Classification.', 'sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis. sent: Article 1 uses EXAM approach for Text Classification, while Article 2 uses FastText for Text Classification. sent: Article 1 uses EXAM approach for Text Classification, while Article 2 uses fastText for Text Classification.', 'sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis. sent: Article 1 uses EXAM approach for Text Classification, while Article 2 uses Char-level CNN for Text Classification.', 'sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis. sent: Article 1 uses FastText approach for Text Classification, while Article 2 uses EXAM for Text Classification. sent: Article 1 uses fastText approach for Text Classification, while Article 2 uses EXAM for Text Classification.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis.', 'sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis.', 'sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses seq2-bown-CNN for Sentiment Analysis.', 'sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis.', 'sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis.', 'sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis.', 'sent: Article 1 uses 2DASL approach for Face Alignment, while Article 2 uses Nonlinear 3D Face Morphable Model for Face Alignment.', 'sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling.', 'sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling.', 'sent: Article 1 uses Meta BiLSTM approach for Part-Of-Speech Tagging, while Article 2 uses S-LSTM for Part-Of-Speech Tagging.']"
28,28,06_paper_04,06_paper_09,64,69,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses HealthVer from health related domain of 14330 evidence-claim pairs.","sent: Article 1 uses BioSentVec approach for document retrieval. sent: Article 1 uses BioBERT combined with MLP (multi-layer perceptron) approach for rationale selection. sent: Article 1 uses BioBERT combined with MLP (multi-layer perceptron) approach for verdict prediction, while Article 2 uses T5-base approach for verdict prediction.","[['1506.03767', '1301.3557'], ['1506.03767', '1206.2944'], ['1301.3557', '1506.03767'], ['1301.3557', '1512.03385'], ['1512.03385', '1301.3557'], ['1512.03385', '1206.2944'], ['1206.2944', '1512.03385'], ['1811.09386', '1607.01759'], ['1607.01759', '1811.09386'], ['1506.08959', '1407.3867'], ['1407.3867', '1506.08959']]","['sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses SVHN dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses SVHN dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  CIFAR-100, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses multiple datasets:  SVHN, and  CIFAR-100.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.', 'sent: Article 1 and Article 2 use Amazon Review Polarity dataset.sent: Article 1 and Article 2 use Amazon Review Full dataset.sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset.sent: Article 1 and Article 2 use Yahoo  Answers dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Sogou News,  Yelp Fine-grained classification, and  Yelp Binary classification.', 'sent: Article 1 and Article 2 use Amazon Review Polarity dataset.sent: Article 1 and Article 2 use Amazon Review Full dataset.sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset.sent: Article 1 and Article 2 use Yahoo  Answers dataset. sent: Article 1 uses multiple datasets:  Sogou News,  Yelp Fine-grained classification, and  Yelp Binary classification, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses CompCars dataset, while Article 2 uses  CUB-200-2011 dataset.', 'sent: Article 1 uses  CUB-200-2011 dataset, while Article 2 uses CompCars dataset.']","['sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification.', 'sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses GP EI for Image Classification.', 'sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses Spectral Representations for Convolutional Neural Networks for Image Classification.', 'sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification.', 'sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification.', 'sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses GP EI for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses GP EI for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses GP EI for Image Classification.', 'sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification.', 'sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis. sent: Article 1 uses EXAM approach for Text Classification, while Article 2 uses FastText for Text Classification. sent: Article 1 uses EXAM approach for Text Classification, while Article 2 uses fastText for Text Classification.', 'sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis. sent: Article 1 uses FastText approach for Text Classification, while Article 2 uses EXAM for Text Classification. sent: Article 1 uses fastText approach for Text Classification, while Article 2 uses EXAM for Text Classification.', 'sent: Article 1 uses AlexNet approach for Fine-Grained Image Classification, while Article 2 uses Part RCNN for Fine-Grained Image Classification. sent: Article 1 uses GoogLeNet approach for Fine-Grained Image Classification, while Article 2 uses Part RCNN for Fine-Grained Image Classification.', 'sent: Article 1 uses Part RCNN approach for Fine-Grained Image Classification, while Article 2 uses AlexNet for Fine-Grained Image Classification. sent: Article 1 uses Part RCNN approach for Fine-Grained Image Classification, while Article 2 uses GoogLeNet for Fine-Grained Image Classification.']"
29,29,06_paper_04,06_paper_10,64,610,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses CovidFact sourced from Sub-Reddit from health (Covid related)  domain.","sent: Article 1 uses BioSentVec approach for document retrieval, while Article 2 uses Google Search for document retrieval. sent: Article 1 uses BioBERT combined with MLP (multi-layer perceptron) approach for rationale selection, while Article 2 uses Sentence-BERT approach for rationale selection. sent: Article 1 uses BioBERT combined with MLP (multi-layer perceptron) approach for verdict prediction, while Article 2 uses RoBERTa (finetuned on GLUE dataset) approach for verdict prediction.","[['1701.06264', '1506.03767'], ['1701.06264', '1512.03385'], ['1506.03767', '1701.06264'], ['1512.03385', '1701.06264'], ['1512.03385', '1506.01497'], ['1506.01497', '1612.03144'], ['1506.01497', '1512.03385'], ['1603.03958', '1406.4773'], ['1604.05529', '1805.08237'], ['1805.08237', '1604.05529']]","['sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses SVHN dataset, while Article 2 uses CIFAR-100 dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses SVHN dataset, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses SVHN dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses SVHN dataset.', 'sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 uses multiple datasets:  CIFAR-10, and  COCO, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses PASCAL VOC 2007 dataset, while Article 2 uses COCO dataset.', 'sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  CIFAR-10, and  COCO.', 'sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.', 'sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 uses UD dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses UD dataset.']","['sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses Spectral Representations for Convolutional Neural Networks for Image Classification.', 'sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification.', 'sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification.', 'sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification.', 'sent: Article 1 uses ResNet-101 approach for Object Detection, while Article 2 uses Faster R-CNN for Object Detection. sent: Article 1 uses ResNet approach for Object Detection, while Article 2 uses Faster R-CNN for Object Detection. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Object Detection, while Article 2 uses Faster R-CNN for Object Detection.', 'sent: Article 1 uses Faster R-CNN approach for Object Detection, while Article 2 uses Faster R-CNN   FPN for Object Detection.', 'sent: Article 1 uses Faster R-CNN approach for Object Detection, while Article 2 uses ResNet-101 for Object Detection. sent: Article 1 uses Faster R-CNN approach for Object Detection, while Article 2 uses ResNet for Object Detection. sent: Article 1 uses Faster R-CNN approach for Object Detection, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Object Detection.', 'sent: Article 1 uses Template adaptation approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.', 'sent: Article 1 uses Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses Meta BiLSTM for Part-Of-Speech Tagging.', 'sent: Article 1 uses Meta BiLSTM approach for Part-Of-Speech Tagging, while Article 2 uses Bi-LSTM for Part-Of-Speech Tagging.']"
30,30,06_paper_05,06_paper_06,65,66,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses CoVERT sourced from fact-checked tweets and evidence from online resources from medical domain of 300 tweets.",sent: Article 1 and Article 2 use BM25 combined with T5 as reranking approach for document retrieval. sent: Article 1 and Article 2 use Longformer (binary head)  approach for rationale selection. sent: Article 1 and Article 2 use Longformer (ternary head) encoder approach for verdict prediction.,"[['1712.02616', '1511.07122'], ['1712.02616', '1611.08323'], ['1511.07122', '1707.02968'], ['1611.08323', '1707.02968'], ['1706.02596', '1610.09027'], ['1610.09027', '1706.02596'], ['1603.03958', '1406.4773'], ['1808.06281', '1710.06555'], ['1710.06555', '1808.06281'], ['1604.05529', '1805.02474'], ['1805.08237', '1805.02474']]","['sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  CamVid,  PASCAL VOC 2012, and  ADE20K.', 'sent: Article 1 and Article 2 use Cityscapes dataset.', 'sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset. sent: Article 1 uses multiple datasets:  CamVid,  ADE20K, and  Cityscapes, while Article 2 uses ImageNet dataset.', 'sent: Article 1 uses Cityscapes dataset, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.', 'sent: Article 1 uses TriviaQA dataset, while Article 2 uses bAbi dataset.', 'sent: Article 1 uses bAbi dataset, while Article 2 uses TriviaQA dataset.', 'sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.', 'sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 uses DukeMTMC-reID dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses DukeMTMC-reID dataset.', 'sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 uses UD dataset, while Article 2 uses multiple datasets:  MR,  IMDb, and  CoNLL 2003  English .', 'sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MR,  IMDb, and  CoNLL 2003  English .']","['sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses Dilation10 for Semantic Segmentation. sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses Dilated Convolutions for Semantic Segmentation. sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses DilatedNet for Semantic Segmentation.', 'sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses FRRN for Semantic Segmentation.', 'sent: Article 1 uses Dilation10 approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses Dilation10 approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation.', 'sent: Article 1 uses FRRN approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses FRRN approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation.', 'sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses SDNC for Question Answering.', 'sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering.', 'sent: Article 1 uses Template adaptation approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.', 'sent: Article 1 uses Incremental Learning approach for Person Re-Identification, while Article 2 uses MSCAN for Person Re-Identification.', 'sent: Article 1 uses MSCAN approach for Person Re-Identification, while Article 2 uses Incremental Learning for Person Re-Identification.', 'sent: Article 1 uses Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses S-LSTM for Part-Of-Speech Tagging.', 'sent: Article 1 uses Meta BiLSTM approach for Part-Of-Speech Tagging, while Article 2 uses S-LSTM for Part-Of-Speech Tagging.']"
31,31,06_paper_05,06_paper_07,65,67,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses PubHealth sourced from websites and news/news reviews websites from health topics and biomedicine domain of 11832 claims.","sent: Article 1 uses BM25 combined with T5 as reranking approach for document retrieval. sent: Article 1 uses Longformer (binary head) approach for rationale selection, while Article 2 uses Sentence-BERT approach for rationale selection. sent: Article 1 uses Longformer (ternary head) approach for verdict prediction, while Article 2 uses SciBERT approach for verdict prediction.","[['1804.09337', '1511.07122'], ['1804.09337', '1611.08323'], ['1511.07122', '1804.09337'], ['1611.08323', '1804.09337'], ['1511.06038', '1610.09027'], ['1606.01549', '1610.09027'], ['1610.09027', '1511.06038'], ['1610.09027', '1606.01549'], ['1511.02274', '1603.02814'], ['1603.05474', '1603.03958'], ['1603.05474', '1406.4773'], ['1903.09359', '1808.01558'], ['1808.01558', '1804.01005'], ['1703.05693', '1710.06555'], ['1710.06555', '1703.05693']]","['sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset.sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  CamVid, and  ADE20K.', 'sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses PASCAL VOC 2012 dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset.sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses multiple datasets:  CamVid, and  ADE20K, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses PASCAL VOC 2012 dataset.', 'sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses bAbi dataset.', 'sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses bAbi dataset.', 'sent: Article 1 uses bAbi dataset, while Article 2 uses multiple datasets:  WikiQA, and  QASent.', 'sent: Article 1 uses bAbi dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.', 'sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset.', 'sent: Article 1 and Article 2 use IJB-A dataset.', 'sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.', 'sent: Article 1 uses AFLW2000-3D dataset, while Article 2 uses AFLW2000 dataset.', 'sent: Article 1 and Article 2 use AFLW2000 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  BIWI,  AFLW2000-3D, and  Florence.', 'sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 uses DukeMTMC-reID dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Market-1501 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses DukeMTMC-reID dataset.']","['sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses Dilation10 for Semantic Segmentation. sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses Dilated Convolutions for Semantic Segmentation. sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses DilatedNet for Semantic Segmentation.', 'sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses FRRN for Semantic Segmentation.', 'sent: Article 1 uses Dilation10 approach for Semantic Segmentation, while Article 2 uses Smooth Network with Channel Attention Block for Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Semantic Segmentation, while Article 2 uses Smooth Network with Channel Attention Block for Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Semantic Segmentation, while Article 2 uses Smooth Network with Channel Attention Block for Semantic Segmentation.', 'sent: Article 1 uses FRRN approach for Semantic Segmentation, while Article 2 uses Smooth Network with Channel Attention Block for Semantic Segmentation.', 'sent: Article 1 and Article 2 use LSTMapproach for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses SDNC for Question Answering.', 'sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses SDNC for Question Answering.', 'sent: Article 1 and Article 2 use LSTMapproach for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering.', 'sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses GA for Question Answering.', 'sent: Article 1 uses SAN approach for Visual Question Answering, while Article 2 uses CNN-RNN for Visual Question Answering.', 'sent: Article 1 uses NAN approach for Face Verification, while Article 2 uses Template adaptation for Face Verification.', 'sent: Article 1 uses NAN approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.', 'sent: Article 1 uses 2DASL approach for Face Alignment, while Article 2 uses MCL for Face Alignment.', 'sent: Article 1 uses MCL approach for Face Alignment, while Article 2 uses 3DDFA for Face Alignment. sent: Article 1 uses MCL approach for Face Alignment, while Article 2 uses 3DDFA   SDM for Face Alignment.', 'sent: Article 1 uses SVDNet approach for Person Re-Identification, while Article 2 uses MSCAN for Person Re-Identification.', 'sent: Article 1 uses MSCAN approach for Person Re-Identification, while Article 2 uses SVDNet for Person Re-Identification.']"
32,32,06_paper_05,06_paper_08,65,68,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses ClimateFever from climate related topics domain of 1500 sentences.","sent: Article 1 uses BM25 combined with T5 as reranking  approach for document retrieval. sent: Article 1 uses Longformer (binary head) approach for rationale selection. sent: Article 1 uses Longformer (ternary head)  approach for verdict prediction, while Article 2 uses ClimateBERT approach for verdict prediction.","[['1511.06038', '1610.09027'], ['1606.01549', '1610.09027'], ['1610.09027', '1511.06038'], ['1610.09027', '1606.01549'], ['1610.09027', '1503.03244'], ['1503.03244', '1610.09027'], ['1903.09359', '1804.03786'], ['1804.01005', '1804.03786'], ['1604.05529', '1805.02474'], ['1805.08237', '1805.02474']]","['sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses bAbi dataset.', 'sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses bAbi dataset.', 'sent: Article 1 uses bAbi dataset, while Article 2 uses multiple datasets:  WikiQA, and  QASent.', 'sent: Article 1 uses bAbi dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.', 'sent: Article 1 uses bAbi dataset, while Article 2 uses SemEvalCQA dataset.', 'sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses bAbi dataset.', 'sent: Article 1 uses AFLW2000-3D dataset, while Article 2 uses AFLW2000 dataset.', 'sent: Article 1 and Article 2 use AFLW2000 dataset. sent: Article 1 uses multiple datasets:  BIWI,  AFLW2000-3D, and  Florence, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 uses UD dataset, while Article 2 uses multiple datasets:  MR,  IMDb, and  CoNLL 2003  English .', 'sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MR,  IMDb, and  CoNLL 2003  English .']","['sent: Article 1 and Article 2 use LSTMapproach for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses SDNC for Question Answering.', 'sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses SDNC for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses SDNC for Question Answering.', 'sent: Article 1 and Article 2 use LSTMapproach for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering.', 'sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses GA for Question Answering.', 'sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses SDNC approach for Question Answering, while Article 2 uses ARC-II for Question Answering.', 'sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses SDNC for Question Answering.', 'sent: Article 1 uses 2DASL approach for Face Alignment, while Article 2 uses Nonlinear 3D Face Morphable Model for Face Alignment.', 'sent: Article 1 uses 3DDFA approach for Face Alignment, while Article 2 uses Nonlinear 3D Face Morphable Model for Face Alignment. sent: Article 1 uses 3DDFA   SDM approach for Face Alignment, while Article 2 uses Nonlinear 3D Face Morphable Model for Face Alignment.', 'sent: Article 1 uses Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses S-LSTM for Part-Of-Speech Tagging.', 'sent: Article 1 uses Meta BiLSTM approach for Part-Of-Speech Tagging, while Article 2 uses S-LSTM for Part-Of-Speech Tagging.']"
33,33,06_paper_05,06_paper_09,65,69,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses HealthVer from health related domain of 14330 evidence-claim pairs.","sent: Article 1 uses BM25 combined with T5 as reranking  approach for document retrieval. sent: Article 1 uses Longformer (binary head) approach for rationale selection. sent: Article 1 uses Longformer (ternary head) approach for verdict prediction, while Article 2 uses T5-base approach for verdict prediction.","[['1511.07122', '1504.01013'], ['1504.01013', '1611.08323'], ['1504.01013', '1511.07122'], ['1506.08959', '1407.3867'], ['1407.3867', '1506.08959']]","['sent: Article 1 uses multiple datasets:  CamVid,  PASCAL VOC 2012,  ADE20K, and  Cityscapes, while Article 2 uses PASCAL Context dataset.', 'sent: Article 1 uses PASCAL Context dataset, while Article 2 uses Cityscapes dataset.', 'sent: Article 1 uses PASCAL Context dataset, while Article 2 uses multiple datasets:  CamVid,  PASCAL VOC 2012,  ADE20K, and  Cityscapes.', 'sent: Article 1 uses CompCars dataset, while Article 2 uses  CUB-200-2011 dataset.', 'sent: Article 1 uses  CUB-200-2011 dataset, while Article 2 uses CompCars dataset.']","['sent: Article 1 uses Dilation10 approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation.', 'sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses FRRN for Semantic Segmentation.', 'sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses Dilation10 for Semantic Segmentation. sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses Dilated Convolutions for Semantic Segmentation. sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses DilatedNet for Semantic Segmentation.', 'sent: Article 1 uses AlexNet approach for Fine-Grained Image Classification, while Article 2 uses Part RCNN for Fine-Grained Image Classification. sent: Article 1 uses GoogLeNet approach for Fine-Grained Image Classification, while Article 2 uses Part RCNN for Fine-Grained Image Classification.', 'sent: Article 1 uses Part RCNN approach for Fine-Grained Image Classification, while Article 2 uses AlexNet for Fine-Grained Image Classification. sent: Article 1 uses Part RCNN approach for Fine-Grained Image Classification, while Article 2 uses GoogLeNet for Fine-Grained Image Classification.']"
34,34,06_paper_05,06_paper_10,65,610,"sent: Article 1 uses SciFact sourced from S2ORC dataset from diverse topics in biomedicine domain of 1409 claims against 5183 abstracts, while Article 2 uses CovidFact sourced from Sub-Reddit from health (Covid related)  domain.","sent: Article 1 uses BM25 combined with T5 as reranking approach for document retrieval, while Article 2 uses Google Search for document retrieval. sent: Article 1 uses Longformer (binary head) approach for rationale selection, while Article 2 uses Sentence-BERT approach for rationale selection. sent: Article 1 uses Longformer (ternary head) approach for verdict prediction, while Article 2 uses RoBERTa (finetuned on GLUE dataset) approach for verdict prediction.","[['1611.08323', '1704.08545'], ['1511.07122', '1704.08545'], ['1704.08545', '1511.07122'], ['1704.08545', '1611.08323'], ['1605.08803', '1812.09916'], ['1603.03958', '1406.4773'], ['1604.05529', '1805.08237'], ['1805.08237', '1604.05529']]","['sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CamVid dataset.', 'sent: Article 1 and Article 2 use CamVid dataset.sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses multiple datasets:  PASCAL VOC 2012, and  ADE20K, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use CamVid dataset.sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  PASCAL VOC 2012, and  ADE20K.', 'sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses CamVid dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset.', 'sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.', 'sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 uses UD dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses UD dataset.']","['sent: Article 1 uses FRRN approach for Real-Time Semantic Segmentation, while Article 2 uses ICNet for Real-Time Semantic Segmentation. sent: Article 1 uses FRRN approach for Semantic Segmentation, while Article 2 uses ICNet for Semantic Segmentation.', 'sent: Article 1 uses Dilation10 approach for Real-Time Semantic Segmentation, while Article 2 uses ICNet for Real-Time Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Real-Time Semantic Segmentation, while Article 2 uses ICNet for Real-Time Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Real-Time Semantic Segmentation, while Article 2 uses ICNet for Real-Time Semantic Segmentation. sent: Article 1 uses Dilation10 approach for Semantic Segmentation, while Article 2 uses ICNet for Semantic Segmentation. sent: Article 1 uses Dilated Convolutions approach for Semantic Segmentation, while Article 2 uses ICNet for Semantic Segmentation. sent: Article 1 uses DilatedNet approach for Semantic Segmentation, while Article 2 uses ICNet for Semantic Segmentation.', 'sent: Article 1 uses ICNet approach for Real-Time Semantic Segmentation, while Article 2 uses Dilation10 for Real-Time Semantic Segmentation. sent: Article 1 uses ICNet approach for Real-Time Semantic Segmentation, while Article 2 uses Dilated Convolutions for Real-Time Semantic Segmentation. sent: Article 1 uses ICNet approach for Real-Time Semantic Segmentation, while Article 2 uses DilatedNet for Real-Time Semantic Segmentation. sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses Dilation10 for Semantic Segmentation. sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses Dilated Convolutions for Semantic Segmentation. sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses DilatedNet for Semantic Segmentation.', 'sent: Article 1 uses ICNet approach for Real-Time Semantic Segmentation, while Article 2 uses FRRN for Real-Time Semantic Segmentation. sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses FRRN for Semantic Segmentation.', 'sent: Article 1 uses Real NVP approach for Image Generation, while Article 2 uses MMD-GAN-rep for Image Generation. sent: Article 1 uses PixelRNN approach for Image Generation, while Article 2 uses MMD-GAN-rep for Image Generation.', 'sent: Article 1 uses Template adaptation approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.', 'sent: Article 1 uses Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses Meta BiLSTM for Part-Of-Speech Tagging.', 'sent: Article 1 uses Meta BiLSTM approach for Part-Of-Speech Tagging, while Article 2 uses Bi-LSTM for Part-Of-Speech Tagging.']"
35,35,06_paper_06,06_paper_07,66,67,"sent: Article 1 uses CoVERT sourced from fact-checked tweets and evidence from online resources from medical domain of 300 tweets, while Article 2 uses PubHealth sourced from websites and news/news reviews websites from health topics and biomedicine domain of 11832 claims.","sent: Article 1 uses BM25 combined with T5 as reranking approach for document retrieval. sent: Article 1 uses Longformer (binary head) approach for rationale selection, while Article 2 uses Sentence-BERT approach for rationale selection. sent: Article 1 uses Longformer (ternary head) approach for verdict prediction, while Article 2 uses SciBERT approach for verdict prediction.","[['1804.09337', '1712.02616'], ['1804.09337', '1707.02968'], ['1712.02616', '1804.09337'], ['1511.06038', '1706.02596'], ['1706.02596', '1511.06038'], ['1706.02596', '1606.01549'], ['1606.01549', '1706.02596'], ['1506.03767', '1707.02968'], ['1506.03767', '1603.05027'], ['1512.03385', '1707.02968'], ['1512.03385', '1603.05027'], ['1707.02968', '1611.05431'], ['1707.02968', '1512.03385'], ['1603.05027', '1611.05431'], ['1603.05027', '1512.03385'], ['1611.05431', '1707.02968'], ['1611.05431', '1603.05027'], ['1901.11504', '1805.02474'], ['1805.02474', '1509.01626'], ['1509.01626', '1805.02474'], ['1505.04597', '1802.06955'], ['1802.06955', '1505.04597'], ['1610.10099', '1711.02132'], ['1711.02132', '1610.10099'], ['1603.05474', '1406.4773'], ['1808.06281', '1703.05693'], ['1703.05693', '1808.06281']]","['sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses PASCAL VOC 2012 dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use PASCAL VOC 2012 dataset. sent: Article 1 uses Cityscapes dataset, while Article 2 uses ImageNet dataset.', 'sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses PASCAL VOC 2012 dataset.', 'sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses TriviaQA dataset.', 'sent: Article 1 uses TriviaQA dataset, while Article 2 uses multiple datasets:  WikiQA, and  QASent.', 'sent: Article 1 uses TriviaQA dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.', 'sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses TriviaQA dataset.', 'sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset.', 'sent: Article 1 uses multiple datasets:  CIFAR-10,  COCO, and  PASCAL VOC 2007, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses CIFAR-100 dataset.', 'sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 uses PASCAL VOC 2012 dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses multiple datasets:  CIFAR-10,  COCO, and  PASCAL VOC 2007.', 'sent: Article 1 uses multiple datasets:  CIFAR-10, and  CIFAR-100, while Article 2 uses ImageNet dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.', 'sent: Article 1 and Article 2 use ImageNet dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses PASCAL VOC 2012 dataset.', 'sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10, and  CIFAR-100.', 'sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.', 'sent: Article 1 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification.', 'sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.', 'sent: Article 1 and Article 2 use LUNA dataset.sent: Article 1 and Article 2 use DRIVE dataset.sent: Article 1 and Article 2 use CHASE DB1 dataset.sent: Article 1 and Article 2 use STARE dataset.sent: Article 1 and Article 2 use Kaggle Skin Lesion Segmentation dataset. sent: Article 1 uses multiple datasets:  PhC-U373,  ISBI 2012 EM Segmentation,  DIC-HeLa, and  CT-150, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use LUNA dataset.sent: Article 1 and Article 2 use DRIVE dataset.sent: Article 1 and Article 2 use CHASE DB1 dataset.sent: Article 1 and Article 2 use STARE dataset.sent: Article 1 and Article 2 use Kaggle Skin Lesion Segmentation dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  PhC-U373,  ISBI 2012 EM Segmentation,  DIC-HeLa, and  CT-150.', 'sent: Article 1 and Article 2 use WMT2014 English-German dataset.sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 uses WMT2015 English-German dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use WMT2014 English-German dataset.sent: Article 1 and Article 2 use WMT2014 English-French dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses WMT2015 English-German dataset.', 'sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.', 'sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.', 'sent: Article 1 and Article 2 use DukeMTMC-reID dataset.sent: Article 1 and Article 2 use Market-1501 dataset.']","['sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses Mapillary for Semantic Segmentation.', 'sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation.', 'sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses Smooth Network with Channel Attention Block for Semantic Segmentation.', 'sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering.', 'sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering.', 'sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses GA for Question Answering.', 'sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering.', 'sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification.', 'sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification.', 'sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification.', 'sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification.', 'sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification.', 'sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification.', 'sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification.', 'sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification.', 'sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification.', 'sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis.', 'sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis.', 'sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis.', 'sent: Article 1 uses U-Net approach for Lung Nodule Segmentation, while Article 2 uses R2U-Net for Lung Nodule Segmentation. sent: Article 1 uses U-Net approach for Skin Cancer Segmentation, while Article 2 uses R2U-Net for Skin Cancer Segmentation. sent: Article 1 uses U-Net approach for Retinal Vessel Segmentation, while Article 2 uses R2U-Net for Retinal Vessel Segmentation.', 'sent: Article 1 uses R2U-Net approach for Lung Nodule Segmentation, while Article 2 uses U-Net for Lung Nodule Segmentation. sent: Article 1 uses R2U-Net approach for Skin Cancer Segmentation, while Article 2 uses U-Net for Skin Cancer Segmentation. sent: Article 1 uses R2U-Net approach for Retinal Vessel Segmentation, while Article 2 uses U-Net for Retinal Vessel Segmentation.', 'sent: Article 1 uses ByteNet approach for Machine Translation, while Article 2 uses Weighted Transformer  large  for Machine Translation.', 'sent: Article 1 uses Weighted Transformer  large  approach for Machine Translation, while Article 2 uses ByteNet for Machine Translation.', 'sent: Article 1 uses NAN approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.', 'sent: Article 1 uses Incremental Learning approach for Person Re-Identification, while Article 2 uses SVDNet for Person Re-Identification.', 'sent: Article 1 uses SVDNet approach for Person Re-Identification, while Article 2 uses Incremental Learning for Person Re-Identification.']"
36,36,06_paper_06,06_paper_08,66,68,"sent: Article 1 uses CoVERT sourced from fact-checked tweets and evidence from online resources from medical domain of 300 tweets, while Article 2 uses ClimateFever from climate related topics domain of 1500 sentences.","sent: Article 1 uses BM25 combined with T5 as reranking approach for document retrieval. sent: Article 1 uses Longformer (binary head) approach for rationale selection. sent: Article 1 uses Longformer (ternary head) approach for verdict prediction, while Article 2 uses ClimateBERT approach for verdict prediction.","[['1511.06038', '1706.02596'], ['1706.02596', '1511.06038'], ['1706.02596', '1606.01549'], ['1706.02596', '1503.03244'], ['1606.01549', '1706.02596'], ['1503.03244', '1706.02596'], ['1607.01759', '1805.02474'], ['1901.11504', '1805.02474'], ['1805.02474', '1811.09386'], ['1805.02474', '1509.01626'], ['1805.02474', '1412.1058'], ['1805.02474', '1607.01759'], ['1805.02474', '1812.01207'], ['1811.09386', '1805.02474'], ['1509.01626', '1805.02474'], ['1412.1058', '1805.02474'], ['1805.02474', '1808.09075']]","['sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses TriviaQA dataset.', 'sent: Article 1 uses TriviaQA dataset, while Article 2 uses multiple datasets:  WikiQA, and  QASent.', 'sent: Article 1 uses TriviaQA dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.', 'sent: Article 1 uses TriviaQA dataset, while Article 2 uses SemEvalCQA dataset.', 'sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses TriviaQA dataset.', 'sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses TriviaQA dataset.', 'sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.', 'sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.', 'sent: Article 1 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers.', 'sent: Article 1 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification.', 'sent: Article 1 and Article 2 use IMDb dataset. sent: Article 1 uses multiple datasets:  MR,  CoNLL 2003  English , and  Penn Treebank, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers.', 'sent: Article 1 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses multiple datasets:  SST-2 Binary classification, and  SemEval 2018 Task 1E-c.', 'sent: Article 1 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.', 'sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.', 'sent: Article 1 and Article 2 use IMDb dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MR,  CoNLL 2003  English , and  Penn Treebank.', 'sent: Article 1 and Article 2 use CoNLL 2003  English  dataset. sent: Article 1 uses multiple datasets:  MR,  IMDb, and  Penn Treebank, while Article 2 does not use specific datasets for experiments.']","['sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering.', 'sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering.', 'sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses GA for Question Answering.', 'sent: Article 1 uses Reading Twice for NLU approach for Question Answering, while Article 2 uses ARC-II for Question Answering.', 'sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering.', 'sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses Reading Twice for NLU for Question Answering.', 'sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis.', 'sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis.', 'sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis.', 'sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses seq2-bown-CNN for Sentiment Analysis.', 'sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis.', 'sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis.', 'sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis.', 'sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis.', 'sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis.', 'sent: Article 1 uses S-LSTM approach for Named Entity Recognition  NER , while Article 2 uses Neural-CRF AE for Named Entity Recognition  NER . sent: Article 1 uses S-LSTM approach for Named Entity Recognition  NER , while Article 2 uses CRF   AutoEncoder for Named Entity Recognition  NER .']"
37,37,06_paper_06,06_paper_09,66,69,"sent: Article 1 uses CoVERT sourced from fact-checked tweets and evidence from online resources from medical domain of 300 tweets, while Article 2 uses HealthVer from health related domain of 14330 evidence-claim pairs.","sent: Article 1 uses BM25 combined with T5 as reranking approach for document retrieval. sent: Article 1 uses Longformer (binary head) approach for rationale selection. sent: Article 1 uses Longformer (ternary head) approach for verdict prediction, while Article 2 uses T5-base approach for verdict prediction.","[['1712.02616', '1504.01013'], ['1504.01013', '1707.02968'], ['1301.3557', '1707.02968'], ['1301.3557', '1603.05027'], ['1707.02968', '1301.3557'], ['1707.02968', '1206.2944'], ['1603.05027', '1301.3557'], ['1603.05027', '1206.2944'], ['1206.2944', '1707.02968'], ['1206.2944', '1603.05027'], ['1607.01759', '1805.02474'], ['1805.02474', '1607.01759'], ['1603.06042', '1603.03793'], ['1603.03793', '1603.06042'], ['1611.00144', '1703.04247'], ['1703.04247', '1611.00144']]","['sent: Article 1 uses Cityscapes dataset, while Article 2 uses PASCAL Context dataset.', 'sent: Article 1 uses PASCAL Context dataset, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.', 'sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses SVHN dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100.', 'sent: Article 1 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012, while Article 2 uses CIFAR-10 dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses SVHN dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CIFAR-100 dataset.', 'sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.', 'sent: Article 1 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers.', 'sent: Article 1 and Article 2 use Penn Treebank dataset.', 'sent: Article 1 and Article 2 use Penn Treebank dataset.', 'sent: Article 1 and Article 2 use Company  dataset.sent: Article 1 and Article 2 use Dianping dataset.sent: Article 1 and Article 2 use Criteo dataset.sent: Article 1 and Article 2 use Bing News dataset.sent: Article 1 and Article 2 use Amazon dataset.sent: Article 1 and Article 2 use MovieLens 20M dataset. sent: Article 1 uses iPinYou dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Company  dataset.sent: Article 1 and Article 2 use Dianping dataset.sent: Article 1 and Article 2 use Criteo dataset.sent: Article 1 and Article 2 use Bing News dataset.sent: Article 1 and Article 2 use Amazon dataset.sent: Article 1 and Article 2 use MovieLens 20M dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses iPinYou dataset.']","['sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation.', 'sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation.', 'sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification.', 'sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification.', 'sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification.', 'sent: Article 1 uses JFT-300M Finetuning approach for Image Classification, while Article 2 uses GP EI for Image Classification. sent: Article 1 uses ImageNet JFT-300M Initialization approach for Image Classification, while Article 2 uses GP EI for Image Classification.', 'sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification.', 'sent: Article 1 uses ResNet-1001 approach for Image Classification, while Article 2 uses GP EI for Image Classification.', 'sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification.', 'sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification.', 'sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis.', 'sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis.', 'sent: Article 1 uses Andor et al  approach for Dependency Parsing, while Article 2 uses Arc-hybrid for Dependency Parsing.', 'sent: Article 1 uses Arc-hybrid approach for Dependency Parsing, while Article 2 uses Andor et al  for Dependency Parsing.', 'sent: Article 1 uses IPNN approach for Click-Through Rate Prediction, while Article 2 uses DeepFM for Click-Through Rate Prediction. sent: Article 1 uses PNN approach for Click-Through Rate Prediction, while Article 2 uses DeepFM for Click-Through Rate Prediction. sent: Article 1 uses PNN  approach for Click-Through Rate Prediction, while Article 2 uses DeepFM for Click-Through Rate Prediction. sent: Article 1 uses OPNN approach for Click-Through Rate Prediction, while Article 2 uses DeepFM for Click-Through Rate Prediction.', 'sent: Article 1 uses DeepFM approach for Click-Through Rate Prediction, while Article 2 uses IPNN for Click-Through Rate Prediction. sent: Article 1 uses DeepFM approach for Click-Through Rate Prediction, while Article 2 uses PNN for Click-Through Rate Prediction. sent: Article 1 uses DeepFM approach for Click-Through Rate Prediction, while Article 2 uses PNN  for Click-Through Rate Prediction. sent: Article 1 uses DeepFM approach for Click-Through Rate Prediction, while Article 2 uses OPNN for Click-Through Rate Prediction.']"
38,38,06_paper_06,06_paper_10,66,610,"sent: Article 1 uses CoVERT sourced from fact-checked tweets and evidence from online resources from medical domain of 300 tweets, while Article 2 uses CovidFact sourced from Sub-Reddit from health (Covid related)  domain.","sent: Article 1 uses BM25 combined with T5 as reranking approach for document retrieval, while Article 2 uses Google Search for document retrieval. sent: Article 1 uses Longformer (binary head) approach for rationale selection, while Article 2 uses Sentence-BERT approach for rationale selection. sent: Article 1 uses Longformer (ternary head) approach for verdict prediction, while Article 2 uses RoBERTa (finetuned on GLUE dataset) approach for verdict prediction.","[['1712.02616', '1704.08545'], ['1704.08545', '1707.02968'], ['1701.06264', '1707.02968'], ['1701.06264', '1603.05027'], ['1603.03958', '1406.4773'], ['1611.00144', '1601.02376'], ['1601.02376', '1611.00144'], ['1604.05529', '1805.02474']]","['sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CamVid dataset.', 'sent: Article 1 uses multiple datasets:  CamVid, and  Cityscapes, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.', 'sent: Article 1 uses multiple datasets:  CIFAR-10, and  SVHN, while Article 2 uses multiple datasets:  ImageNet, and  PASCAL VOC 2012.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses SVHN dataset, while Article 2 uses CIFAR-100 dataset.', 'sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.', 'sent: Article 1 and Article 2 use Company  dataset.sent: Article 1 and Article 2 use iPinYou dataset.sent: Article 1 and Article 2 use Criteo dataset. sent: Article 1 uses multiple datasets:  Amazon,  Bing News,  Dianping, and  MovieLens 20M, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Company  dataset.sent: Article 1 and Article 2 use iPinYou dataset.sent: Article 1 and Article 2 use Criteo dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Amazon,  Bing News,  Dianping, and  MovieLens 20M.', 'sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 uses UD dataset, while Article 2 uses multiple datasets:  MR,  IMDb, and  CoNLL 2003  English .']","['sent: Article 1 uses Mapillary approach for Semantic Segmentation, while Article 2 uses ICNet for Semantic Segmentation.', 'sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses JFT-300M Finetuning for Semantic Segmentation. sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses ImageNet JFT-300M Initialization for Semantic Segmentation.', 'sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses JFT-300M Finetuning for Image Classification. sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses ImageNet JFT-300M Initialization for Image Classification.', 'sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses ResNet-1001 for Image Classification.', 'sent: Article 1 uses Template adaptation approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.', 'sent: Article 1 uses IPNN approach for Click-Through Rate Prediction, while Article 2 uses FNN for Click-Through Rate Prediction. sent: Article 1 uses PNN approach for Click-Through Rate Prediction, while Article 2 uses FNN for Click-Through Rate Prediction. sent: Article 1 uses PNN  approach for Click-Through Rate Prediction, while Article 2 uses FNN for Click-Through Rate Prediction. sent: Article 1 uses OPNN approach for Click-Through Rate Prediction, while Article 2 uses FNN for Click-Through Rate Prediction.', 'sent: Article 1 uses FNN approach for Click-Through Rate Prediction, while Article 2 uses IPNN for Click-Through Rate Prediction. sent: Article 1 uses FNN approach for Click-Through Rate Prediction, while Article 2 uses PNN for Click-Through Rate Prediction. sent: Article 1 uses FNN approach for Click-Through Rate Prediction, while Article 2 uses PNN  for Click-Through Rate Prediction. sent: Article 1 uses FNN approach for Click-Through Rate Prediction, while Article 2 uses OPNN for Click-Through Rate Prediction.', 'sent: Article 1 uses Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses S-LSTM for Part-Of-Speech Tagging.']"
39,39,06_paper_07,06_paper_08,67,68,"sent: Article 1 uses PubHealth sourced from websites and news/news reviews websites from health topics and biomedicine domain of 11832 claims, while Article 2 uses ClimateFever from climate related topics domain of 1500 sentences.","sent: Article 1 uses Sentence-BERT approach for rationale selection. sent: Article 1 uses SciBERT approach for verdict prediction, while Article 2 uses ClimateBERT approach for verdict prediction.","[['1511.06038', '1606.01549'], ['1511.06038', '1503.03244'], ['1606.01549', '1511.06038'], ['1606.01549', '1503.03244'], ['1503.03244', '1606.01549'], ['1503.03244', '1511.06038'], ['1901.11504', '1802.05577'], ['1901.11504', '1603.06021'], ['1901.11504', '1508.05326'], ['1802.05577', '1901.11504'], ['1603.06021', '1901.11504'], ['1508.05326', '1901.11504'], ['1509.01626', '1811.09386'], ['1509.01626', '1607.01759'], ['1811.09386', '1509.01626'], ['1607.01759', '1509.01626'], ['1607.01759', '1901.11504'], ['1901.11504', '1805.02474'], ['1901.11504', '1811.09386'], ['1901.11504', '1509.01626'], ['1901.11504', '1412.1058'], ['1901.11504', '1607.01759'], ['1901.11504', '1812.01207'], ['1805.02474', '1509.01626'], ['1509.01626', '1412.1058'], ['1509.01626', '1805.02474'], ['1509.01626', '1812.01207'], ['1412.1058', '1509.01626'], ['1808.01558', '1804.03786'], ['1711.03953', '1609.07959'], ['1609.07959', '1711.03953'], ['1609.07959', '1808.10143'], ['1808.10143', '1609.07959'], ['1607.04492', '1709.04348'], ['1709.04348', '1607.04492']]","['sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.', 'sent: Article 1 uses multiple datasets:  WikiQA, and  QASent, while Article 2 uses SemEvalCQA dataset.', 'sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses multiple datasets:  WikiQA, and  QASent.', 'sent: Article 1 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test, while Article 2 uses SemEvalCQA dataset.', 'sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses multiple datasets:  CNN   Daily Mail,  Quasar, and  Children s Book Test.', 'sent: Article 1 uses SemEvalCQA dataset, while Article 2 uses multiple datasets:  WikiQA, and  QASent.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  MultiNLI,  SST-2 Binary classification,  Quora Question Pairs, and  SciTail.', 'sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset. sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification, and  Yelp Binary classification, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers.', 'sent: Article 1 and Article 2 use Yelp Fine-grained classification dataset.sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset.sent: Article 1 and Article 2 use Yelp Binary classification dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Sogou News,  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers.', 'sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset. sent: Article 1 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers, while Article 2 uses multiple datasets:  Yelp Fine-grained classification, and  Yelp Binary classification.', 'sent: Article 1 and Article 2 use Yelp Fine-grained classification dataset.sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset.sent: Article 1 and Article 2 use Yelp Binary classification dataset. sent: Article 1 uses multiple datasets:  Sogou News,  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers, while Article 2 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification.', 'sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.', 'sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  Amazon Review Polarity,  Amazon Review Full,  DBpedia,  AG News, and  Yahoo  Answers.', 'sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification.', 'sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses IMDb dataset.', 'sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers.', 'sent: Article 1 and Article 2 use SST-2 Binary classification dataset. sent: Article 1 uses multiple datasets:  MultiNLI,  Quora Question Pairs,  SciTail, and  SNLI, while Article 2 uses SemEval 2018 Task 1E-c dataset.', 'sent: Article 1 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification.', 'sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification, while Article 2 uses IMDb dataset.', 'sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.', 'sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification, while Article 2 uses multiple datasets:  SST-2 Binary classification, and  SemEval 2018 Task 1E-c.', 'sent: Article 1 uses IMDb dataset, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  DBpedia,  AG News, and  Yelp Binary classification.', 'sent: Article 1 and Article 2 use AFLW2000 dataset.', 'sent: Article 1 uses multiple datasets:  Penn Treebank  Word Level , and  WikiText-2, while Article 2 uses multiple datasets:  enwiki8,  Text8, and  Hutter Prize.', 'sent: Article 1 uses multiple datasets:  enwiki8,  Text8, and  Hutter Prize, while Article 2 uses multiple datasets:  Penn Treebank  Word Level , and  WikiText-2.', 'sent: Article 1 uses multiple datasets:  enwiki8,  Text8, and  Hutter Prize, while Article 2 uses multiple datasets:  Penn Treebank  Word Level ,  WikiText-2, and  Penn Treebank.', 'sent: Article 1 uses multiple datasets:  Penn Treebank  Word Level ,  WikiText-2, and  Penn Treebank, while Article 2 uses multiple datasets:  enwiki8,  Text8, and  Hutter Prize.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Quora Question Pairs dataset.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses Quora Question Pairs dataset, while Article 2 does not use specific datasets for experiments.']","['sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses GA for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses GA for Question Answering.', 'sent: Article 1 uses LSTM approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses Attentive LSTM approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses LSTM  lexical overlap   dist output  approach for Question Answering, while Article 2 uses ARC-II for Question Answering.', 'sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering.', 'sent: Article 1 uses GA reader approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses NSE approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses GA   feature   fix L w  approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses GA Reader approach for Question Answering, while Article 2 uses ARC-II for Question Answering. sent: Article 1 uses GA approach for Question Answering, while Article 2 uses ARC-II for Question Answering.', 'sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses GA reader for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses NSE for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses GA   feature   fix L w  for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses GA Reader for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses GA for Question Answering.', 'sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses LSTM for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses Attentive LSTM for Question Answering. sent: Article 1 uses ARC-II approach for Question Answering, while Article 2 uses LSTM  lexical overlap   dist output  for Question Answering.', 'sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 450D DR-BiLSTM Ensemble for Natural Language Inference.', 'sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 300D LSTM encoders for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 300D SPINN-PI encoders for Natural Language Inference.', 'sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses Unlexicalized features for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses 100D LSTM encoders for Natural Language Inference. sent: Article 1 uses MT-DNN approach for Natural Language Inference, while Article 2 uses   Unigram and bigram features for Natural Language Inference.', 'sent: Article 1 uses 450D DR-BiLSTM approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses 450D DR-BiLSTM Ensemble approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference.', 'sent: Article 1 uses 300D LSTM encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses 300D SPINN-PI encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference.', 'sent: Article 1 uses Unlexicalized features approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses 100D LSTM encoders approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference. sent: Article 1 uses   Unigram and bigram features approach for Natural Language Inference, while Article 2 uses MT-DNN for Natural Language Inference.', 'sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Text Classification, while Article 2 uses EXAM for Text Classification.', 'sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Text Classification, while Article 2 uses FastText for Text Classification. sent: Article 1 uses Char-level CNN approach for Text Classification, while Article 2 uses fastText for Text Classification.', 'sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis. sent: Article 1 uses EXAM approach for Text Classification, while Article 2 uses Char-level CNN for Text Classification.', 'sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis. sent: Article 1 uses FastText approach for Text Classification, while Article 2 uses Char-level CNN for Text Classification. sent: Article 1 uses fastText approach for Text Classification, while Article 2 uses Char-level CNN for Text Classification.', 'sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses MT-DNN for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses MT-DNN for Sentiment Analysis.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses seq2-bown-CNN for Sentiment Analysis.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis.', 'sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis.', 'sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses seq2-bown-CNN for Sentiment Analysis.', 'sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis.', 'sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis.', 'sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis.', 'sent: Article 1 uses MCL approach for Face Alignment, while Article 2 uses Nonlinear 3D Face Morphable Model for Face Alignment.', 'sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS   dynamic eval approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-MoS approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling.', 'sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS   dynamic eval for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-MoS for Language Modelling.', 'sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses Large mLSTM  emb  WN  VD approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses Unregularised mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC x5 for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses LSTM Encoder-Decoder   LSTM-LM for Language Modelling. sent: Article 1 uses Large mLSTM approach for Language Modelling, while Article 2 uses AWD-LSTM-DOC for Language Modelling.', 'sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC x5 approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses LSTM Encoder-Decoder   LSTM-LM approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses Large mLSTM  emb  WN  VD for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses Unregularised mLSTM for Language Modelling. sent: Article 1 uses AWD-LSTM-DOC approach for Language Modelling, while Article 2 uses Large mLSTM for Language Modelling.', 'sent: Article 1 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention approach for Natural Language Inference, while Article 2 uses 448D Densely Interactive Inference Network  DIIN  code  Ensemble for Natural Language Inference. sent: Article 1 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention approach for Natural Language Inference, while Article 2 uses DIIN for Natural Language Inference. sent: Article 1 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention approach for Natural Language Inference, while Article 2 uses 448D Densely Interactive Inference Network  DIIN  code  for Natural Language Inference. sent: Article 1 uses 300D NTI-SLSTM-LSTM encoders approach for Natural Language Inference, while Article 2 uses 448D Densely Interactive Inference Network  DIIN  code  Ensemble for Natural Language Inference. sent: Article 1 uses 300D NTI-SLSTM-LSTM encoders approach for Natural Language Inference, while Article 2 uses DIIN for Natural Language Inference. sent: Article 1 uses 300D NTI-SLSTM-LSTM encoders approach for Natural Language Inference, while Article 2 uses 448D Densely Interactive Inference Network  DIIN  code  for Natural Language Inference.', 'sent: Article 1 uses 448D Densely Interactive Inference Network  DIIN  code  Ensemble approach for Natural Language Inference, while Article 2 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention for Natural Language Inference. sent: Article 1 uses 448D Densely Interactive Inference Network  DIIN  code  Ensemble approach for Natural Language Inference, while Article 2 uses 300D NTI-SLSTM-LSTM encoders for Natural Language Inference. sent: Article 1 uses DIIN approach for Natural Language Inference, while Article 2 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention for Natural Language Inference. sent: Article 1 uses DIIN approach for Natural Language Inference, while Article 2 uses 300D NTI-SLSTM-LSTM encoders for Natural Language Inference. sent: Article 1 uses 448D Densely Interactive Inference Network  DIIN  code  approach for Natural Language Inference, while Article 2 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention for Natural Language Inference. sent: Article 1 uses 448D Densely Interactive Inference Network  DIIN  code  approach for Natural Language Inference, while Article 2 uses 300D NTI-SLSTM-LSTM encoders for Natural Language Inference.']"
40,40,06_paper_07,06_paper_09,67,69,"sent: Article 1 uses PubHealth sourced from websites and news/news reviews websites from health topics and biomedicine domain of 11832 claims, while Article 2 uses HealthVer from health related domain of 14330 evidence-claim pairs.","sent: Article 1 uses Sentence-BERT approach for rationale selection. sent: Article 1 uses SciBERT approach for verdict prediction, while Article 2 uses T5-base approach for verdict prediction.","[['1804.09337', '1504.01013'], ['1504.01013', '1804.09337'], ['1506.03767', '1301.3557'], ['1506.03767', '1206.2944'], ['1301.3557', '1506.03767'], ['1301.3557', '1512.03385'], ['1301.3557', '1611.05431'], ['1512.03385', '1301.3557'], ['1512.03385', '1206.2944'], ['1611.05431', '1206.2944'], ['1611.05431', '1301.3557'], ['1206.2944', '1512.03385'], ['1206.2944', '1611.05431'], ['1509.01626', '1607.01759'], ['1607.01759', '1509.01626'], ['1607.01759', '1901.11504'], ['1901.11504', '1607.01759'], ['1607.04492', '1512.08422'], ['1512.08422', '1607.04492']]","['sent: Article 1 uses multiple datasets:  PASCAL VOC 2012, and  Cityscapes, while Article 2 uses PASCAL Context dataset.', 'sent: Article 1 uses PASCAL Context dataset, while Article 2 uses multiple datasets:  PASCAL VOC 2012, and  Cityscapes.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses SVHN dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use CIFAR-100 dataset. sent: Article 1 uses SVHN dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  SVHN, and  CIFAR-100, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.', 'sent: Article 1 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100, while Article 2 uses ImageNet dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses multiple datasets:  SVHN, and  CIFAR-100.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses ImageNet dataset, while Article 2 uses CIFAR-10 dataset.', 'sent: Article 1 uses ImageNet dataset, while Article 2 uses multiple datasets:  CIFAR-10,  SVHN, and  CIFAR-100.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.', 'sent: Article 1 uses CIFAR-10 dataset, while Article 2 uses ImageNet dataset.', 'sent: Article 1 and Article 2 use Yelp Fine-grained classification dataset.sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset.sent: Article 1 and Article 2 use Yelp Binary classification dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Sogou News,  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers.', 'sent: Article 1 and Article 2 use Yelp Fine-grained classification dataset.sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset.sent: Article 1 and Article 2 use Yelp Binary classification dataset. sent: Article 1 uses multiple datasets:  Sogou News,  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers, while Article 2 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification.', 'sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset.']","['sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation.', 'sent: Article 1 uses Piecewise approach for Semantic Segmentation, while Article 2 uses Smooth Network with Channel Attention Block for Semantic Segmentation.', 'sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification.', 'sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses GP EI for Image Classification.', 'sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses Spectral Representations for Convolutional Neural Networks for Image Classification.', 'sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification.', 'sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification.', 'sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification.', 'sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses GP EI for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses GP EI for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses GP EI for Image Classification.', 'sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses GP EI for Image Classification.', 'sent: Article 1 uses ResNeXt-101 approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification.', 'sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification.', 'sent: Article 1 uses GP EI approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification.', 'sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Text Classification, while Article 2 uses FastText for Text Classification. sent: Article 1 uses Char-level CNN approach for Text Classification, while Article 2 uses fastText for Text Classification.', 'sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis. sent: Article 1 uses FastText approach for Text Classification, while Article 2 uses Char-level CNN for Text Classification. sent: Article 1 uses fastText approach for Text Classification, while Article 2 uses Char-level CNN for Text Classification.', 'sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses MT-DNN for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses MT-DNN for Sentiment Analysis.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis.', 'sent: Article 1 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention approach for Natural Language Inference, while Article 2 uses 300D Tree-based CNN encoders for Natural Language Inference. sent: Article 1 uses 300D NTI-SLSTM-LSTM encoders approach for Natural Language Inference, while Article 2 uses 300D Tree-based CNN encoders for Natural Language Inference.', 'sent: Article 1 uses 300D Tree-based CNN encoders approach for Natural Language Inference, while Article 2 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention for Natural Language Inference. sent: Article 1 uses 300D Tree-based CNN encoders approach for Natural Language Inference, while Article 2 uses 300D NTI-SLSTM-LSTM encoders for Natural Language Inference.']"
41,41,06_paper_07,06_paper_10,67,610,"sent: Article 1 uses PubHealth sourced from websites and news/news reviews websites from health topics and biomedicine domain of 11832 claims, while Article 2 uses CovidFact sourced from Sub-Reddit from health (Covid related)  domain.","sent: Article 2 uses Google Search for document retrieval. sent: Article 1 uses Sentence-BERT approach for rationale selection, while Article 2 uses Sentence-BERT approach for rationale selection. sent: Article 1 uses SciBERT approach for verdict prediction, while Article 2 uses RoBERTa (finetuned on GLUE dataset) approach for verdict prediction.","[['1804.09337', '1704.08545'], ['1704.08545', '1804.09337'], ['1701.06264', '1506.03767'], ['1701.06264', '1512.03385'], ['1701.06264', '1611.05431'], ['1506.03767', '1701.06264'], ['1512.03385', '1701.06264'], ['1511.02274', '1603.02814'], ['1512.03385', '1506.01497'], ['1506.01497', '1612.03144'], ['1506.01497', '1512.03385'], ['1505.04597', '1802.06955'], ['1802.06955', '1505.04597'], ['1603.05474', '1603.03958'], ['1603.05474', '1406.4773']]","['sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses PASCAL VOC 2012 dataset, while Article 2 uses CamVid dataset.', 'sent: Article 1 and Article 2 use Cityscapes dataset. sent: Article 1 uses CamVid dataset, while Article 2 uses PASCAL VOC 2012 dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses SVHN dataset, while Article 2 uses CIFAR-100 dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses SVHN dataset, while Article 2 uses multiple datasets:  COCO, and  PASCAL VOC 2007.', 'sent: Article 1 uses multiple datasets:  CIFAR-10, and  SVHN, while Article 2 uses ImageNet dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 uses SVHN dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses multiple datasets:  COCO, and  PASCAL VOC 2007, while Article 2 uses SVHN dataset.', 'sent: Article 1 and Article 2 use COCO Visual Question Answering  VQA  real images 1 0 open ended dataset.', 'sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 uses multiple datasets:  CIFAR-10, and  COCO, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses PASCAL VOC 2007 dataset, while Article 2 uses COCO dataset.', 'sent: Article 1 and Article 2 use PASCAL VOC 2007 dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  CIFAR-10, and  COCO.', 'sent: Article 1 and Article 2 use LUNA dataset.sent: Article 1 and Article 2 use DRIVE dataset.sent: Article 1 and Article 2 use CHASE DB1 dataset.sent: Article 1 and Article 2 use STARE dataset.sent: Article 1 and Article 2 use Kaggle Skin Lesion Segmentation dataset. sent: Article 1 uses multiple datasets:  PhC-U373,  ISBI 2012 EM Segmentation,  DIC-HeLa, and  CT-150, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use LUNA dataset.sent: Article 1 and Article 2 use DRIVE dataset.sent: Article 1 and Article 2 use CHASE DB1 dataset.sent: Article 1 and Article 2 use STARE dataset.sent: Article 1 and Article 2 use Kaggle Skin Lesion Segmentation dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  PhC-U373,  ISBI 2012 EM Segmentation,  DIC-HeLa, and  CT-150.', 'sent: Article 1 and Article 2 use IJB-A dataset.', 'sent: Article 1 uses IJB-A dataset, while Article 2 uses Labeled Faces in the Wild dataset.']","['sent: Article 1 uses Smooth Network with Channel Attention Block approach for Semantic Segmentation, while Article 2 uses ICNet for Semantic Segmentation.', 'sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses Smooth Network with Channel Attention Block for Semantic Segmentation.', 'sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses Spectral Representations for Convolutional Neural Networks for Image Classification.', 'sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses ResNet-101 for Image Classification. sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses ResNet for Image Classification. sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Image Classification.', 'sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses ResNeXt-101 for Image Classification.', 'sent: Article 1 uses Spectral Representations for Convolutional Neural Networks approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification.', 'sent: Article 1 uses ResNet-101 approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification. sent: Article 1 uses ResNet approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification.', 'sent: Article 1 uses SAN approach for Visual Question Answering, while Article 2 uses CNN-RNN for Visual Question Answering.', 'sent: Article 1 uses ResNet-101 approach for Object Detection, while Article 2 uses Faster R-CNN for Object Detection. sent: Article 1 uses ResNet approach for Object Detection, while Article 2 uses Faster R-CNN for Object Detection. sent: Article 1 uses Faster R-CNN   box refinement   context   multi-scale testing approach for Object Detection, while Article 2 uses Faster R-CNN for Object Detection.', 'sent: Article 1 uses Faster R-CNN approach for Object Detection, while Article 2 uses Faster R-CNN   FPN for Object Detection.', 'sent: Article 1 uses Faster R-CNN approach for Object Detection, while Article 2 uses ResNet-101 for Object Detection. sent: Article 1 uses Faster R-CNN approach for Object Detection, while Article 2 uses ResNet for Object Detection. sent: Article 1 uses Faster R-CNN approach for Object Detection, while Article 2 uses Faster R-CNN   box refinement   context   multi-scale testing for Object Detection.', 'sent: Article 1 uses U-Net approach for Lung Nodule Segmentation, while Article 2 uses R2U-Net for Lung Nodule Segmentation. sent: Article 1 uses U-Net approach for Skin Cancer Segmentation, while Article 2 uses R2U-Net for Skin Cancer Segmentation. sent: Article 1 uses U-Net approach for Retinal Vessel Segmentation, while Article 2 uses R2U-Net for Retinal Vessel Segmentation.', 'sent: Article 1 uses R2U-Net approach for Lung Nodule Segmentation, while Article 2 uses U-Net for Lung Nodule Segmentation. sent: Article 1 uses R2U-Net approach for Skin Cancer Segmentation, while Article 2 uses U-Net for Skin Cancer Segmentation. sent: Article 1 uses R2U-Net approach for Retinal Vessel Segmentation, while Article 2 uses U-Net for Retinal Vessel Segmentation.', 'sent: Article 1 uses NAN approach for Face Verification, while Article 2 uses Template adaptation for Face Verification.', 'sent: Article 1 uses NAN approach for Face Verification, while Article 2 uses DeepId2 for Face Verification.']"
42,42,06_paper_08,06_paper_09,68,69,"sent: Article 1 uses ClimateFever from climate related topics domain of 1500 sentences, while Article 2 uses HealthVer from health related domain of 14330 evidence-claim pairs.","sent: Article 1 uses ClimateBERT approach for verdict prediction, while Article 2 uses T5-base approach for verdict prediction.","[['1509.01626', '1607.01759'], ['1811.09386', '1607.01759'], ['1607.01759', '1509.01626'], ['1607.01759', '1811.09386'], ['1607.01759', '1901.11504'], ['1607.01759', '1805.02474'], ['1607.01759', '1412.1058'], ['1607.01759', '1812.01207'], ['1901.11504', '1607.01759'], ['1805.02474', '1607.01759'], ['1412.1058', '1607.01759'], ['1812.01207', '1607.01759'], ['1607.04492', '1512.08422'], ['1709.04348', '1512.08422'], ['1512.08422', '1607.04492'], ['1512.08422', '1709.04348']]","['sent: Article 1 and Article 2 use Yelp Fine-grained classification dataset.sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset.sent: Article 1 and Article 2 use Yelp Binary classification dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Sogou News,  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers.', 'sent: Article 1 and Article 2 use Amazon Review Polarity dataset.sent: Article 1 and Article 2 use Amazon Review Full dataset.sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset.sent: Article 1 and Article 2 use Yahoo  Answers dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Sogou News,  Yelp Fine-grained classification, and  Yelp Binary classification.', 'sent: Article 1 and Article 2 use Yelp Fine-grained classification dataset.sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset.sent: Article 1 and Article 2 use Yelp Binary classification dataset. sent: Article 1 uses multiple datasets:  Sogou News,  Amazon Review Polarity,  Amazon Review Full, and  Yahoo  Answers, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Amazon Review Polarity dataset.sent: Article 1 and Article 2 use Amazon Review Full dataset.sent: Article 1 and Article 2 use DBpedia dataset.sent: Article 1 and Article 2 use AG News dataset.sent: Article 1 and Article 2 use Yahoo  Answers dataset. sent: Article 1 uses multiple datasets:  Sogou News,  Yelp Fine-grained classification, and  Yelp Binary classification, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers, while Article 2 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification.', 'sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers, while Article 2 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank.', 'sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers, while Article 2 uses IMDb dataset.', 'sent: Article 1 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers, while Article 2 uses multiple datasets:  SST-2 Binary classification, and  SemEval 2018 Task 1E-c.', 'sent: Article 1 uses multiple datasets:  Quora Question Pairs,  SNLI,  MultiNLI,  SciTail, and  SST-2 Binary classification, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers.', 'sent: Article 1 uses multiple datasets:  MR,  IMDb,  CoNLL 2003  English , and  Penn Treebank, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers.', 'sent: Article 1 uses IMDb dataset, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers.', 'sent: Article 1 uses multiple datasets:  SST-2 Binary classification, and  SemEval 2018 Task 1E-c, while Article 2 uses multiple datasets:  Yelp Fine-grained classification,  Amazon Review Polarity,  Amazon Review Full,  Sogou News,  DBpedia,  AG News,  Yelp Binary classification, and  Yahoo  Answers.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 uses Quora Question Pairs dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use SNLI dataset.', 'sent: Article 1 and Article 2 use SNLI dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses Quora Question Pairs dataset.']","['sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis. sent: Article 1 uses Char-level CNN approach for Text Classification, while Article 2 uses FastText for Text Classification. sent: Article 1 uses Char-level CNN approach for Text Classification, while Article 2 uses fastText for Text Classification.', 'sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses EXAM approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis. sent: Article 1 uses EXAM approach for Text Classification, while Article 2 uses FastText for Text Classification. sent: Article 1 uses EXAM approach for Text Classification, while Article 2 uses fastText for Text Classification.', 'sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses Char-level CNN for Sentiment Analysis. sent: Article 1 uses FastText approach for Text Classification, while Article 2 uses Char-level CNN for Text Classification. sent: Article 1 uses fastText approach for Text Classification, while Article 2 uses Char-level CNN for Text Classification.', 'sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses EXAM for Sentiment Analysis. sent: Article 1 uses FastText approach for Text Classification, while Article 2 uses EXAM for Text Classification. sent: Article 1 uses fastText approach for Text Classification, while Article 2 uses EXAM for Text Classification.', 'sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses MT-DNN for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses MT-DNN for Sentiment Analysis.', 'sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses S-LSTM for Sentiment Analysis.', 'sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses seq2-bown-CNN for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses seq2-bown-CNN for Sentiment Analysis.', 'sent: Article 1 uses FastText approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis. sent: Article 1 uses fastText approach for Sentiment Analysis, while Article 2 uses Transformer  finetune  for Sentiment Analysis.', 'sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses MT-DNN approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis.', 'sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses S-LSTM approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis.', 'sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses seq2-bown-CNN approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis.', 'sent: Article 1 uses Transformer  finetune  approach for Sentiment Analysis, while Article 2 uses FastText for Sentiment Analysis. sent: Article 1 uses Transformer  finetune  approach for Sentiment Analysis, while Article 2 uses fastText for Sentiment Analysis.', 'sent: Article 1 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention approach for Natural Language Inference, while Article 2 uses 300D Tree-based CNN encoders for Natural Language Inference. sent: Article 1 uses 300D NTI-SLSTM-LSTM encoders approach for Natural Language Inference, while Article 2 uses 300D Tree-based CNN encoders for Natural Language Inference.', 'sent: Article 1 uses 448D Densely Interactive Inference Network  DIIN  code  Ensemble approach for Natural Language Inference, while Article 2 uses 300D Tree-based CNN encoders for Natural Language Inference. sent: Article 1 uses DIIN approach for Natural Language Inference, while Article 2 uses 300D Tree-based CNN encoders for Natural Language Inference. sent: Article 1 uses 448D Densely Interactive Inference Network  DIIN  code  approach for Natural Language Inference, while Article 2 uses 300D Tree-based CNN encoders for Natural Language Inference.', 'sent: Article 1 uses 300D Tree-based CNN encoders approach for Natural Language Inference, while Article 2 uses 300D Full tree matching NTI-SLSTM-LSTM w  global attention for Natural Language Inference. sent: Article 1 uses 300D Tree-based CNN encoders approach for Natural Language Inference, while Article 2 uses 300D NTI-SLSTM-LSTM encoders for Natural Language Inference.', 'sent: Article 1 uses 300D Tree-based CNN encoders approach for Natural Language Inference, while Article 2 uses 448D Densely Interactive Inference Network  DIIN  code  Ensemble for Natural Language Inference. sent: Article 1 uses 300D Tree-based CNN encoders approach for Natural Language Inference, while Article 2 uses DIIN for Natural Language Inference. sent: Article 1 uses 300D Tree-based CNN encoders approach for Natural Language Inference, while Article 2 uses 448D Densely Interactive Inference Network  DIIN  code  for Natural Language Inference.']"
43,43,06_paper_08,06_paper_10,68,610,"sent: Article 1 uses ClimateFever from climate related topics domain of 1500 sentences, while Article 2 uses CovidFact sourced from Sub-Reddit from health (Covid related)  domain.","sent: Article 2 uses Google Search for document retrieval. sent:  Article 2 uses Sentence-BERT approach for rationale selection. sent: Article 1 uses ClimateBERT approach for verdict prediction, while Article 2 uses RoBERTa (finetuned on GLUE dataset) approach for verdict prediction.","[['1604.05529', '1805.02474']]","['sent: Article 1 and Article 2 use Penn Treebank dataset. sent: Article 1 uses UD dataset, while Article 2 uses multiple datasets:  MR,  IMDb, and  CoNLL 2003  English .']","['sent: Article 1 uses Bi-LSTM approach for Part-Of-Speech Tagging, while Article 2 uses S-LSTM for Part-Of-Speech Tagging.']"
44,44,06_paper_09,06_paper_10,69,610,"sent: Article 1 uses HealthVer from health related domain of 14330 evidence-claim pairs, while Article 2 uses CovidFact sourced from Sub-Reddit from health (Covid related)  domain.","sent: Article 2 uses Google Search for document retrieval. sent:  Article 2 uses Sentence-BERT approach for rationale selection. sent: Article 1 uses T5-base approach for verdict prediction, while Article 2 uses RoBERTa (finetuned on GLUE dataset) approach for verdict prediction.","[['1704.08545', '1504.01013'], ['1701.06264', '1301.3557'], ['1701.06264', '1206.2944'], ['1301.3557', '1701.06264'], ['1609.05158', '1511.02228'], ['1511.02228', '1609.05158'], ['1506.08959', '1407.3867'], ['1407.3867', '1506.08959'], ['1611.00144', '1601.02376'], ['1601.02376', '1703.04247'], ['1601.02376', '1611.00144'], ['1703.04247', '1601.02376'], ['1403.6652', '1802.08352'], ['1802.08352', '1403.6652']]","['sent: Article 1 uses multiple datasets:  CamVid, and  Cityscapes, while Article 2 uses PASCAL Context dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses CIFAR-100 dataset.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset. sent: Article 1 uses SVHN dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use CIFAR-10 dataset.sent: Article 1 and Article 2 use SVHN dataset. sent: Article 1 uses CIFAR-100 dataset, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 uses multiple datasets:  Ultra Video Group HD - 4x upscaling,  Vid4 - 4x upscaling, and  Xiph HD - 4x upscaling, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Set14 - 4x upscaling dataset.sent: Article 1 and Article 2 use BSD100 - 4x upscaling dataset.sent: Article 1 and Article 2 use Set5 - 4x upscaling dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Ultra Video Group HD - 4x upscaling,  Vid4 - 4x upscaling, and  Xiph HD - 4x upscaling.', 'sent: Article 1 uses CompCars dataset, while Article 2 uses  CUB-200-2011 dataset.', 'sent: Article 1 uses  CUB-200-2011 dataset, while Article 2 uses CompCars dataset.', 'sent: Article 1 and Article 2 use Company  dataset.sent: Article 1 and Article 2 use iPinYou dataset.sent: Article 1 and Article 2 use Criteo dataset. sent: Article 1 uses multiple datasets:  Amazon,  Bing News,  Dianping, and  MovieLens 20M, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Company  dataset.sent: Article 1 and Article 2 use Criteo dataset. sent: Article 1 uses iPinYou dataset, while Article 2 uses multiple datasets:  Amazon,  Bing News,  Dianping, and  MovieLens 20M.', 'sent: Article 1 and Article 2 use Company  dataset.sent: Article 1 and Article 2 use iPinYou dataset.sent: Article 1 and Article 2 use Criteo dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  Amazon,  Bing News,  Dianping, and  MovieLens 20M.', 'sent: Article 1 and Article 2 use Company  dataset.sent: Article 1 and Article 2 use Criteo dataset. sent: Article 1 uses multiple datasets:  Amazon,  Bing News,  Dianping, and  MovieLens 20M, while Article 2 uses iPinYou dataset.', 'sent: Article 1 and Article 2 use Citeseer dataset.sent: Article 1 and Article 2 use Pubmed dataset.sent: Article 1 and Article 2 use Cora dataset. sent: Article 1 uses multiple datasets:  BlogCatalog,  NELL, and  Wikipedia, while Article 2 does not use specific datasets for experiments.', 'sent: Article 1 and Article 2 use Citeseer dataset.sent: Article 1 and Article 2 use Pubmed dataset.sent: Article 1 and Article 2 use Cora dataset. sent: Article 1 does not use specific datasets for experiments, while Article 2 uses multiple datasets:  BlogCatalog,  NELL, and  Wikipedia.']","['sent: Article 1 uses ICNet approach for Semantic Segmentation, while Article 2 uses Piecewise for Semantic Segmentation.', 'sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses Stochastic Pooling for Image Classification.', 'sent: Article 1 uses CLS-GAN approach for Image Classification, while Article 2 uses GP EI for Image Classification.', 'sent: Article 1 uses Stochastic Pooling approach for Image Classification, while Article 2 uses CLS-GAN for Image Classification.', 'sent: Article 1 uses ESPCN approach for Image Super-Resolution, while Article 2 uses IA for Image Super-Resolution. sent: Article 1 uses bicubic approach for Image Super-Resolution, while Article 2 uses IA for Image Super-Resolution.', 'sent: Article 1 uses IA approach for Image Super-Resolution, while Article 2 uses ESPCN for Image Super-Resolution. sent: Article 1 uses IA approach for Image Super-Resolution, while Article 2 uses bicubic for Image Super-Resolution.', 'sent: Article 1 uses AlexNet approach for Fine-Grained Image Classification, while Article 2 uses Part RCNN for Fine-Grained Image Classification. sent: Article 1 uses GoogLeNet approach for Fine-Grained Image Classification, while Article 2 uses Part RCNN for Fine-Grained Image Classification.', 'sent: Article 1 uses Part RCNN approach for Fine-Grained Image Classification, while Article 2 uses AlexNet for Fine-Grained Image Classification. sent: Article 1 uses Part RCNN approach for Fine-Grained Image Classification, while Article 2 uses GoogLeNet for Fine-Grained Image Classification.', 'sent: Article 1 uses IPNN approach for Click-Through Rate Prediction, while Article 2 uses FNN for Click-Through Rate Prediction. sent: Article 1 uses PNN approach for Click-Through Rate Prediction, while Article 2 uses FNN for Click-Through Rate Prediction. sent: Article 1 uses PNN  approach for Click-Through Rate Prediction, while Article 2 uses FNN for Click-Through Rate Prediction. sent: Article 1 uses OPNN approach for Click-Through Rate Prediction, while Article 2 uses FNN for Click-Through Rate Prediction.', 'sent: Article 1 uses FNN approach for Click-Through Rate Prediction, while Article 2 uses DeepFM for Click-Through Rate Prediction.', 'sent: Article 1 uses FNN approach for Click-Through Rate Prediction, while Article 2 uses IPNN for Click-Through Rate Prediction. sent: Article 1 uses FNN approach for Click-Through Rate Prediction, while Article 2 uses PNN for Click-Through Rate Prediction. sent: Article 1 uses FNN approach for Click-Through Rate Prediction, while Article 2 uses PNN  for Click-Through Rate Prediction. sent: Article 1 uses FNN approach for Click-Through Rate Prediction, while Article 2 uses OPNN for Click-Through Rate Prediction.', 'sent: Article 1 uses DeepFM approach for Click-Through Rate Prediction, while Article 2 uses FNN for Click-Through Rate Prediction.', 'sent: Article 1 uses DeepWalk approach for Node Classification, while Article 2 uses alpha-LoNGAE for Node Classification.', 'sent: Article 1 uses alpha-LoNGAE approach for Node Classification, while Article 2 uses DeepWalk for Node Classification.']"
