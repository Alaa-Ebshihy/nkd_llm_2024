{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6-pPG1-GcIL"
      },
      "source": [
        "# Helping functions and Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIz6QrH0Ggfu"
      },
      "source": [
        "## IO Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LzmNMvbGiMI"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Contains some utils functions for io operations.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import csv\n",
        "import requests\n",
        "\n",
        "\n",
        "def path_exits(path):\n",
        "    return os.path.exists(path)\n",
        "\n",
        "\n",
        "def mkdir(path):\n",
        "    if not path_exits(path):\n",
        "        os.mkdir(path)\n",
        "\n",
        "\n",
        "def makedirs(path):\n",
        "    if not path_exits(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\n",
        "def list_files_in_dir(dir):\n",
        "    return [file for file in os.listdir(dir) if is_file(join(dir, file))]\n",
        "\n",
        "\n",
        "def list_directories(dir):\n",
        "    return [subdir for subdir in os.listdir(dir) if os.path.isdir(join(dir, subdir))]\n",
        "\n",
        "\n",
        "def is_file(path):\n",
        "    return os.path.isfile(path)\n",
        "\n",
        "\n",
        "def is_dir(path):\n",
        "    return os.path.isdir(path)\n",
        "\n",
        "\n",
        "def join(path1, path2):\n",
        "    return os.path.join(path1, path2)\n",
        "\n",
        "\n",
        "def write_json(path, dict):\n",
        "    with open(path, 'w') as outfile:\n",
        "        json.dump(dict, outfile, indent=2)\n",
        "    outfile.close()\n",
        "\n",
        "\n",
        "def read_json(path):\n",
        "    with open(path, \"r\") as infile:\n",
        "        data = json.load(infile)\n",
        "    infile.close()\n",
        "    return data\n",
        "\n",
        "\n",
        "def read_file_into_list(input_file):\n",
        "    lines = []\n",
        "    with open(input_file, \"r\") as infile_fp:\n",
        "        for line in infile_fp.readlines():\n",
        "            lines.append(line.strip())\n",
        "    infile_fp.close()\n",
        "    return lines\n",
        "\n",
        "\n",
        "def write_list_to_file(output_file, list):\n",
        "    with open(output_file, \"w\") as outfile_fp:\n",
        "        for line in list:\n",
        "            outfile_fp.write(line + \"\\r\\n\")\n",
        "    outfile_fp.close()\n",
        "\n",
        "\n",
        "def write_text_to_file(output_file, text):\n",
        "    with open(output_file, \"w\") as output_fp:\n",
        "        output_fp.write(text)\n",
        "    output_fp.close()\n",
        "\n",
        "\n",
        "def write_pickle(data, file_path):\n",
        "    pickle.dump(data, open(file_path, \"wb\"))\n",
        "\n",
        "\n",
        "def read_pickle(file_path):\n",
        "    return pickle.load(open(file_path, 'rb'))\n",
        "\n",
        "\n",
        "def write_to_csv(filepath, header, rows, delimiter=','):\n",
        "    with open(filepath, 'w', encoding='UTF8', newline='') as output_fp:\n",
        "        writer = csv.writer(output_fp, delimiter=delimiter)\n",
        "        writer.writerow(header)\n",
        "        writer.writerows(rows)\n",
        "    output_fp.close()\n",
        "\n",
        "def get_request_content(end_point, params):\n",
        "    get_response = requests.get(end_point, params=params)\n",
        "    return get_response.json()\n",
        "\n",
        "\n",
        "def download_file(url, download_path):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n",
        "    }\n",
        "    r = requests.get(url, stream=True, allow_redirects=True, headers=headers)\n",
        "    with open(download_path, 'wb') as f:\n",
        "        for ch in r:\n",
        "            f.write(ch)\n",
        "\n",
        "def write_dict_to_csv(filepath, fields, dict):\n",
        "  with open(filepath, 'w') as csvfile:\n",
        "    # creating a csv dict writer object\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fields)\n",
        "\n",
        "    # writing headers (field names)\n",
        "    writer.writeheader()\n",
        "\n",
        "    # writing data rows\n",
        "    writer.writerows(dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2ZTbSv4Snl3"
      },
      "source": [
        "# Input and output information\n",
        "Include any paths required for input or output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG-yC_wjSsDe"
      },
      "outputs": [],
      "source": [
        "# https://github.com/dwadden/multivers\n",
        "# TODO: set the correct path\n",
        "multivers_root_dir = \"/path/to/multivers\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLVkz050-CoY"
      },
      "source": [
        "## Manually Collected papers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ITO4-YClz1m"
      },
      "source": [
        "#### Interviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7B5Y38clyHP"
      },
      "outputs": [],
      "source": [
        "# /path/to/nkd_llm_2024/: https://anonymous.4open.science/r/nkd_llm_2024-07E6\n",
        "# TODO: set the correct path\n",
        "interviews_papers_base_dir = \"/path/to/nkd_llm_2024/data/interviews\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KJdoLnwCIk-"
      },
      "outputs": [],
      "source": [
        "interviews_claims_base_path = join(interviews_papers_base_dir, 'claims')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDF1rZVXCIlK"
      },
      "outputs": [],
      "source": [
        "interviews_corpus_dir = join(interviews_papers_base_dir, 'corpus')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5tM2QN0uhRK"
      },
      "outputs": [],
      "source": [
        "interviews_predictions_base_path = join(interviews_papers_base_dir, 'predictions')\n",
        "makedirs(interviews_predictions_base_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtqnIBiq1s0x"
      },
      "source": [
        "### Surveys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32ekdkeF1uZb"
      },
      "outputs": [],
      "source": [
        "# /path/to/nkd_llm_2024/: https://anonymous.4open.science/r/nkd_llm_2024-07E6\n",
        "# TODO: set the correct path\n",
        "surveys_papers_base_dir = \"/path/to/nkd_llm_2024/data/surveys\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEPQ7gWNqTgV"
      },
      "outputs": [],
      "source": [
        "surveys_claims_base_path = join(surveys_papers_base_dir, 'claims')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rc1RRjUxqTgb"
      },
      "outputs": [],
      "source": [
        "surveys_corpus_dir = join(surveys_papers_base_dir, 'corpus')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eev9rM6A2CNf"
      },
      "outputs": [],
      "source": [
        "surveys_predictions_base_path = join(surveys_papers_base_dir, 'predictions')\n",
        "makedirs(surveys_predictions_base_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5Dug8czSvXF"
      },
      "source": [
        "# The Predictions commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fTAjlC_S3xX"
      },
      "source": [
        "## Install the libraries\n",
        "this needes to be done every time we re-connect because the installed libraries are lost after the disconnect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AF1SYljJS7mX"
      },
      "outputs": [],
      "source": [
        "!apt-get install python3.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IusHd18ES_cv"
      },
      "outputs": [],
      "source": [
        "!apt-get install python3.8-distutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GC4D5WgPTA9n"
      },
      "outputs": [],
      "source": [
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JamPNBUoTCVP"
      },
      "outputs": [],
      "source": [
        "!update-alternatives --config python3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtHuexxYTEN1"
      },
      "outputs": [],
      "source": [
        "!python3 --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1K2RGEPTFtf"
      },
      "outputs": [],
      "source": [
        "!sudo apt install python3-pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Plagoew4THJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a91eb66-0fa5-47e5-9aee-e82d80ffdad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CSED/PhD/Implementation/Scientific_Articles_KD/fact_checking/multivers\n"
          ]
        }
      ],
      "source": [
        "%cd $multivers_root_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp0GAXD2TIh_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7d3a3a71-723e-4886-86c1-da32610364ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Cython==0.29.21\n",
            "  Downloading Cython-0.29.21-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==1.1.3\n",
            "  Downloading datasets-1.1.3-py3-none-any.whl (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 KB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.19.4\n",
            "  Downloading numpy-1.19.4-cp38-cp38-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.1.5\n",
            "  Downloading pandas-1.1.5-cp38-cp38-manylinux1_x86_64.whl (9.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning==1.2.1\n",
            "  Downloading pytorch_lightning-1.2.1-py3-none-any.whl (814 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m814.2/814.2 KB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytz==2020.4\n",
            "  Downloading pytz-2020.4-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.0/509.0 KB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==0.23.2\n",
            "  Downloading scikit_learn-0.23.2-cp38-cp38-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.6.0\n",
            "  Downloading scipy-1.6.0-cp38-cp38-manylinux1_x86_64.whl (27.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.2/27.2 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy==2.3.5\n",
            "  Downloading spacy-2.3.5-cp38-cp38-manylinux2014_x86_64.whl (10.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scispacy==0.3.0\n",
            "  Downloading scispacy-0.3.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp38-cp38-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp38-cp38-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.8/776.8 MB\u001b[0m \u001b[31m946.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.8.2\n",
            "  Downloading torchvision-0.8.2-cp38-cp38-manylinux1_x86_64.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.49\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.8/69.8 KB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.2.2\n",
            "  Downloading transformers-4.2.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai==0.26.4\n",
            "  Downloading openai-0.26.4.tar.gz (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 KB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/194.6 KB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.17-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 KB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow>=0.17.1\n",
            "  Downloading pyarrow-17.0.0-cp38-cp38-manylinux_2_28_x86_64.whl (40.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests>=2.19.0\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dateutil>=2.7.3\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 KB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard>=2.2.0\n",
            "  Downloading tensorboard-2.14.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyYAML!=5.4.*,>=5.1\n",
            "  Downloading PyYAML-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (746 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m746.5/746.5 KB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 KB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec[http]>=0.8.1\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 KB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Collecting joblib>=0.11\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 KB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting thinc<7.5.0,>=7.4.1\n",
            "  Downloading thinc-7.4.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
            "  Downloading murmurhash-1.0.10-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "  Downloading preshed-3.0.9-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.2/154.2 KB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Collecting wasabi<1.1.0,>=0.4.0\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Collecting srsly<1.1.0,>=1.0.2\n",
            "  Downloading srsly-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (370 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m370.5/370.5 KB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
            "  Downloading cymem-2.0.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy==2.3.5->-r requirements.txt (line 9)) (59.6.0)\n",
            "Collecting blis<0.8.0,>=0.4.0\n",
            "  Downloading blis-0.7.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pysbd\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nmslib>=1.7.3.6\n",
            "  Downloading nmslib-2.1.1-cp38-cp38-manylinux2010_x86_64.whl (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Collecting pillow>=4.1.1\n",
            "  Downloading pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 KB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex!=2019.12.17\n",
            "  Downloading regex-2024.9.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.0/785.0 KB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging\n",
            "  Downloading packaging-24.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.10.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.12.0\n",
            "  Downloading yarl-1.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m461.8/461.8 KB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.7/129.7 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0\n",
            "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting attrs>=17.3.0\n",
            "  Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybind11<2.6.2\n",
            "  Downloading pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 KB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psutil\n",
            "  Downloading psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.5/290.5 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.5->-r requirements.txt (line 4)) (1.16.0)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 KB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
            "  Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 KB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 KB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf>=3.19.6\n",
            "  Downloading protobuf-5.28.2-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 KB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio>=1.48.2\n",
            "  Downloading grpcio-1.66.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting werkzeug>=1.0.1\n",
            "  Downloading werkzeug-3.0.4-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 KB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
            "  Downloading google_auth-2.35.0-py2.py3-none-any.whl (208 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 KB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py>=0.4\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 KB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->-r requirements.txt (line 5)) (0.37.1)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 KB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting click\n",
            "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 KB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting importlib-metadata>=4.4\n",
            "  Downloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
            "Collecting MarkupSafe>=2.1.1\n",
            "  Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\n",
            "Collecting zipp>=3.20\n",
            "  Downloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
            "Collecting pyasn1<0.7.0,>=0.4.6\n",
            "  Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.1/83.1 KB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 KB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai\n",
            "  Building wheel for openai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.26.4-py3-none-any.whl size=67714 sha256=10a52d7808406083115b56061ab27f40bd4046f1a8531685cd7e135a540cea20\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/d8/4e/268f029bd3277c1dd9e8781a0e0296e0a63822665bfa2429fc\n",
            "Successfully built openai\n",
            "Installing collected packages: wasabi, tokenizers, pytz, plac, cymem, zipp, xxhash, urllib3, typing-extensions, tqdm, threadpoolctl, tensorboard-data-server, srsly, regex, PyYAML, python-dateutil, pysbd, pybind11, pyasn1, psutil, protobuf, pillow, packaging, oauthlib, numpy, murmurhash, MarkupSafe, joblib, idna, grpcio, future, fsspec, frozenlist, filelock, dill, Cython, click, charset-normalizer, certifi, catalogue, cachetools, attrs, async-timeout, aiohappyeyeballs, absl-py, werkzeug, torch, scipy, sacremoses, rsa, requests, pyasn1-modules, pyarrow, preshed, pandas, nmslib, multiprocess, multidict, importlib-metadata, blis, aiosignal, yarl, transformers, torchvision, thinc, scikit-learn, requests-oauthlib, markdown, google-auth, datasets, spacy, google-auth-oauthlib, aiohttp, tensorboard, scispacy, openai, pytorch-lightning\n",
            "Successfully installed Cython-0.29.21 MarkupSafe-2.1.5 PyYAML-6.0.2 absl-py-2.1.0 aiohappyeyeballs-2.4.3 aiohttp-3.10.8 aiosignal-1.3.1 async-timeout-4.0.3 attrs-24.2.0 blis-0.7.11 cachetools-5.5.0 catalogue-1.0.2 certifi-2024.8.30 charset-normalizer-3.3.2 click-8.1.7 cymem-2.0.8 datasets-1.1.3 dill-0.3.9 filelock-3.16.1 frozenlist-1.4.1 fsspec-2024.9.0 future-1.0.0 google-auth-2.35.0 google-auth-oauthlib-1.0.0 grpcio-1.66.2 idna-3.10 importlib-metadata-8.5.0 joblib-1.4.2 markdown-3.7 multidict-6.1.0 multiprocess-0.70.17 murmurhash-1.0.10 nmslib-2.1.1 numpy-1.19.4 oauthlib-3.2.2 openai-0.26.4 packaging-24.1 pandas-1.1.5 pillow-10.4.0 plac-1.1.3 preshed-3.0.9 protobuf-5.28.2 psutil-6.0.0 pyarrow-17.0.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 pybind11-2.6.1 pysbd-0.3.4 python-dateutil-2.9.0.post0 pytorch-lightning-1.2.1 pytz-2020.4 regex-2024.9.11 requests-2.32.3 requests-oauthlib-2.0.0 rsa-4.9 sacremoses-0.1.1 scikit-learn-0.23.2 scipy-1.6.0 scispacy-0.3.0 spacy-2.3.5 srsly-1.0.7 tensorboard-2.14.0 tensorboard-data-server-0.7.2 thinc-7.4.6 threadpoolctl-3.5.0 tokenizers-0.9.4 torch-1.7.1 torchvision-0.8.2 tqdm-4.49.0 transformers-4.2.2 typing-extensions-4.12.2 urllib3-2.2.3 wasabi-0.10.1 werkzeug-3.0.4 xxhash-3.5.0 yarl-1.13.1 zipp-3.20.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "certifi",
                  "charset_normalizer",
                  "requests"
                ]
              },
              "id": "5417c1f5de3642abbeef6f8a8d1b6631"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r requirements.txt --default-timeout=100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TrSkUPoTLDg"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb5gF6XsP1in"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAHhZ3z_FYap"
      },
      "outputs": [],
      "source": [
        "def split_corpus(corpus_file_path, splited_corpus_path):\n",
        "  doc_ids__splits__map = {}\n",
        "  splits__doc_ids__map = {}\n",
        "  lines = read_file_into_list(corpus_file_path)\n",
        "  splited_lines = []\n",
        "  for line in lines:\n",
        "    line = json.loads(line)\n",
        "    doc_id = line['doc_id']\n",
        "    title = line['title']\n",
        "    doc_ids__splits__map[doc_id] = []\n",
        "\n",
        "    word_count = 0\n",
        "    split_count = 0\n",
        "    split = []\n",
        "    for index, sent in enumerate(line['abstract']):\n",
        "      word_count += len(sent.split())\n",
        "      split.append(sent)\n",
        "\n",
        "      if word_count >= 500 or index == len(line['abstract']) - 1:\n",
        "        split_id = int(doc_id * 10 + split_count)\n",
        "        splited_lines.append({\"doc_id\": split_id, \"title\": title, \"abstract\": split})\n",
        "        doc_ids__splits__map[doc_id].append(split_id)\n",
        "        splits__doc_ids__map[split_id] = doc_id\n",
        "        word_count = 0\n",
        "        split_count += 1\n",
        "        split = []\n",
        "\n",
        "  write_list_to_file(splited_corpus_path, [json.dumps(line) for line in splited_lines])\n",
        "  return doc_ids__splits__map, splits__doc_ids__map\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAZA8rR2L1QJ"
      },
      "outputs": [],
      "source": [
        "def convert_claims_docids_to_splitid(doc_ids__splits__map, claims_file_path, split_claims_file_path):\n",
        "  lines = read_file_into_list(claims_file_path)\n",
        "  splited_lines = []\n",
        "  for line in lines:\n",
        "    line = json.loads(line)\n",
        "    split_doc_ids = []\n",
        "    for doc_id in line['doc_ids']:\n",
        "      split_doc_ids.extend(doc_ids__splits__map[doc_id])\n",
        "    splited_lines.append({\"id\": line['id'], \"claim\": line['claim'], \"doc_ids\": split_doc_ids})\n",
        "\n",
        "  write_list_to_file(split_claims_file_path, [json.dumps(line) for line in splited_lines])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqXHz2tqwOqh"
      },
      "outputs": [],
      "source": [
        "def run_prediction(claims_file_path, corpus_file_path, prediction_path):\n",
        "  !python3 multivers/predict.py --checkpoint_path=checkpoints/scifact.ckpt --input_file=$claims_file_path --corpus_file=$corpus_file_path --output_file=$prediction_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyFmRos8v0Y_"
      },
      "outputs": [],
      "source": [
        "def run_abstract_prediction(claims_dir, abstract_corpus_file_path, prediction_dir):\n",
        "  datasets_claims_file_path= join(claims_dir, 'datasets_claims.jsonl')\n",
        "  approach_claims_file_path= join(claims_dir, 'approach_claims.jsonl')\n",
        "  keydifferences_claims_file_path= join(claims_dir, 'split_keydifferences_claims.jsonl')\n",
        "\n",
        "  print('running pridections for', prediction_dir)\n",
        "  datasets_abstract_prediction_path = join(prediction_dir, 'datasets_abstract.jsonl')\n",
        "  approach_abstract_prediction_path= join(prediction_dir, 'approach_abstract.jsonl')\n",
        "  keydifferences_abstract_prediction_path= join(prediction_dir, 'keydifferences_abstract.jsonl')\n",
        "\n",
        "  run_prediction(datasets_claims_file_path, abstract_corpus_file_path, datasets_abstract_prediction_path)\n",
        "  run_prediction(approach_claims_file_path, abstract_corpus_file_path, approach_abstract_prediction_path)\n",
        "  run_prediction(keydifferences_claims_file_path, abstract_corpus_file_path, keydifferences_abstract_prediction_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M04apF47xiro"
      },
      "outputs": [],
      "source": [
        "def run_long_prediction(claims_dir, corpus_file_path, prediction_dir, corpus_type):\n",
        "  datasets_claims_file_path= join(claims_dir, 'datasets_claims.jsonl')\n",
        "  approach_claims_file_path= join(claims_dir, 'approach_claims.jsonl')\n",
        "  keydifferences_claims_file_path= join(claims_dir, 'split_keydifferences_claims.jsonl')\n",
        "  print('running pridections for', prediction_dir)\n",
        "\n",
        "  datasets_prediction_path = join(prediction_dir, f'datasets_{corpus_type}.jsonl')\n",
        "  approach_prediction_path= join(prediction_dir, f'approach_{corpus_type}.jsonl')\n",
        "  keydifferences_prediction_path = join(prediction_dir, f'keydifferences_{corpus_type}.jsonl')\n",
        "\n",
        "  splited_corpus_path = corpus_file_path[:-6] + '_split.jsonl'\n",
        "  doc_ids__splits__map, splits__doc_ids__map = split_corpus(corpus_file_path, splited_corpus_path)\n",
        "  split_datasets_claims_file_path = datasets_claims_file_path[:-6] + f'_{corpus_type}_split.jsonl'\n",
        "  convert_claims_docids_to_splitid(doc_ids__splits__map, datasets_claims_file_path, split_datasets_claims_file_path)\n",
        "  run_prediction(split_datasets_claims_file_path, splited_corpus_path, datasets_prediction_path)\n",
        "\n",
        "  split_approach_claims_file_path = approach_claims_file_path[:-6] + f'_{corpus_type}_split.jsonl'\n",
        "  convert_claims_docids_to_splitid(doc_ids__splits__map, approach_claims_file_path, split_approach_claims_file_path)\n",
        "  run_prediction(split_approach_claims_file_path, splited_corpus_path, approach_prediction_path)\n",
        "\n",
        "\n",
        "  split_keydifferences_claims_file_path = keydifferences_claims_file_path[:-6] + f'_{corpus_type}_split.jsonl'\n",
        "  convert_claims_docids_to_splitid(doc_ids__splits__map, keydifferences_claims_file_path, split_keydifferences_claims_file_path)\n",
        "  run_prediction(split_keydifferences_claims_file_path, splited_corpus_path, keydifferences_prediction_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5LbIeu3CrkU"
      },
      "outputs": [],
      "source": [
        "def run_key_differences_prediction(claims_dir, corpus_file_path, prediction_dir):\n",
        "  raw_keydifferences_claims_file_path = join(claims_dir, 'raw_keydifferences_claims.jsonl')\n",
        "  raw_keydifferences_prediction_path= join(prediction_dir, 'raw_keydifferences.jsonl')\n",
        "\n",
        "  # prediction difference\n",
        "  run_prediction(raw_keydifferences_claims_file_path, corpus_file_path, raw_keydifferences_prediction_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orrL74hjvK8-"
      },
      "source": [
        "#### Interviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1yezjgy9vOSy",
        "outputId": "5cefe88e-fd4e-4d17-83cb-5f833431e3c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running prediction for prompt 3 and run 1\n",
            "100% 26/26 [00:06<00:00,  3.80it/s]\n",
            "running prediction for prompt 3 and run 2\n",
            "100% 26/26 [00:06<00:00,  3.93it/s]\n",
            "running prediction for prompt 3 and run 3\n",
            "100% 26/26 [00:06<00:00,  4.14it/s]\n",
            "running prediction for prompt 3 and run 4\n",
            "100% 26/26 [00:06<00:00,  3.97it/s]\n",
            "running prediction for prompt 3 and run 5\n",
            "100% 26/26 [00:06<00:00,  3.91it/s]\n"
          ]
        }
      ],
      "source": [
        "for gpt_model in list_directories(interviews_claims_base_path):\n",
        "  for prompt_version in list_directories(join(interviews_claims_base_path, gpt_model)):\n",
        "    for run in list_directories(join(interviews_claims_base_path, f'{gpt_model}/{prompt_version}')):\n",
        "      print(f'running prediction for gpt {gpt_model}, prompt {prompt_version} and run {run}')\n",
        "      run_sub_dir = f'{gpt_model}/{prompt_version}/{run}'\n",
        "      makedirs(join(interviews_predictions_base_path, run_sub_dir))\n",
        "      run_key_differences_prediction(join(interviews_claims_base_path, run_sub_dir), join(interviews_corpus_dir, 'difference.jsonl'), join(interviews_predictions_base_path, run_sub_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZtMgoXz2_G-"
      },
      "source": [
        "#### surveys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for gpt_model in list_directories(surveys_claims_base_path):\n",
        "  for prompt_version in list_directories(join(surveys_claims_base_path, gpt_model)):\n",
        "    for run in list_directories(join(surveys_claims_base_path, f'{gpt_model}/{prompt_version}')):\n",
        "      print(f'running prediction for gpt {gpt_model}, prompt {prompt_version} and run {run}')\n",
        "      run_sub_dir = f'{gpt_model}/{prompt_version}/{run}'\n",
        "      makedirs(join(surveys_predictions_base_path, run_sub_dir))\n",
        "      run_key_differences_prediction(join(surveys_claims_base_path, run_sub_dir), join(surveys_corpus_dir, 'difference.jsonl'), join(surveys_predictions_base_path, run_sub_dir))"
      ],
      "metadata": {
        "id": "Hjrk-XPSJnfg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": [],
      "collapsed_sections": [
        "9fTAjlC_S3xX"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}